[{
    "title": "Different between pointer and value for struct type in golang",
    "date": "",
    "description": "The different between pointer and value for struct type in golang with examples",
    "body": "I always had a question \u0026ldquo;Should I use value for type in struct? Or using pointer?\u0026rdquo;\nAfter reading many different projects on the web, it seems that they all have different practices.\nUntil I recently encountered a problem that gave me some ideas.\nIn our projects, we often start by defining the Request incoming Struct\nThen use json.Unmarshal to convert the incoming json string into Struct for subsequent operations\nPreviously, the default value for fields was false, so we pass by value.\nIn this case, the default value is true and the problem happened.\nTake the first look at the code and execution results\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type John struct { Name string `json:\u0026#34;name\u0026#34;` CanRun bool `json:\u0026#34;canRun\u0026#34;` CanFly bool `json:\u0026#34;canFly\u0026#34;` } type Doe struct { Name string `json:\u0026#34;name\u0026#34;` CanRun *bool `json:\u0026#34;canRun\u0026#34;` CanFly *bool `json:\u0026#34;canFly\u0026#34;` } func main() { var param = []byte(`{\u0026#34;Name\u0026#34;:\u0026#34;John Doe\u0026#34;, \u0026#34;canRun\u0026#34;:true}`) defult := true var john = new(John) json.Unmarshal(param, \u0026amp;john) jsondata, _ := json.Marshal(john) fmt.Println(string(jsondata)) var doe = new(Doe) json.Unmarshal(param, \u0026amp;doe) doejsondata, _ := json.Marshal(doe) fmt.Println(string(doejsondata)) if doe.CanFly == nil { doe.CanFly = \u0026amp;defult } doejsondata, _ = json.Marshal(doe) fmt.Println(string(doejsondata)) } In the example, there are two Structs.\nJohn, whose Type is a Bollinger pass by value, and Doe, whose Type is a Bollinger pass by pointer.\nIf the CanFly field is \u0026ldquo;optional\u0026rdquo; and the default value is \u0026ldquo;true\u0026rdquo;\nThe param in the example code play as a request without CanFly parameter\nAssign the variables of type \u0026ldquo;John\u0026rdquo; and \u0026ldquo;Doe\u0026rdquo; Use json.Unmarshal to convert the incoming json string to Struct Convert to json strings via json.Marshal for easy reading and printing on the screen The first line of the output is \u0026ldquo;John\u0026rdquo;, which using the boolean value as type.\nWhen calling new(John) will automatically bring in the zero value .\nAnd the default zero value of boolean is false.\nAnd Name and CanRnu are overwritten with zero value because they are passed in.\nBut because CanFly is optional and not passed in by the user, it is still false\nWe can\u0026rsquo;t identify the false is from user or from zero value.\nIf you use \u0026ldquo;pointer\u0026rdquo; as the type in Struct, you can see that the second line of CanFly is null.\nSo you can set the default value to \u0026ldquo;true\u0026rdquo; later on.\nThis is what I learned about the default value problem when converting json strings to Struct, and how to solve it with a pointer.\n",
    "ref": "/en/blog/202306-different-between-pointer-and-value-for-struct-type-in-golang/"
  },{
    "title": "Using FFmpeg to convert media files",
    "date": "",
    "description": "Convert .ts video files to .mp4",
    "body": "Sometimes, we can use certain methods to download the .M3U8 file and then merge it into a .ts file.\nHowever, computers usually cannot directly play this format.\nWe can use FFmpeg to convert it into a common .mp4 file for easy playback.\nInstallation First, we need to download FFmpeg. If you are a Mac user, you can install it using Homebrew .\nbrew install ffmpeg To check if the installation is successful, you can use the following command:\nffmpeg -version Convert media files Once you have installed it, the next step is file conversion.\nUse the following command and replace the source .ts file and the destination path with your own target path:\nffmpeg -i /source_path/vedio.ts -acodec copy -vcodec copy -f mp4 /target_path/new_vedio.mp4 After executing the command, you will be able to see the converted file in the target path!\nWith FFmpeg, you no longer need to install additional software or rely on online file conversion services.\n",
    "ref": "/en/blog/202301-ffmpeg-video-convert/"
  },{
    "title": "[AWS 101] What are SNS and SQS",
    "date": "",
    "description": "Briefly note what AWS SNS and SQS services do and their use cases",
    "body": "Amazon Simple Queue Service (SQS) and Amazon Simple Notification Service (SNS) may look similar.\nBut in reality, they are quite different.\nTo better clarify the differences between the two, let\u0026rsquo;s compare the notes on both services side by side.\nAWS SNS Full name is: Amazon Simple Notification Service\nThe publisher system can use Topics to distribute event messages to a large number of subscriber systems for parallel processing.\nThe subscribers include:\nAmazon SQS AWS Lambda function HTTPS endpoint Amazon Kinesis Data Firehose The architecture looks like this:\nFeatures: Send real-time messages to all subscribers. Do not retain sent messages. Follow the Publish–subscribe pattern. AWS SQS Full name is: Amazon Simple Queue Service (SQS)\nIt is a fully managed message queue service.\nMessages are sent to a queue and processed by the recipient who retrieves them.\nIt is a Serverless solution, but the executor can be either Lambda function or non-Serverless projects like Laravel.\nType Two types of queues are provided to cater to different application requirements:\nStandard Queue The application can handle messages that arrive more than once and are not ordered sequentially.\nUnlimited Throughput: Standard queues support nearly unlimited transactions per second (TPS) for each API action. At-Least-Once Delivery: A message is guaranteed to be delivered at least once, but occasionally, multiple copies of the message may be delivered. Best-Effort Ordering: Messages are occasionally not delivered in the exact order they were sent. FIFO Queue When the order of operations and events is crucial or duplicates are not acceptable:\nHigh Throughput: By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). Exactly-Once Processing: Messages are delivered exactly once and remain available until the consumer processes and deletes them. First-In-First-Out Delivery: Messages are strictly delivered and received in the order they were sent (First In, First Out - FIFO). Poll Short polling This method is similar to someone repeatedly asking you if there is any job available, even if there is no job.\nYou will get immediate results after each inquiry, even if the queue is empty.\nHowever, to ensure you receive the latest status, you need to keep querying continuously.\nLong polling After making a request, you will not receive an immediate response unless there is a timeout or a message in the queue.\nThis approach minimizes additional polling to reduce costs while ensuring the fastest possible reception of new messages.\nWhen the queue is empty, a long-polling request for the next message can wait for a maximum of 20 seconds.\nFeatures: The receiver actively polls messages. A queue can only be associated with a single consumer. The message is deleted only after the consumer responds with the completion of the processing. Difference between SNS and SQS SNS When new content is updated, event notifications are sent to all subscribers.\nIt follows a push-based architecture where messages are automatically pushed to the subscribers.\nIn simple terms, it is a broadcast notification. SQS Separating queue tasks from the codebase involves a Pull-Based architecture\nConsumers are responsible for pulling messages from the queue and processing them on their own.\nIn simple terms, it is a Queue Here\u0026rsquo;s an example of how these features and architectures can be combined: 1. Methods for creating an SNS Topic and SQS: Access the SNS service and create an SNS topic. Access the SQS service and create an SQS queue. Subscribe the newly created SQS queue to the SNS topic just created to retrieve podcast content. In the SQS Dashboard, select the target SQS queue. Click on the Action dropdown menu and choose Subscribe to Amazon SNS topic. Select the topic and click \u0026ldquo;Save.\u0026rdquo; 2. Simulate sending a message from SNS 到 SNS Topic 頁面並選擇 Topic 後，點選右上角Publish message 輸入測試用的內容，並按下Publish message 3. Confirm if the subscription message has been received After publishing a message in SNS, all endpoints subscribed to the topic should receive it.\nOpen the SQS page and select the SQS queue that is subscribed to the topic. Click on Send and receive message. 往Scroll down to the \u0026ldquo;Receive messages\u0026rdquo; section and select Poll for Messages. Once successful, you will see a list of messages appearing below, and you can click on them to view detailed content. Reference source: Send Fanout Event Notifications ",
    "ref": "/en/blog/202210-aws101-what-is-sns-and-sqs/"
  },{
    "title": "Serverless 101",
    "date": "",
    "description": "Introducing What is Serverless with AWS Services",
    "body": "Since our company\u0026rsquo;s services are built on AWS and heavily rely on various serverless computing services,\nI had no prior experience in this area. This series serves as a compilation of notes for me,\nand its content may be subject to constant revisions in the future.\nAlthough it\u0026rsquo;s called \u0026ldquo;serverless,\u0026rdquo; servers still exist in reality; they are simply deployed and maintained by the cloud platform.\nFor users, all they need to do is write the code without having to worry about server tuning.\nBackend engineers often have to manage servers on the side, and they know how painful it is.\nHowever, with the Serverless architecture, infrastructure management can be delegated to cloud service providers simply by writing configuration files (and pulling out the magical card from the boss).\nCommon cloud service providers all have their own Serverless services\nAWS：Lambda Microsoft Azure：Functions Google：Cloud Functions The Serverless-related services provided by AWS can be broadly categorized as follows:\nServerless ≠ Lambda Function That is a common misconception.\nIn addition to Lambda Functions, Serverless also encompasses the composition of event sources and other resources.\nThis includes APIs, databases, and event triggers that work together to execute tasks.\nAnd AWS provides AWS Serverless Application Model(AWS SAM） It is an open source architecture for building serverless applications\nPros Basically, the infrastructure is outsourced to a cloud service provider to help you solve\nHigh availability: Increase program traffic flexibility and stability through automatic scaling and load balancing Reduced server idle performance: charge according to usage, use as much as you want instead of keeping the host on all the time Reduced server setup and maintenance labor costs Security: No need to worry about the overall security of the server Cons It primarily stems from the issues arising from not having servers running continuously.\nDebugging can be a bit challenging and requires the use of log processing mechanisms like CloudWatch. Can\u0026rsquo;t handle Schedule job =\u0026gt; Lambda not always running There is a startup time involved when executing Lambdas. Request cannot be guaranteed to run on the same Lambda =\u0026gt; Lambdas are stateless, state synchronization must be handled through other services. Architecture By using Lambda to write different functions, it replaces the traditional monolithic codebase that runs on virtual servers.\nThis can be seen as an implementation of Function as a Service (FaaS) paradigm.\nThe general flow is as follows:\nEvent Source -\u0026gt; Lambda Function -\u0026gt; Service Event Source: An event or trigger occurs, such as an API request, database update, or message in a queue. This event acts as the source of the operation. Lambda Function: The event is passed to a Lambda function, which is a small piece of code responsible for processing the event and performing the necessary actions or computations. The Lambda function is executed in a serverless environment. Service: The Lambda function interacts with other services or resources, such as databases, APIs, or external systems, to complete the required operations. These services provide the necessary functionality and data processing capabilities. For a simple to-do list application, the system architecture diagram using Serverless would look like this\nThe key focus would be on the dashed section labeled ToDo App on the right side of the diagram.\nThis section aligns with the flow of Event Source -\u0026gt; Lambda Function -\u0026gt; Service mentioned above.\n",
    "ref": "/en/blog/202210-what-is-serverless/"
  },{
    "title": "Interacting with smart contracts using Golang",
    "date": "",
    "description": "Generating a Go package with abigen tool to interact with smart contracts and perform contract operations",
    "body": "Previously, I was calling Smart Contracts through PHP, which raised two issues.\nOne was the slowness, and the other was the lack of maintenance of the package.\nFortunately, Golang provides an official package that is both fast and solves these two problems.\nThis article explains how to convert existing Smart Contract ABIs into a Go package for interacting with contracts.\nPreparation To set up the necessary packages on Mac, you need to install the following dependencies through Homebrew beforehand.\nbrew update brew tap ethereum/ethereum brew install ethereum brew install solidity brew install protobuf Otherwise, you may encounter issues in the subsequent steps.\nPlease install solc\nPlease install protoc\nInstall go-ethereum Next, you need to install the conversion tool we will be using.\nThe official package comes with the abigen command, which allows you to convert the ABI into a Go package.\ngit clone https://github.com/ethereum/go-ethereum.git cd go-ethereum make devtools Generate Go library file Once the installation is complete, you will need the smart contract ABI to perform the conversion.\nHowever, the topic of what a smart contract ABI is falls outside the scope of this article.\nIf you require a more in-depth explanation of smart contract operations in the future, it can be covered separately.\nOnce you have obtained the abi.json file for the smart contract, you can proceed with the following command execution:\nabigen --abi=\u0026#34;./erc721.abi.json\u0026#34; --type=\u0026#34;erc721\u0026#34; --pkg=erc721 --out=\u0026#34;erc721.go\u0026#34; flags description usage \u0026ndash;abi file path for smart contract abi.json ./erc721.abi.json \u0026ndash;type type name in struct erc721 \u0026ndash;pkg go package name in output file erc721 \u0026ndash;out output file name erc721.go As a result, you will have a file named erc721.go, and upon opening it, you will find that the package name is erc721.\nBy comparing the functions inside the file, you will notice that they correspond to the functions in the abi.\nNow, you can interact with the contract by importing and referencing the erc721 package.\nUsing the package in Golang. To interact with the generated erc721 package in Go, you need to first import the package in your code.\nSince blockchain operations are essential, you should also import go-ethereum. Here\u0026rsquo;s an example:\nimport ( erc721 \u0026#34;project/package-erc721/path\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/common\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/ethclient\u0026#34; ) Then interacting with smart contracts using go-ethereum.\nThe following example demonstrates the interaction with the TotalSupply function in a smart contract.\nPassing a smart contract\u0026rsquo;s contract address string: hexAddress.\nYou can obtain the total supply issued by the contract, and this example focuses on an ERC721 NFT contract.\ninfuraUrl := \u0026#34;https://mainnet.infura.io/v3/38ad7d4b...97aa1d5c583\u0026#34; client, err := ethclient.Dial(infuraUrl) if err != nil { return nil, err } defer client.Close() address := common.HexToAddress(hexAddress) instance, err := erc721.NewErc721(address, client) if err != nil { return nil, err } totalSupply, err := instance.TotalSupply(nil) if err != nil { return nil, err } return totalSupply, nil This post illustrates how to use abigen to generate a corresponding Go package for a Smart Contract.\nSo far, the functions called, including Owner, OwnerOf, and TotalSupply, have successfully retrieved data from the blockchain.\nThere are other applications to explore in the future as opportunities arise for further operations.\n",
    "ref": "/en/blog/202207-go-ethereum-abigen/"
  },{
    "title": "Create a Redis service on GCP",
    "date": "",
    "description": "Set up a Redis service using Google Cloud MemoryStore.",
    "body": "GCP offers the Google Cloud MemoryStore service for creating Redis or Memcached caching machine services.\nThe pricing is similar to that of setting up a database.\nI thought it would be as inexpensive as using virtual machines (VMs).\nCreating Redis Go to the MemoryStore page and click on either \u0026ldquo;Redis\u0026rdquo; or \u0026ldquo;Memcached\u0026rdquo; to begin the setup process.\nIn this example, for Redis, click on \u0026ldquo;Create Instance\u0026rdquo; to proceed.\nEnter some basic configurations, which mainly involve adjusting the size and name.\nAfter configuring the options as prompted, click on \u0026ldquo;Create Instance\u0026rdquo; and then wait for it to be created.\nAt this point, there will be a short waiting period, so you can take a break in the meantime.\nThis is the method to create Redis on GCP.\nOnce created, you only need to map the endpoint in your application to the IP of the Redis instance to start using it.\nAlternatively, you can also use third-party GUI tools like Another Redis Desktop Manager to access via IP address.\n",
    "ref": "/en/blog/202206-gcp-redis/"
  },{
    "title": "Make your computer be temporarily accessible on the internet. - Ngrok",
    "date": "",
    "description": "Use Ngrok to temporarily obtain a publicly accessible URL for your computer.",
    "body": "In development, it is often encountered that testing on the local machine requires a publicly accessible URL for services.\nOr webhooks to use as a callback. Moreover, these services often require HTTPS certification.\nAre there any other options besides setting up a cloud server for this purpose?\nNgrok As a forwarding server, it can redirect external requests to a specified port on your machine.\nThe principle is to connect to the Ngrok cloud server, which exposes the address you specify on your local machine.\nNgrok then provides a public URL for accessing the content.\nThe advantages are that it is fast, provides HTTPS services for enhanced security, and you can even set up password protection.\nOfficial documentation and download links Install Mac brew install ngrok Linux Determine the hardware architecture of your host machine.\ncat /proc/cpuinfo Download the specified file from the official website and follow the steps to install.\nOr just install using snap\nsudo apt update sudo apt install snapd sudo snap install ngrok Start service Enter the command to start and listen on port 8080.\nngrok http 8080 You will then be able to see the publicly accessible URL.\n註冊 ngrok You can use the service without registering, but after a period of time, the connection will be terminated.\nAnd upon restarting, a new URL will be assigned.\nHowever, when testing webhooks or providing the URL to others, having to reassign the URL means reconfiguring webhooks or notifying others, which can be inconvenient.\nAfter logging into your Ngrok account, go to the Your Authtoken page.\nCopy the Authtoken and then enter it in the terminal using the following command:\nngrok config add-authtoken {Your Authtoken} Seeing the following message indicates that the authentication process is complete.\nAuthtoken saved to configuration file: /Users/user_name/.ngrok2/ngrok.yml After completing the registration, you can use the Ngrok service without worrying about the connection being terminated after a while.\n",
    "ref": "/en/blog/202204-ngrok/"
  },{
    "title": "Jenkins CI/CD 04 - Using SSH Commands to Operate another VM Instance in GCP",
    "date": "",
    "description": "Enable Jenkins to perform remote operations on a remote host through SSH, replacing manual deployment.",
    "body": "In a non-automated deployment scenario, manually connecting to the server host\u0026rsquo;s internals is required every time.\nExecuting deployment commands or running deployment command executables not only involves inconvenience but also carries the risk of human error.\nWith Jenkins\u0026rsquo; pipeline, we can replace manual execution, making deployments easier and more pleasant.\nPackage Installation Yes, you guessed it; we need to install a package again.\nSpoiler alert: Don\u0026rsquo;t install it just yet. Consider it after reading the next paragraph.\nThis time, we need to install support for the pipeline: SSH Pipeline Steps .\nThe installation process is the same as for other packages.\nIn the side menu, select Manage Jenkins -\u0026gt; Manage Plugins and search for ssh-steps.\nThen install the SSH Pipeline StepsVersion package.\nUsing gcloud for Login Then it suddenly occurred to me that we had installed gcloud earlier, and gcloud itself has the capability to directly connect via SSH to other instances within the same project! No need to install additional packages! Congratulations!\nIf you haven\u0026rsquo;t installed the gcloud command yet, you can refer to the tutorial in a previous article: Jenkins CI/CD 03 - Building and Pushing Docker Images to GCR .\nThe prerequisite for using gcloud for SSH commands is to have the corresponding permissions set in your Identity and Access Management (IAM).\nIf the commands in the following steps are denied, you\u0026rsquo;ll have to figure it out yourself.\nIf you see an error message, adjust the permissions on the IAM page as suggested.\nFirst, switch the user to Jenkins on the Jenkins host:\nsudo su jenkins Then, enter the command:\ngcloud compute ssh INTERNAL_INSTANCE_NAME --zone=ZONE --internal-ip The first time you connect, it will prompt you to generate and add an SSH key.\nThis is where IAM permissions issues are most likely to occur.\nIf you can\u0026rsquo;t log in, it means you need to adjust the permissions for the key you previously set.\nAfter setting up the IAM correctly, follow the prompts to generate an SSH key, which will be automatically added to the metadata in Compute Engine. Afterward, you will be able to log in directly.\nConnecting with gcloud Commands in the Pipeline // jenkinsfile pipeline { agent any stages { stage(\u0026#39;Deploy branch: develop to beta\u0026#39;) { when { branch \u0026#39;develop\u0026#39; } steps { echo \u0026#34;ssh to store-beta-api instance.\u0026#34; withCredentials([file(credentialsId: \u0026#39;jenkins-gcr\u0026#39;, variable: \u0026#39;GC_KEY\u0026#39;)]) { sh \u0026#34;gcloud compute ssh store-beta-api --zone=asia-east1-b --internal-ip --command \u0026#39;cd /data/store-backend \u0026amp;\u0026amp; sudo sh ./_scripts/deploy_beta.sh\u0026#39;\u0026#34; } echo \u0026#34;Deploy beta done\u0026#34; } } } } A crucial point to note is that you need to use --command to specify the remote command, and it should be a single command string. because the connection is terminated after executing that line. The state doesn\u0026rsquo;t persist!\nIn the example, we demonstrate SSH connection using gcloud commands to a VM instance named store-beta-api within the same GCP project in the asia-east1-b region, accessed through its internal static IP address.\nOnce the connection is successful, two commands are executed: one to enter the /data/store-backend directory and the other to run the ./_scripts/deploy_beta.sh executable file within that directory.\nIn small-scale services or internal testing environments, you typically have only one VM instance running.\nWith this example, you can automatically connect to the host\u0026rsquo;s internals for deployment when the trigger conditions are met.\nReference: gcloud compute ssh ",
    "ref": "/en/blog/202204-jenkins-cicd-4-ssh-remote-server/"
  },{
    "title": "Jenkins CI/CD 03 - Building and Pushing Docker Images to GCR",
    "date": "",
    "description": "This article discusses the process of building a Docker image and pushing it to the Google Container Registry (GCR) within a Jenkins pipeline using the 'gcloud' command.",
    "body": "Google Container Registry is a service provided by Google for storing, managing, and securing Docker container images.\nThe objective at hand is to package a Docker image and push it to Google Cloud Platform (GCP) using the \u0026lsquo;gcloud\u0026rsquo; command for future deployment purposes.\nGoogle Container Registry (GCR) Service First and foremost, you need a Google Cloud project, which can be created following the instructions provided in the official documentation: Creating and managing projects .\nInstalling gcloud on the Jenkins Host The following steps outline the installation process for gcloud as the official documentation: Installing the gcloud CLI .\nPrerequisites Ensure that the following packages are installed: apt-transport-https, ca-certificates, and gnupg. You can list all installed packages using the following command:\napt list --installed If any of the packages are missing, you can install them using the following command:\nsudo apt-get install apt-transport-https ca-certificates gnupg Add the gcloud CLI Distribution URI as a Package Source echo \u0026#34;deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list # If \u0026#39;signed-by\u0026#39; is not supported, you can use the following command instead echo \u0026#34;deb https://packages.cloud.google.com/apt cloud-sdk main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list Import the Google Cloud Public Key If the apt-key command supports the --keyring parameter, you can use the following command:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Otherwise, you can use the following command:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - For Debian 11+ or Ubuntu 21.10+ distributions that do not support the apt-key command, you can use this command:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo tee /usr/share/keyrings/cloud.google.gpg Update and Install gcloud CLI Run the following commands to update and install the gcloud CLI:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install google-cloud-cli Initialize gcloud Since we are operating gcloud within a Google Compute Engine virtual machine under the same GCP project,\nthe gcloud auth login step is skipped.\nInstead, the VM host is used as the login account, and the command will prompt for confirmation as follows:\nYou are running on a Google Compute Engine virtual machine. It is recommended that you use service accounts for authentication. You can run: $ gcloud config set account `ACCOUNT` to switch accounts if necessary. Your credentials may be visible to others with access to this virtual machine. Are you sure you want to authenticate with your personal account? Do you want to continue (Y/n)? You can check the list of projects with the following command:\ngcloud projects list If you encounter the error message:\nERROR: (gcloud.projects.list) PERMISSION_DENIED: Request had insufficient authentication scopes.\nFirst stop the VM and navigate to the editing page.\nFind the Access scopes section and enable Full access to all Cloud APIs.\nAfter restarting the VM, execute the command and follow the prompts:\ngcloud init # Your Google Cloud SDK is configured and ready to use! You can use the following command to confirm the configuration settings:\ngcloud config configurations list Setting Jenkins Permissions for Push Once you\u0026rsquo;ve verified that the gcloud command is functional, you need to set up Jenkins to push images to GCR.\nGenerate a GCP Key Navigate to the GCP Console, click on APIs \u0026amp; Services, and select the Credentials tab.\nClick Create credentials and choose Service Account.\nFollow the prompts to create the service account, then click on the account you just created to enter the edit page.\nSwitch to the Keys tab and click \u0026ldquo;Add Key -\u0026gt; create a new key\u0026rdquo; choosing the \u0026ldquo;JSON\u0026rdquo; type.\nA download prompt will appear; download the JSON key file to your local machine.\nConfigure the Key in Jenkins 進到Jenkins -\u0026gt; 管理 Jenkins -\u0026gt; Manage Credentials\n選擇作用範圍，這邊用預設的Jenkins也就是全域(global)\nNavigate to Jenkins -\u0026gt; Manage Jenkins -\u0026gt; Manage Credentials.\nChoose the scope, which, in this example, is the default Jenkins as global.\nClick \u0026ldquo;Add Credentials\u0026rdquo; and select Secret file. Fill in the required information:\nID: A unique identifier for calling within Jenkins, in this example, it is named jenkins-gcr. File: Upload the JSON key file you downloaded earlier. This completes the Jenkins key setup.\nPipeline Example // jenkinsfile pipeline { agent any environment { GCR_HOST = \u0026#34;asia.gcr.io\u0026#34; // Choose a GCR host location that is geographically close or simply use gcr.io PROJECT_ID = \u0026#34;my-project-315408\u0026#34; // The ID of your GCP project FOLDER = \u0026#34;my-project-backend-test\u0026#34; VERSION = \u0026#34;${TAG_NAME}\u0026#34; IMAGE = \u0026#34;$GCR_HOST/$PROJECT_ID/$FOLDER:$VERSION\u0026#34; // Assemble the image name } stages { stage(\u0026#39;Build docker image\u0026#39;) { when { branch \u0026#39;main\u0026#39; } steps { // Docker build command specifying the path to the Dockerfile in your project sh \u0026#34;docker build . -f ./builds/docker/php81/Dockerfile -t ${IMAGE}\u0026#34; } } stage(\u0026#39;Push image to Google Container Registry\u0026#39;) { when { branch \u0026#39;main\u0026#39; } steps { // \u0026#39;jenkins-gcr\u0026#39; is the unique identifier ID set up earlier withCredentials([file(credentialsId: \u0026#39;jenkins-gcr\u0026#39;, variable: \u0026#39;GC_KEY\u0026#39;)]) { sh \u0026#34;cat \u0026#39;$GC_KEY\u0026#39; | docker login -u _json_key --password-stdin https://asia.gcr.io\u0026#34; sh \u0026#34;gcloud auth activate-service-account --key-file=${GC_KEY}\u0026#34; sh \u0026#34;gcloud auth configure-docker\u0026#34; echo \u0026#34;Pushing image to GCR\u0026#34; sh \u0026#34;docker push ${IMAGE}\u0026#34; } } } } } Finally, trigger the Jenkins service, which, in the example, is done by pushing commits to the main branch.\nChecking the Storage Location 若是沒有錯誤訊息，則可以到Google Cloud Registry頁面查看\n把剛剛上傳的 image 名稱想成在 linux 終端機下指令，就可以理解 GCR 中的資料夾結構\nIf no error messages were encountered, you can check the Google Cloud Registry page to view the image.\nThe image name in GCR is analogous to the path structure in Linux.\n",
    "ref": "/en/blog/202204-jenkins-cicd-3-push-docker-image-to-gcr/"
  },{
    "title": "Scalable Server 02 Load Balancing",
    "date": "",
    "description": "Configuring Load Balancing on GCP for Enhanced Server Flexibility",
    "body": "In the previous post Scalable Server 01 Auto Scaling , it was mentioned that we need to configure a set of Load Balancers to distribute internet requests to different server hosts.\nThis allows us to dynamically adjust the number of hosts based on traffic and maintain stable service operations.\nThe previously created instance group consists of individual hosts with unique IPs.\nHowever, using load balancing allows us to conceal the individual instance IPs and redirect traffic to new machines as traffic increases.\nThis ensures a certain level of security and system stability, preventing the original hosts from crashing due to excessive load.\nCreate a Load Balancing First, locate Network Services in the sidebar menu and select Load Balancing.\nAfter pressing Create Load Balancer on the top toolbar, select HTTP(S) Load Balancer.\nSince the traffic is coming from the internet, select From the internet to VM or serverless service.\nBackend Service configuration Next, select Create a backend service\nChoose port number 80 for internal communication, as our internal servers do not have SSL certificates attached.\nFrontend Service configuration Next, go to Frontend Configuration.\nIf we want to allow access via a fixed set of IP addresses, we\u0026rsquo;ll need to create a set of IP addresses here.\nYou can either pre-create them or reserve a set of fixed IP addresses through on-screen prompts.\nIf you have an SSL certificate, you can also configure it here.\nAfter saving, wait for it to be created.\nThen, click into the details of the newly created load balancer.\nIn the Frontend section, you\u0026rsquo;ll find an IP address.\nPaste this into your browser to access the service you just configured!\nSo, our load balancer is now successfully set up!\nWith the automatic scaling feature, when traffic increases and triggers the scaling rules, new virtual machines will be launched.\nThe load balancer will distribute requests, helping us handle the traffic and maintain stable service operations.\nReference:\n[GCP 教學] 打造彈性、快速且安全的雲端基本服務架構 – 負載平衡 Load Balancer 和 Instance Group GCP VM 雲端主機最基本防護 - Load Balance ",
    "ref": "/en/blog/202204-gcp-load-balancing/"
  },{
    "title": "Introducing Laravel Sail and Basic Operations",
    "date": "",
    "description": "Using Laravel Sail to launch the development environment with ease and joy.",
    "body": "In the context of Laravel development environment setup, both official and unofficial sources offer a plethora of approaches.\nThis time, we will be introducing a package introduced after Laravel 8: Laravel Sail .\nIn the past, to save beginners time in the initial setup of environments, there have been numerous tools for development environments.\nFor example, Laravel Homestead , built using virtual machines, was one option.\nThere were also Docker-based development environments like laradock .\nOr creating a Laravel development environment using Docker , as I personally do.\nIn Laravel 8 and onwards, Laravel Sail has been integrated as a built-in package.\nCompared to Laravel Homestead, it requires fewer resources as it utilizes Docker.\nIn contrast to laradock, which also operates within a Docker environment, Laravel Sail offers simpler configuration, and in some cases, even requires no configuration, achieving a plug-and-play experience.\nYou need to install Docker before start using Laravel Sail\nInstall Sail In Laravel 9, Sail is now included as a built-in feature, allowing you to directly launch services through commands.\nphp artisan sail:install For older versions of Laravel that do not have Sail, you can install it using Composer.\ncomposer require laravel/sail --dev Upon executing the command, an interactive interface will prompt you to select the desired services.\nWhich services would you like to install? [mysql]: [0] mysql [1] pgsql [2] mariadb [3] redis [4] memcached [5] meilisearch [6] minio [7] mailhog [8] selenium \u0026gt; 0,3,7 Sail scaffolding installed successfully. After making your selections, a docker-compose.yml file will be generated in the project\u0026rsquo;s root directory.\nYou can then use a command to launch the Sail service.\n./vendor/bin/sail up 設定指令別名 If you\u0026rsquo;re feeling a bit lazy and don\u0026rsquo;t want to type out the entire command, you can set up an alias for it.\nOpen either vim ~/.bashrc or vim ~/.zshrc (depending on your terminal).\nAdd the following line to the file:\nalias sail=\u0026#34;./vendor/bin/sail\u0026#34; After that, you can directly execute Sail commands within the project using the sail alias.\nThe following operations are performed assuming you\u0026rsquo;ve added the alias.\nSail command The usage is similar to Docker commands:\nsail up -d：Start and run in the background sail stop：Stop sail down：Stop and remove containers sail build --no-cache：Rebuild containers, ignoring all caches for a complete rebuild. Adjust docker image The default image includes the basic Laravel environment.\nIf you need to make adjustments, such as installing additional PHP extensions, you will need to export the relevant configurations.\nphp artisan sail:publish After executing the command, a docker folder will be added to the project, containing container settings and Dockerfiles.\nExecute command Sail provides a convenient way to call various commands by adding sail in front of them.\nEssentially, you prepend sail to the desired commands.\nExecute PHP command sail php --version Execute Composer command sail composer install # composer install Execute Artisan command sail artisan queue:work # php artisan queue:work sail artisan schedule:work # php artisan schedule:work Execute shell command sail shell myShell.sh # sh myShell.sh By following these simple steps, you can easily set up a local development environment and execute commands as well.\nLaravel Sail significantly reduces many of the development hassles!\n",
    "ref": "/en/blog/202204-laravel-sail/"
  },{
    "title": "Jenkins CI/CD 02 Basic Pipeline Setup and GitHub Integration",
    "date": "",
    "description": "Creating a basic pipeline workflow and triggering it through GitHub webhooks",
    "body": "In Jenkins , the build process is known as a pipeline.\nIt can also be triggered through webhooks, offering various triggering and execution methods.\nBelow, we\u0026rsquo;ll demonstrate how to integrate GitHub webhooks and provide common trigger condition examples.\nCreating the First Pipeline Navigate to Open Blue Ocean in the sidebar to create a new pipeline.\nFollow the prompts to create an access token. In this demonstration, I had to generate a new token because I deleted the old one, resulting in a Token deleted error message.\nClick Create an access token here to open the personal access token creation page on GitHub.\nYou can also find it by going to\nGitHub \u0026gt; Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new token\nThe necessary permissions are typically pre-filled; simply click Generate token.\nCopy the generated token and paste it into Jenkins.\nIf you forget to do so at this point, you\u0026rsquo;ll need to regenerate the token.\nOnce verification is complete, you can select a project.\nClick \u0026ldquo;Create,\u0026rdquo; and Jenkins will scan all branches within the GitHub repository.\nConnecting Webhooks to Listen for Events 進入專案的 Settings \u0026gt; Webhook \u0026gt; Add webhook\n在Payload URL填入jenkins主機網址/github-webhook/\n如果 port 不是使用 80 或 443，則要完整輸入。例如 Jenkins 預設使用的是8080port\nNow, let\u0026rsquo;s make some changes to the project and commit and push them to GitHub.\nYou might notice that Jenkins isn\u0026rsquo;t responding.\nThis is because we haven\u0026rsquo;t configured Jenkins to listen to GitHub events.\nHow can we do that? Through webhooks!\nWebhooks transmit events to a specified URL when events configured in the webhook settings are triggered.\nTo allow Jenkins to receive GitHub events, you must bind the webhook to your project.\nThis is typically automated in Drone when you build a project, but not in Jenkins.\nGo to your project\u0026rsquo;s Settings \u0026gt; Webhook \u0026gt; Add webhook.\nIn the Payload URL field, enter your Jenkins server\u0026rsquo;s URL followed by /github-webhook/.\nIf your port isn\u0026rsquo;t 80 or 443, you should include the complete URL.\nFor example, if Jenkins is using the default port 8080:\nClick \u0026ldquo;Add webhook\u0026rdquo; to create it and then test the connection.\nIf the test fails, you can adjust the settings and click the three dots on the right to Redeliver for a retest.\nFrom now on, every time a push event occurs(based on the webhook settings), Jenkins will be notified.\nThis way, we achieve the goal of triggering a Jenkins build process when new content is pushed to GitHub.\nListening for Tag Events In version releases, we often tag the current version.\nIn my previous experience with Drone, when a tag was pushed to the repository, the corresponding event was sent via webhook. However, in Jenkins, it\u0026rsquo;s not that simple.\nI struggled with this for a while until I found the solution in the official documentation: When using tags in Jenkins Pipeline .\nBy default, Jenkins doesn\u0026rsquo;t set up tag listening.\nYou need to access your pipeline settings, navigate to Branch Sources, and in the Behaviours section at the bottom, check Discover tags to enable tag listening.\nI\u0026rsquo;m still not entirely sure why this design choice was made.\nBasic Pipeline Example Here, I\u0026rsquo;ll provide a basic demonstration of several commonly used trigger conditions.\nDetails on what actions to take after triggering will be discussed later.\nStart by creating a file named jenkinsfile in the project\u0026rsquo;s root directory.\nJenkins uses this file as the default pipeline configuration.\n// jenkinsfile pipeline { agent any stages { stage(\u0026#39;Example Build\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } stage(\u0026#39;Example Deploy\u0026#39;) { when { branch \u0026#39;production\u0026#39; } steps { echo \u0026#39;Deploying\u0026#39; } } stage(\u0026#39;Example Tag Deploy\u0026#39;) { when { buildingTag() } steps { echo \u0026#34;Building $TAG_NAME\u0026#34; } } stage(\u0026#39;Example Tag Deploy\u0026#39;) { when { tag \u0026#34;release-*\u0026#34; } steps { echo \u0026#34;Building $TAG_NAME\u0026#34; } } } } Here, we demonstrate four trigger conditions:\nNo when statement: This stage triggers regardless of the conditions. when branch: This stage triggers only when a specified branch is pushed. when buildingTag(): This stage triggers when any tag is created. when tag \u0026quot;release-*: This stage triggers only when tags conforming to the format \u0026ldquo;release-*\u0026rdquo; are created. These four conditions provide a foundation for various pipeline workflows.\nFor more usage options, consult the official documentation: Pipeline Syntax .\nAfter reviewing the documentation, you might find that Drone is preferable.\nAs Jenkins offers a multitude of options, making it easy to get lost.\nDifferent developer have different preferences for using the UI or command line.\nHowever, the fundamental concepts remain the same, and experience in one can be applied to the other.\nThis concludes the basic introduction to Jenkins pipelines.\n",
    "ref": "/en/blog/202203-jenkins-cicd-2-basic-pipeline/"
  },{
    "title": "Scalable Server 01 Auto Scaling",
    "date": "",
    "description": "Configuring Auto Scaling on GCP for Enhanced Server Flexibility",
    "body": "Create image First, you need to stop the specified VM.\nNext, in the sidebar, navigate to Storage -\u0026gt; Images, then click on it to select Create Image.\nMake sure not to choose Virtual Machines -\u0026gt; achine Image as it will lead to confusion later on!\nIn the beginning, I mistakenly pressed Create Machine Image from the VM section and got stuck in this alternate world, which wasted my whole afternoon.\nThe simplest way to determine this is by not stopping the VM in the first step.\nWhen creating an image, if it doesn\u0026rsquo;t prompt you to stop, then you\u0026rsquo;re doing it wrong.\nFollowing the instructions and successfully creating the image, you can proceed to the next step.\nCreate Instance templates Go to Instance Templates, then click on Create Instance Template.\nChoose the hardware specifications or hardware rules This section is where you determine the specifications required for each new instance when launched within the instance group. The setup here resembles the configuration during VM creation. Additionally, you can configure the firewall settings to open ports 80 and 443 if your network service requires them.\nSelect the VM image as the boot template The difference here is that we are using the recently created image as the template.\nSo, under the boot disk section, click on Change -\u0026gt; Custom image and select the image you\u0026rsquo;ve just created.\nExecute commands automatically at startup Sometimes, when you start a new VM, you need to run certain commands to start services like Docker.\nWithout these commands at startup, your VM will only be powered on but won\u0026rsquo;t have services like the Docker daemon running.\nClick on the Management section to expand its contents, and you\u0026rsquo;ll find instructions for automation.\nYou can specify boot scripts that will run when the instance starts up or restarts. Boot scripts are useful for installing software, updating items, and ensuring that services run smoothly within the virtual machine.\nYou can include commands in this section to ensure that when the VM starts, the necessary services are initiated.\n#! /bin/bash sudo systemctl start docker sudo docker start laravel-app nginx redis sudo docker exec -i laravel-app /var/www/html/artisan migrate sudo docker exec -i laravel-app /var/www/html/artisan l5-swagger:generate --all sudo docker exec -i laravel-app /usr/bin/supervisord -c ./builds/docker/beta/php81/horizon.conf sudo docker exec -i laravel-app /var/www/html/artisan queue:work \u0026amp; Official documentation: Using startup scripts on Linux VMs Instance Group When the triggering conditions are met, the instance group will automatically configure machines based on the previously set instance template.\nTo begin, open the Instance Groups and click on Create Instance Group.\nOn the interface, you will see three options on the left.\nThe interface might have changed since the existing online articles were published.\nNew managed instance group (stateless) New managed instance group (stateful) New unmanaged instance group Among these options, only the two under Managed Instance Group have Auto Scaling functionality.\nAdditionally, the Unmanaged Instance Group allows you to add existing VM instances without the need to create an instance template first.\nReference documentation: Using managed instance groups Select the instance template you want to use and specify the Auto Scaling rules. Once done, click on the Create button at the bottom to complete the creation of the instance group and the Auto Scaling configuration.\nConfigure instance group health checks and automatic healing To ensure that newly started instances are available, automatically shut down any non-functional instances, and replace them with new ones, open the instance group\u0026rsquo;s editing page, and scroll down to the Autohealing section.\nSelect Create Health Check and configure it by providing a name and other settings. For example, if you choose the HTTP protocol, you can send requests to specific routes, while the default TCP option simply checks for normal transmission.\nAfter configuring the health check, let it run, and then go back to the instance group\u0026rsquo;s detailed page to check the health report. If everything is fine, it should indicate that everything is healthy without any issues.\nRolling Update When a new image is created, you\u0026rsquo;ll need to create a new instance template first.\nThen, you need to go back to the instance group and edit it to use the new instance template.\nThis ensures that new instances launched in the future will use the updated template.\nModifying the template won\u0026rsquo;t automatically restart existing instances.\nIf you want to achieve seamless updates for online services, you might consider implementing a rolling update.\nTo perform a rolling update, go to the Instance Groups, select the newly created instance group, and look for a button like UPDATE VMS. The button name may vary, so please check for a similar option.\nNext, select the new instance template and in the Update Settings, choose Automatic for the Update type.\nThis ensures that not only new VMs but also existing ones are replaced with the new template.\nYou will see that it starts a new instance using the new instance template.\nOnce the new instance template has been successfully started, it will proceed to delete the old instances.\nThis ensures a seamless upgrade without impacting your online services.\nAt this point, the service is not fully functional. The instance group is still a collection of individual, independent hosts. While it has the ability to automatically scale the number of hosts, it doesn\u0026rsquo;t inherently handle request routing or load balancing.\nIn accordance with the diagram below. Currently, we have only completed the Instance Group and the subsequent steps.\nWe also need to use the Load Balancing service to provide a fixed IP address responsible for receiving all requests.\nThrough Load Balancing, we can distribute traffic to individual hosts within the instance group.\nWe will talk about this in the further post.\n",
    "ref": "/en/blog/202203-gcp-auto-scaling/"
  },{
    "title": "Jenkins CI/CD 01 Installation",
    "date": "",
    "description": "Setup Jenkins CI/CD step by step",
    "body": "Create Your Own Automated Deployment Service with Jenkins Jenkins is a Java-based CI/CD system tool that we are exploring in this tutorial,\nfollowing our previous introduction to Drone If you plan to use Docker for installation and require Docker commands within Jenkins, you\u0026rsquo;ll need to address Docker-in-Docker issues.\nFor those less familiar with Docker and operating systems, it is recommended to follow the method described below, installing Jenkins directly on your host machine.\nBelow, we\u0026rsquo;ll go through the step-by-step installation process as per the official installation documentation First, ensure that you have Java installed:\njava -version sudo apt install openjdk-11-jre Next, install the Jenkins service on your host machine, opting for the stable version (LTS - Long Term Support):\ncurl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc \u0026gt; /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install jenkins Start the Jenkins service using the following command, and then access it via your web browser at localhost:8080.\nYou\u0026rsquo;ll be prompted to enter the initial admin password:\nsudo systemctl enable --now jenkins Retrieve the initial admin password using the following command:\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword Plugin Installation By default, Jenkins does not support pipelines, so you\u0026rsquo;ll need to install a pipeline plugin.\nIn this tutorial, we\u0026rsquo;re using Blue Ocean .\nTo use Docker as an agent in your build process, you\u0026rsquo;ll also need to install two additional extensions:\nDocker plugin Docker Pipeline You can search for and install these plugins in the Jenkins plugin management section:\nAdditionally, ensure that Docker is properly installed on your Jenkins host machine.\nDocker Command Permissions To configure Docker command permissions, first switch to the Jenkins user:\nsudo su jenkins Then, execute any Docker command, e.g., docker ps.\nIf you encounter a permission denied message like:\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post\nAvoid running sudo chmod 777 /var/run/docker.sock.\nThe correct approach is to add the Jenkins user to the Docker user group:\ncat /etc/passwd # List all users sudo groupadd docker # Create a Docker user group (usually created when Docker is installed) sudo usermod -aG docker jenkins # Add the Jenkins user to the Docker group Restart the Jenkins service. You can trigger the restart by entering the following URL in your web browser:\nhttp://jenlins_url/restart\nClick the restart button, and once it\u0026rsquo;s back up, it will load the new configuration.\nNow you can run Docker commands as the Jenkins user.\n",
    "ref": "/en/blog/202203-jenkins-cicd-1-installation/"
  },{
    "title": "Laravel Queue Usage",
    "date": "",
    "description": "Using queues to defer processing of tasks that don't need immediate attention.",
    "body": "The purpose of queues is to defer processing of a time-consuming task that doesn\u0026rsquo;t require immediate completion. Like sending emails, users don\u0026rsquo;t need to wait for the email to be sent successfully before the next step.\nAllowing the application to respond more quickly to web requests and smooth user experience.\nPreparation Laravel supports multiple queue drivers\nDatabase Redis Amazon SQS If testing locally, you can configure it in the .env file as follows:\nQUEUE_CONNECTION=sync This will execute immediately after sending the task, making it more convenient for testing Queue-related code.\nIn the following example, we will demonstrate using database as the driver.\nQUEUE_CONNECTION=database Create Queue table Because Queue is a feature provided by Laravel.\nYou can directly create a database table named Jobs to record pending Queue information using a command.\nphp artisan queue:table php artisan migrate Create Job files You can create it manually or by using a command.\nphp artisan make:job SyncBlockchainNftItemJob At this point, a file will be generated in the specified path: app/Jobs/SyncBlockchainNftItemJob.php\nWrite the Job program logic. Just modify the SyncBlockchainNftItemJob.php we created earlier.\nAnd the main functionality should be written inside the handle() method, like this\npublic function handle() { if ($this-\u0026gt;userId) { Artisan::call(\u0026#34;nft-items:sync $this-\u0026gt;userId\u0026#34;); } } Call an Artisan command to execute something.\nThe required parameters can be initialized in the __construct at the beginning of the Job program file and can be used as input parameters when creating Queue tasks in the future.\nprotected $userId; public function __construct($userId) { $this-\u0026gt;userId = $userId; } Call the Job to create a task. Now that the Job is ready, how do you call it from the Controller?\nAfter including SyncBlockchainNftItemJob, you can use dispatch wherever you want to create a task and assign it to the specified Job.\n$this-\u0026gt;dispatch(new SyncBlockchainNftItemJob($user-\u0026gt;id)); // Alternatively, you can keep it even simpler. SyncBlockchainNftItemJob::dispatch($user-\u0026gt;id); Starting Queue Worker If sync was not used as the driver earlier, the Queue won\u0026rsquo;t execute!\nYou need to use a command to instruct the Queue to start working!\nphp artisan queue:work It\u0026rsquo;s important to note that once Queue Workers are started, they won\u0026rsquo;t automatically update when there are code changes.\nDuring the deployment phase, remember to use a command to restart the Queue worker. Otherwise, it will continue running the old version of the code!\nphp artisan queue:restart Check status ps -ef|grep queue:work Check execution status Executing the Controller\u0026rsquo;s code, you\u0026rsquo;ll notice that when the Queue is triggered, a new record is added to the jobs table.\nAnd during execution, the terminal will display corresponding information.\nIf you see Processed, it means the task has been completed, and at this point, the record in jobs will be removed.\nSupervisor When the Queue is running, various situations can lead to critical errors, preventing the execution of tasks.\nIn such cases, it\u0026rsquo;s recommended by the official documentation to use Supervisor for management.\nIn the event that the Queue unexpectedly stops operating, Supervisor will restart the Queue service based on the configuration file, ensuring that Jobs can run continuously!\nInstall from docker image FROM php:7.4-fpm RUN apt-get install supervisor CMD /var/www/html/_scripts/cron.sh In the last line of our CMD, we executed a cron.sh as the entry point, which will be used later.\nSupervisor Config Place the file wherever you like.\n# supervisord.conf [program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /var/www/html/artisan queue:work --sleep=90 --tries=3 autostart=true autorestart=true startsecs=5 user=root numprocs=3 redirect_stderr=true stdout_logfile=/var/www/html/storage/logs/supervisord.log stopwaitsecs=3600 [supervisord] logfile=/var/log/supervisor/supervisord_main.log pidfile=/var/www/html/storage/logs/supervisord.pid [supervisorctl] Start Supervisor Create the _scripts/cron.sh file in the project and mount it to the container\u0026rsquo;s /var/www/html path.\n#!/usr/bin/env bash cd /var/www/html supervisord -c ./docker/cron/supervisord.conf # Start supervisord using the configuration file located at the specified path php /var/www/html/artisan schedule:work # Simultaneously initiate the cron job. When the Dockerfile executes a shell script via CMD, it can be considered as\nsh _scripts/cron.sh\nIn cron.sh, we\u0026rsquo;ve done three things:\nNavigate to the container\u0026rsquo;s /var/www/html path. Start the supervisord service using the specified config. Execute php artisan schedule:work. This way, both supervisord and the cron job service are initiated.\nCheck Supervisor status You can use a command to confirm whether supervisord is running.\npgrep -fl supervisord # or ps aux|grep supervisord If an error message appears indicating that the ps or pgrep commands do not exist, you will need to install the package using a command.\napt-get update \u0026amp;\u0026amp; apt-get install procps If you see supervisord listed, it means it\u0026rsquo;s already running.\n",
    "ref": "/en/blog/202203-laravel-queue/"
  },{
    "title": "Enhancing SEO for SPA with Prerender",
    "date": "",
    "description": "Addressing SEO challenges for CSR web pages, demonstrating both self-hosted and SaaS solutions",
    "body": "A Single Page Application (SPA) website utilizes Client-Side Rendering (CSR) to render its content.\nUpon initial loading, the server only returns the root component, and subsequent data is fetched through API interactions.\nHowever, for web crawlers responsible for ranking websites, the content obtained during crawling may appear empty.\nWhile Google claims that their crawler executes JavaScript, other search engines may not necessarily do so.\nTo address this issue, a Prerender service is employed to achieve pre-rendering for web crawlers, optimizing search results.\nPrerender The principle involves setting up an internal headless Chrome to achieve pre-rendering of the webpage.\nTwo methods for using Prerender will be introduced below:\nSelf-hosted: prerender SaaS service: Prerender.io For self-hosting, a reverse proxy needs to be established.\nUpon incoming requests, the user agent is checked. If it is a bot, the request is sent to the self-hosted Prerender service.\nThe service itself includes a headless Chrome, allowing rendering of HTML to be returned.\nIf it\u0026rsquo;s a regular user, the request is forwarded to the frontend server without rendering.\nUsing the Prerender.io SaaS service comes with limitations and may require payment if exceeding the free usage limits. However, it eliminates the need for self-management and setup. The advantage of self-hosting is greater flexibility in configuration.\nBoth solutions can be evaluated based on the use case.\nIf opting for the Prerender.io solution, configuration can only be done when the website is already online.\nSelf-hosted Package: Prerender The documentation is quite clear, and the setup is straightforward. Below is a step-by-step demonstration.\n1. Installation Create a folder, for convenience, let\u0026rsquo;s call it prerender.\nInside the folder, enter the command in the terminal:\nnpm install prerender This completes the installation.\n2. Configuration In the prerender folder, create a file named server.js.\nconst prerender = require(\u0026#39;prerender\u0026#39;); const server = prerender(); server.start(); 3. Start Enter the command in the terminal to start the service.\nnode server.js The service will start on localhost at port 3000. If your service is also running on port 3000, choose a different port.\nWhile your frontend project is running, open the terminal and enter:\ncurl \u0026#34;https://www.google.com.tw/\u0026#34; # Your SPA project URL You will receive an empty DOM with only the root component, as the JS for fetching component content has not been executed.\nThis is a reason for poor SEO in SPAs. Although Google claims that their crawler executes JS, other search engines may not.\nNow comes the magic moment. Open the terminal and enter:\ncurl \u0026#34;http://localhost:3000/render?url=https://www.google.com.tw/\u0026#34; # Your SPA project URL You will get a large package of rendered results!\nThe preceding URL is the Prerender service address, and the parameter includes your project URL.\nPrerender internally starts a headless Chrome to render the page and then returns the result.\nCache Configuration Usually, when you self-host a Prerender service, it will re-render the page each time a request is made.\nHowever, in practice, frequent re-rendering is unnecessary.\nThis can be optimized by setting up caching to retain rendered results for a certain period.\nCache package used: prerender-memory-cache 1. Installation In the terminal, enter the command in the prerender folder:\nnpm install prerender-memory-cache --save 2. Configuration Open the server.js file in the prerender folder and add the configuration declaration before server.start();;\nserver.use(require(\u0026#39;prerender-memory-cache\u0026#39;)) That\u0026rsquo;s it. The second request should use the cached result.\n3. Parameters You can also set parameters in the prerender folder using the terminal.\nCache Maximum Number of Items\nexport CACHE_MAXSIZE=1000 default: 100\nCache Lifespan (seconds)\nexport CACHE_TTL=600 default: 60\n4. Testing After setting the lifespan, use the terminal to test:\ncurl \u0026#34;http://localhost:3000/render?url=http://localhost:3030/\u0026#34; # Your SPA project URL 打完第一次之後，去修改自己專案的一些內容（footer, title 之類）\n確認用瀏覽器直接造訪畫面有出現變更\n接著再重新使用終端機測試，應該會出現舊的 HTML 內容\n這就證明他是使用快取的內容，而不是請求一來就重新渲染\n所以剛剛才會建議修改 footer 或是 title，因為比較好確認\nAfter the first test, modify some content in your project (footer, title, etc.) and confirm the changes by directly visiting the page in the browser. Then, retest using the terminal. You should see the old HTML content.\nThis proves that it is using the cached content rather than re-rendering with each request.\nOnline Service Pricing Plans Below is an example using Cloudflare Workers to set up the Prerender.io service.\nThis is because the frontend project seems to have its own Nginx service, and I don\u0026rsquo;t want to intervene too much.\nBesides, I\u0026rsquo;ve heard that Cloudflare Workers are powerful. It\u0026rsquo;s like a proxy at the forefront, so it can do many things.\nAlthough I\u0026rsquo;ve never had the chance to use it, it seems like a good opportunity.\nPrerender.io Pricing Plans Cloudflare Workers Pricing Plans Configuration Go to the Prerender page, click on Install middleware and select the cloudflare option.\nThe official documentation has been updated.\nJust follow the instructions in the document, and even those who can\u0026rsquo;t code can configure it confidently!\nFollow the document and pictures for Cloudflare Integration .\nOpen the sample configuration file as prompted: prerender/prerender-cloudflare-worker .\nYou\u0026rsquo;ll see the index.js example. The only things to change are API_KEY and PRERENDERED_DOMAINS.\nAPI_KEY：The token in the top left corner of the Prerender.io dashboard. PRERENDERED_DOMAINS：The target domain for which routing rules will be set later. Create a Cloudflare worker and paste the configuration file.\nOrdinary Workers can be tested using the provided playground by Cloudflare, but since we\u0026rsquo;re using a third-party service, it must be verified by Prerender to take effect.\nTherefore, testing with the playground here is not possible.\nAfter configuring the worker, open the Worker tab and choose Add route.\nNext, set the Request Limit Failure Mode.\nOpen Failure (Continue) means that if the Worker fails when a request comes in, it will go directly to the service.\nClose Failure (Block) will throw an error page, preventing users from continuing to access.\nSince we don\u0026rsquo;t want the website to be inaccessible when reaching the free plan limit and not pre-rendering content doesn\u0026rsquo;t affect the website\u0026rsquo;s functionality, choose to skip the Worker and execute it directly in case of failure.\nAfter all, it\u0026rsquo;s just for pre-rendering, and it doesn\u0026rsquo;t matter if crawlers can\u0026rsquo;t access it this time; they can come back next time.\nThe routing rules should match the URL you set in the Worker configuration.\nAfter setting it up, remember to go back to Prerender.io and click \u0026ldquo;Next\u0026rdquo; to complete the verification.\nThen, wait for the verification to be completed.\nUsually, after clicking \u0026ldquo;Verify,\u0026rdquo; the verification results will appear after a few seconds.\nIf you want to simulate the result obtained when crawling after verification, you can use:\ncurl -A Googlebot \u0026#34;https://{your-online-service-url}\u0026#34; This simulates the result obtained by the Googlebot crawler.\nSince we specified a specific user agent in the Worker\u0026rsquo;s Nginx configuration file, the Cloudflare Worker will determine whether to forward it to the Prerender.io service for processing.\nWithout the -A Googlebot parameter, it won\u0026rsquo;t forward for rendering.\nIt will return the CSR result that regular users would get, which is the unrendered webpage content.\n",
    "ref": "/en/blog/202201-using-prerender-improve-spa-seo/"
  },{
    "title": "Successfully launched a VM on GCP on the first try",
    "date": "",
    "description": "Step-by-step guide to launching your first VM instance and configuring it on Google Cloud Platform (GCP).",
    "body": "How to launch a VM virtual machine on Google Cloud Platform (GCP)?\nIt may sound complex, but in practice, it\u0026rsquo;s quite straightforward.\nIf you haven\u0026rsquo;t had any experience with setting up a host before, this time, we\u0026rsquo;ll document the steps for you.\nCreate a new VM instance There\u0026rsquo;s not much to be concerned about. Once you\u0026rsquo;re in the GCP console and on the VM instances page, simply choose the desired specifications, give it a name, and click \u0026lsquo;Create\u0026rsquo;.\nAssigning a static external IP address To ensure that our service has a consistent IP address, we can change the external IP type from ephemeral to static.\nPlease note that converting an ephemeral IP to a static IP may incur additional charges.\nFirst, select the VM instance you want to modify.\nClick on the ellipsis menu (the three dots), and choose View network details.\nNext, from the left-hand menu, select the \u0026lsquo;External IP addresses\u0026rsquo; tab.\nIn the Users column, locate the VM instance you just created.\nYou\u0026rsquo;ll notice that the Name field for the target is currently empty.\nClick on Reserve on the left-hand side, and at this point, it will prompt you to enter a name.\nOnce you\u0026rsquo;ve provided a name, your VM instance\u0026rsquo;s IP address will remain unchanged even after service restarts.\nOpening external ports Your virtual machine is up and running, and you\u0026rsquo;ve also secured a static external IP.\nNow, it\u0026rsquo;s time to establish a connection!\nYou enter the IP address in your browser and\u0026hellip; can\u0026rsquo;t access it! Huh?\nCommonly used network service ports By default, when you create a VM, only port 22 for SSH connections is open.\nIf you set up firewall rules during the creation, it will apply the http-server and https-server network tags, allowing your VM to receive requests on ports 80 and 443.\nYou can check the \u0026lsquo;Network tags\u0026rsquo; section in the VM\u0026rsquo;s information page.\nIf you find that these ports are not open, and you wish to enable them, click on the \u0026lsquo;Edit\u0026rsquo; button for the VM, and navigate to the Firewall settings.\nSelect the http-server and https-server rules that were not checked during initial setup.\nAfter successfully applying these changes, your VM will be able to receive incoming requests on ports 80 and 443.\nOther port If your service requires the use of a different port, you will need to configure it manually.\nLet\u0026rsquo;s use port 8080 as an example for configuration.\nCreate a firewall rule Select the VM instance you want to modify, and from the rightmost three-dot menu, choose \u0026lsquo;View network details\u0026rsquo; to access the \u0026lsquo;Virtual Private Cloud network.\nThen, on the left-hand menu, select \u0026lsquo;Firewall\u0026rsquo; and click on \u0026lsquo;Create firewall rule\u0026rsquo;.\nFor the most part, you can use the default values. Determine if you need to customize based on the provided explanations.\nThere are only a few areas that require adjustments.\nTarget tags These are network tags used when selecting firewall rules later on.\nThey serve the same purpose as the http-server tag when configuring port 80, acting as identifiers.\nSource IPv4 range This is where you specify trusted IP locations.\nIf you want to make it publicly accessible, set it to 0.0.0.0/0 to accept requests from all sources.\nProtocol and Ports To demonstrate opening multiple ports simultaneously, we\u0026rsquo;ve added port 8089.\nThis allows you to specify the external availability of both TCP ports 8080 and 8089.\nYou can observe that ports are separated by a comma ,.\nIf you use a dash -, you can specify a range of ports to be opened.\nApplying the Firewall Rule Go back to Compute Engine and select the VM instance where you want to apply the rule. Click the \u0026lsquo;Edit\u0026rsquo; button.\nManually enter the Target tags you filled out when creating the firewall rule in the Network tags field.\nAfter saving, the rule will be applied.\n",
    "ref": "/en/blog/202201-gcp-vm-basic/"
  },{
    "title": "[A Tour of Go Study Notes] 09 concurrency",
    "date": "",
    "description": "Understanding Go concurrency using the official tutorial",
    "body": "Entering the world of Golang, we first explore basic Golang usage using the official tutorial: A Tour of Go .\nThis section introduces the powerful concurrency feature of Go.\nGoroutine Lightweight thread management operated by Go runtime.\nStarting and executing a new Goroutine can be understood as creating a new Thread.\ngo f(x, y, z) Using go followed by a function call enables the specified function f to run on a new Goroutine.\nf, x, y, z are taken from the current Goroutine, and main also executes within the same thread.\nIn the example below, say(\u0026quot;world\u0026quot;) will start another execution sequence concurrently with the original one executing say(\u0026quot;hello\u0026quot;).\nUsually, say(\u0026quot;world\u0026quot;) will execute first, followed by say(\u0026quot;hello\u0026quot;) upon completion.\nHowever, if go is added before say(\u0026quot;world\u0026quot;), changing it to go say(\u0026quot;world\u0026quot;), it will initiate a new Goroutine while the original sequence continues with say(\u0026quot;hello\u0026quot;).\nBoth threads will run concurrently, and after the main Goroutine finishes execution, other Goroutines will be forcefully closed.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func say(s string) { for i := 0; i \u0026lt; 5; i++ { time.Sleep(100 * time.Millisecond) fmt.Println(s) } } func main() { go say(\u0026#34;world\u0026#34;) say(\u0026#34;hello\u0026#34;) } /* \u0026gt;\u0026gt;\u0026gt; world \u0026gt;\u0026gt;\u0026gt; hello \u0026gt;\u0026gt;\u0026gt; world \u0026gt;\u0026gt;\u0026gt; hello \u0026gt;\u0026gt;\u0026gt; hello \u0026gt;\u0026gt;\u0026gt; world \u0026gt;\u0026gt;\u0026gt; world \u0026gt;\u0026gt;\u0026gt; hello \u0026gt;\u0026gt;\u0026gt; hello */ Channels Channels are typed communications that use \u0026lt;- to send or receive values. They possess a blocking nature, enabling threads to wait.\nSimilar to maps and slices, channels must be created before use.\nch := make(chan int) As channels function like pipelines, sending or receiving halts until the other end is ready.\nThey can be used for synchronizing data content among Goroutines.\nch \u0026lt;- v // Send v to channel ch v := \u0026lt;-ch // Receive a value from ch and assign it to v The following example demonstrates summing numbers in a slice, distributing the task among two Goroutines.\nThe final result is computed only when both Goroutines complete their calculations.\npackage main import \u0026#34;fmt\u0026#34; func sum(s []int, ch chan int) { sum := 0 for _, v := range s { sum += v } ch \u0026lt;- sum // send sum to ch } func main() { s := []int{7, 2, 8, -9, 4, 0} ch := make(chan int) go sum(s[:len(s)/2], ch) go sum(s[len(s)/2:], ch) x, y := \u0026lt;-ch, \u0026lt;-ch // receive from ch fmt.Println(x, y, x+y) } /* \u0026gt;\u0026gt;\u0026gt; -5 17 12 */ Buffered Channels Channels can have a buffer, where the buffer length is provided as the second argument during channel initialization.\nRegular channels, as mentioned earlier, exhibit:\nBlocking the sender when data is pushed without a receiver. Blocking the receiver when pulling from an empty channel. With buffering, blocking occurs only when the buffer is full.\nIn the following example, only after the 101st data is pushed does the sender Goroutine start waiting:\nch := make(chan int, 100) The buffer length determines how many elements the channel can store.\nSending more elements than the buffer length leads to a deadlock.\npackage main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan int, 1) ch \u0026lt;- 1 ch \u0026lt;- 2 fmt.Println(\u0026lt;-ch) fmt.Println(\u0026lt;-ch) } /* \u0026gt;\u0026gt;\u0026gt; fatal error: all goroutines are asleep - deadlock! */ No error occurs if values are retrieved.\npackage main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan int, 1) ch \u0026lt;- 1 fmt.Println(\u0026lt;-ch) ch \u0026lt;- 2 fmt.Println(\u0026lt;-ch) } /* \u0026gt;\u0026gt;\u0026gt; 1 \u0026gt;\u0026gt;\u0026gt; 2 */ Range and Close Senders can close a channel to signify no more values to be sent.\nReceivers can test if a channel is closed using the second argument.\nIf there are no values to receive and the channel is closed, ok is set to false.\nWhen a channel is iterated using a for loop like for i := range c, it continuously receives values until the channel is closed.\npackage main import ( \u0026#34;fmt\u0026#34; ) func fibonacci(n int, c chan int) { x, y := 0, 1 for i := 0; i \u0026lt; n; i++ { c \u0026lt;- x x, y = y, x+y } close(c) } func main() { c := make(chan int, 10) go fibonacci(cap(c), c) for i := range c { fmt.Println(i) } } /* \u0026gt;\u0026gt;\u0026gt; 0 \u0026gt;\u0026gt;\u0026gt; 1 \u0026gt;\u0026gt;\u0026gt; 1 \u0026gt;\u0026gt;\u0026gt; 2 \u0026gt;\u0026gt;\u0026gt; 3 \u0026gt;\u0026gt;\u0026gt; 5 \u0026gt;\u0026gt;\u0026gt; 8 \u0026gt;\u0026gt;\u0026gt; 13 \u0026gt;\u0026gt;\u0026gt; 21 \u0026gt;\u0026gt;\u0026gt; 34 */ Sending data to a closed channel will cause a panic.\nThus, the channel should be closed by the Goroutine that is sending data.\nAdditionally, channels differ from files; typically, they don’t need manual closing.\nClosing is only necessary when it\u0026rsquo;s crucial to inform receivers that no new values will be sent, such as in a range loop.\nSelect Using select, various scenarios involving channels can be handled, including handling during blocking.\nselect { case i := \u0026lt;-c: // use i default: // receiving from c would block } When none of the cases match, the default case executes.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { tick := time.Tick(100 * time.Millisecond) boom := time.After(500 * time.Millisecond) for { select { case \u0026lt;-tick: fmt.Println(\u0026#34;tick.\u0026#34;) case \u0026lt;-boom: fmt.Println(\u0026#34;BOOM!\u0026#34;) return default: fmt.Println(\u0026#34; .\u0026#34;) time.Sleep(50 * time.Millisecond) } } } /* \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; tick. \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; tick. \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; tick. \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; tick. \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; . \u0026gt;\u0026gt;\u0026gt; tick. \u0026gt;\u0026gt;\u0026gt; BOOM! */ sync.Mutex Previously, we used channels to facilitate communication and data transfer between Goroutines. Due to their blocking\nnature, when pulling data without any, it waits for other Goroutines to push data into the channel.\nHowever, what if we don’t use channels and directly share variables among multiple Goroutines?\nWhen multiple Goroutines share a variable and simultaneously operate on it, a Race Condition occurs, where ensuring the correctness of computation results becomes challenging.\nThis involves the concept of mutual exclusion, which can be handled using a Mutex to prevent conflicts.\nGo provides a struct: sync.Mutex , offering two methods:\nLock Unlock Operations between Lock and Unlock make other Goroutines wait.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) // SafeCounter is safe to use concurrently. type SafeCounter struct { mu sync.Mutex v map[string]int } // Inc increments the counter for the given key. func (c *SafeCounter) Inc(key string) { c.mu.Lock() // Lock so only one goroutine at a time can access the map c.v. c.v[key]++ c.mu.Unlock() } // Value returns the current value of the counter for the given key. func (c *SafeCounter) Value(key string) int { c.mu.Lock() // Lock so only one goroutine at a time can access the map c.v. defer c.mu.Unlock() return c.v[key] } func main() { c := SafeCounter{v: make(map[string]int)} for i := 0; i \u0026lt; 1000; i++ { go c.Inc(\u0026#34;somekey\u0026#34;) } time.Sleep(time.Second) fmt.Println(c.Value(\u0026#34;somekey\u0026#34;)) } /* \u0026gt;\u0026gt;\u0026gt; 1000 */ ",
    "ref": "/en/blog/202112-a-tour-of-go-09-concurrency/"
  },{
    "title": "[A Tour of Go Study Notes] 08 interface and error",
    "date": "",
    "description": "Understanding Go interface and error using official tutorials",
    "body": "Entering the world of Golang, we first explore the basics of Golang using the official tutorial: A Tour of Go .\nThis piece introduces interfaces and errors.\nInterface The concept is somewhat akin to PHP\u0026rsquo;s implementation .\nDefining the function name, accepted parameters with types, and the returned value along with its type.\nRather than defining implementation methods, it\u0026rsquo;s up to individual objects inheriting to implement them.\nHowever, Go lacks an inheritance mechanism; there\u0026rsquo;s no need to explicitly use the implement keyword to declare which interface it implements.\nRegardless of the data type, as long as it implements the definitions within an interface, it automatically implements that interface.\nAn interface type defines a set of methods, and if an object implements all the methods of an interface, then that object implements the interface itself.\npackage main import \u0026#34;fmt\u0026#34; type I interface { M() } type T struct { S string } // This method means type T implements the interface I, // but we don\u0026#39;t need to explicitly declare that it does so. func (t T) M() { fmt.Println(t.S) } func main() { var i I = T{\u0026#34;hello\u0026#34;} i.M() } /* \u0026gt;\u0026gt;\u0026gt; hello */ This feature is also known as Duck typing .\n\u0026ldquo;When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.\u0026rdquo;\nIt can be explained that when an object implements all the definitions of a duck interface, it\u0026rsquo;s considered a duck.\nError In Go, error is used to represent error states.\nSimilar to fmt.Stringer, the error type is a built-in interface.\ntype error interface { Error() string } Usually, functions return an error, and checking if the value is nil is used to handle errors.\ni, err := strconv.Atoi(\u0026#34;42\u0026#34;) if err != nil { fmt.Printf(\u0026#34;couldn\u0026#39;t convert number: %v\\n\u0026#34;, err) return } fmt.Println(\u0026#34;Converted integer:\u0026#34;, i) When the error is nil, it signifies success; a non-nil error indicates a failure.\n",
    "ref": "/en/blog/202112-a-tour-of-go-08-interface-and-error/"
  },{
    "title": "[A Tour of Go Study Notes] 07 method",
    "date": "",
    "description": "Understanding Go method using official tutorials",
    "body": "Entering the world of Golang, we first acquaint ourselves with basic Golang usage using the official tutorial: A Tour of Go .\nThis article introduces methods in Go.\nMethod Go is not strictly object-oriented and doesn\u0026rsquo;t have classes.\nHowever, it can achieve similar functionality using Type in conjunction with a receiver parameter.\nMethods are functions with a special receiver parameter.\nThe receiver appears in its own parameter list between the func declaration and the method name.\nIn the example, the Abs function has a receiver of type Vertex named v.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) type Vertex struct { X, Y float64 } func (v Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y) } func main() { v := Vertex{3, 4} fmt.Println(v.Abs()) } /* \u0026gt;\u0026gt;\u0026gt; 5 */ Methods Are Functions A method is just a function with a receiver.\nBelow is a regular function with no significant change in functionality:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) type Vertex struct { X, Y float64 } func Abs(v Vertex) float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y) } func main() { v := Vertex{3, 4} fmt.Println(Abs(v)) } /* \u0026gt;\u0026gt;\u0026gt; 5 */ Method declarations can also use non-struct types.\nOnly types declared within the same package can be used as receivers.\nEven built-in types like int are disallowed if declared in another package.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) type MyFloat float64 func (f MyFloat) Abs() float64 { if f \u0026lt; 0 { return float64(-f) } return float64(f) } func main() { f := MyFloat(-math.Sqrt2) fmt.Println(f.Abs()) } /* \u0026gt;\u0026gt;\u0026gt; 1.4142135623730951 */ Pointer Receivers You can also declare methods with pointer receivers.\nAdding * before the type indicates a pointer receiver, where the receiver type of the method is a pointer.\nIn the example, the function Scale has a receiver declared as *Vertex.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) type Vertex struct { X, Y float64 } func (v *Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y) } func (v *Vertex) Scale(f float64) { v.X = v.X * f v.Y = v.Y * f } func main() { v := Vertex{3, 4} v.Scale(10) fmt.Println(v.Abs()) } /* \u0026gt;\u0026gt;\u0026gt; 50 */ Methods with pointer receivers can directly modify the value they point to, not just perform operations.\nBecause methods often modify the receiver, pointers are commonly used as receivers.\nIf you remove the * from the Scale receiver in the above example, turning it into a regular method, it would make a copy of the Vertex value and operate on it.\nIf executed, the result would be 5, as v.Scale(10) wouldn\u0026rsquo;t affect the result obtained from v := Vertex{3, 4} directly printed as 5.\nPointers and Functions Now, let\u0026rsquo;s rewrite Abs and Scale methods as functions.\nPreviously, v.Scale(10) could directly call a pointer receiver, but function parameters must use the \u0026amp; symbol to declare a pointer.\npackage main import \u0026#34;fmt\u0026#34; type Vertex struct { X, Y float64 } func (v *Vertex) Scale(f float64) { v.X = v.X * f v.Y = v.Y * f } func ScaleFunc(v *Vertex, f float64) { v.X = v.X * f v.Y = v.Y * f } func main() { v := Vertex{3, 4} v.Scale(2) ScaleFunc(\u0026amp;v, 10) p := \u0026amp;Vertex{4, 3} p.Scale(3) ScaleFunc(p, 8) fmt.Println(v, p) } /* \u0026gt;\u0026gt;\u0026gt; {60 80} \u0026amp;{96 72} */ Comparing with the previous two examples, you\u0026rsquo;ll notice that functions with parameters must receive a pointer, or else:\nvar v Vertex ScaleFunc(v, 5) // Compile error! ScaleFunc(\u0026amp;v, 5) // OK But methods with a pointer receiver can be directly called with a value or pointer:\nvar v Vertex v.Scale(5) // OK p := \u0026amp;v p.Scale(10) // OK For v.Scale(5), even though v is a value rather than a pointer, a method with a pointer receiver can still be called.\nEssentially, Go interprets v.Scale(5) as (\u0026amp;v).Scale(5) for convenience.\nThe same happens in reverse; if a function with parameters expects a value, and a pointer is passed:\nvar v Vertex fmt.Println(AbsFunc(v)) // OK fmt.Println(AbsFunc(\u0026amp;v)) // Compile error! But when used as a method, both values and pointers can be passed directly:\nvar v Vertex fmt.Println(v.Abs()) // OK p := \u0026amp;v fmt.Println(p.Abs()) // OK Because p.Abs() is interpreted as (*p).Abs() in this case.\nChoosing a Receiver Type (Value/Pointer) Typically, using a \u0026ldquo;pointer\u0026rdquo; as a receiver is chosen because:\nMethods can modify the value they point to. Avoids copying data on every method call, which is beneficial for large struct types. In the example, Scale and Abs have receivers of type *Vertex.\nEven though Abs doesn\u0026rsquo;t need to modify the receiver, generally, all methods with a type should have either value or pointer receivers but not mix both.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) type Vertex struct { X, Y float64 } func (v *Vertex) Scale(f float64) { v.X = v.X * f v.Y = v.Y * f } func (v *Vertex) Abs() float64 { return math.Sqrt(v.X*v.X + v.Y*v.Y) } func main() { v := \u0026amp;Vertex{3, 4} fmt.Printf(\u0026#34;Before scaling: %+v, Abs: %v\\n\u0026#34;, v, v.Abs()) v.Scale(5) fmt.Printf(\u0026#34;After scaling: %+v, Abs: %v\\n\u0026#34;, v, v.Abs()) } /* \u0026gt;\u0026gt;\u0026gt; Before scaling: \u0026amp;{X:3 Y:4}, Abs: 5 \u0026gt;\u0026gt;\u0026gt; After scaling: \u0026amp;{X:15 Y:20}, Abs: 25 /* ",
    "ref": "/en/blog/202112-a-tour-of-go-07-method/"
  },{
    "title": "Importing Excel or CSV Files into Laravel",
    "date": "",
    "description": "Utilize the Laravel-excel package to import Excel or CSV files and write data to the database.",
    "body": "When the backend interface isn\u0026rsquo;t fully developed or when users have extensive data that they prefer not to input manually, opting for a bulk import via a form becomes essential.\nThis is where the Laravel-excel package comes into play. Additionally, it\u0026rsquo;s worth discussing the challenges encountered when importing a large volume of data at once.\nCurrently, the most widely adopted package among developers is Laravel-excel .\nIt supports most file formats commonly used during import processes and encompasses numerous essential functionalities.\nThe official documentation is well-written and provides clear examples within the code snippets.\nInstallation Install using the command, which auto-discovers and registers into the ServiceProvider and Facade.\nThe current latest version is: Documentation 3.1 composer require maatwebsite/excel If you need to add a custom config, execute the command to generate config/excel.php\ncomposer require maatwebsite/excel php artisan vendor:publish --provider=\u0026#34;Maatwebsite\\Excel\\ExcelServiceProvider\u0026#34; --tag=config Defining File Reading Formats Because Laravel\u0026rsquo;s built-in collection type is incredibly useful, we aim to convert the imported form data into a collection format.\nFirst, create an object definition for the data to be loaded.\nLet\u0026rsquo;s assume we\u0026rsquo;re importing CSV files for product specifications, so we\u0026rsquo;ll create a file named app/Imports/ProductImport.php\n\u0026lt;?php namespace App\\Imports; use Illuminate\\Support\\Collection; use Maatwebsite\\Excel\\Concerns\\ToCollection; class ProductImport implements ToCollection { public $collection; public function collection(Collection $collection) { $this-\u0026gt;collection = $collection-\u0026gt;transform(function ($row) { return collect([ \u0026#39;name\u0026#39; =\u0026gt; $row[1], \u0026#39;main_image\u0026#39; =\u0026gt; $row[2], \u0026#39;author_name\u0026#39; =\u0026gt; $row[3], \u0026#39;material\u0026#39; =\u0026gt; $row[4], \u0026#39;specification\u0026#39; =\u0026gt; $row[5], \u0026#39;description\u0026#39; =\u0026gt; $row[6], ]); }); } } As you can see, we\u0026rsquo;ve used the ToCollection interface and defined the correspondence between collection and the content read from the form.\nReading CSV Files Within the controller, call the Excel that was previously registered via auto-discovery through the Facade to read the uploaded file.\nFirstly, create an object named ProductImport.\nThen, use the Facade to invoke the Excel package and employ its loading functionality. Once loaded, utilize the shift() method of the collection to remove the header data from the first row.\npublic function uploadCsv(Request $request) { $import = new ProductImport(); Excel::import($import, $request-\u0026gt;file(\u0026#39;file\u0026#39;)); $productCollection = $import-\u0026gt;collection; $productCollection-\u0026gt;shift(); } This way, you can load the uploaded CSV file into a collection for further operations.\nUploading Testing For testing with Postman, simply select form-data in the Body tab.\nIn the \u0026lsquo;key\u0026rsquo; field, you can select the \u0026lsquo;file\u0026rsquo; format from the dropdown menu.\nCommon Issues 413 Payload Too Large This error originates from Nginx and implies that the uploaded file is too large, causing the server to reject it.\nReference: 413 Payload Too Large Solution using PHP development environment as an example:\nAdjusting Nginx Config By default, it\u0026rsquo;s set to 2M, meaning files larger than this require adjustments.\nOpen nginx.conf and add this within the server block:\nclient_max_body_size 10M; This allows requests larger than 2M to pass to Nginx for further PHP processing.\nAdjusting php.ini php.ini holds various PHP configurations.\nWe\u0026rsquo;re interested in modifying upload_max_filesize, initially set to 2M, to match Nginx:\nupload_max_filesize = 10M; When using Docker, after creating a custom php.ini file, mount it to the designated location within the container:\n- ./php/config/php.ini:/usr/local/etc/php/conf.d/php.ini Once these adjustments are made, restart or reload the settings for the containers that were modified.\nEnter the container to ensure the configurations are applied — use phpinfo() for PHP and nginx -T for Nginx.\nError: 1390 too many placeholders After successfully uploading the file and starting the program, everything seems to execute as expected during data checks.\nHowever, when constructing SQL statements using query builders, an error pops up:\n1390 Prepared statement contains too many placeholders\nThis MySQL error arises from our use of Prepared statement , which supports only 65535 (2^16-1) parameters.\nIf you attempt to insert a vast amount of data exceeding this limit, whether in columns or rows, the assembly fails, resulting in this error.\nNow that we understand the cause, the solution is evident: avoid inserting too much data at once\u0026hellip; pretty straightforward.\nThe solution involves breaking down a large dataset, like an array containing ten thousand product entries, into multiple arrays of 500 entries each.\nThen, execute the insertion or modification in chunks of 500 entries to circumvent the \u0026rsquo;too many placeholders\u0026rsquo; issue.\nIn PHP, we\u0026rsquo;ll use array_chunk to split:\n$productChunks = array_chunk($products, 500); After execution, you\u0026rsquo;ll obtain multiple arrays, each containing 500 entries from the original dataset.\nThen, iterate through each set of 500 entries, resulting in 20 iterations of 500 entries each, instead of a single loop handling 10,000 entries.\nThis method illustrates how to use the Laravel-excel package to read CSV files and write into a database. Exporting files will be covered in a future example with a practical case study.\n",
    "ref": "/en/blog/202112-laravel-excel-import/"
  },{
    "title": "[A Tour of Go Study Notes] 06 Range, Map, and Function",
    "date": "",
    "description": "Understanding Go Range, Map, and Function using official tutorials",
    "body": "Entering the world of Golang, we first acquaint ourselves with basic Golang usage using the official tutorial: A Tour of Go .\nThis article introduces Range, Map, and Function.\nRange The range form of a for loop can iterate over a slice or map, similar to the foreach usage in other languages.\nWhen iterating over a slice with a for loop, each iteration returns two values.\nThe first value is the element\u0026rsquo;s index, and the second value is a copy of the element\u0026rsquo;s content.\npackage main import \u0026#34;fmt\u0026#34; var pow = []int{1, 2, 4, 8, 16, 32, 64, 128} func main() { for i, v := range pow { fmt.Printf(\u0026#34;2**%d = %d\\n\u0026#34;, i, v) } } /* \u0026gt;\u0026gt;\u0026gt; 2**0 = 1 \u0026gt;\u0026gt;\u0026gt; 2**1 = 2 \u0026gt;\u0026gt;\u0026gt; 2**2 = 4 \u0026gt;\u0026gt;\u0026gt; 2**3 = 8 \u0026gt;\u0026gt;\u0026gt; 2**4 = 16 \u0026gt;\u0026gt;\u0026gt; 2**5 = 32 \u0026gt;\u0026gt;\u0026gt; 2**6 = 64 \u0026gt;\u0026gt;\u0026gt; 2**7 = 128 */ If a value isn\u0026rsquo;t needed (usually the index), it can be ignored using _.\nGolang is sensitive to defining variables that aren\u0026rsquo;t used, which causes an error: key declared but not used.\nfor i, _ := range pow for _, value := range pow If only the index is needed, the second variable can be ignored directly.\nfor i := range pow Here\u0026rsquo;s an example that only retrieves the index:\npackage main import \u0026#34;fmt\u0026#34; func main() { pow := make([]int, 10) for i := range pow { pow[i] = 1 \u0026lt;\u0026lt; uint(i) // Sets the value to left shift the index, equivalent to 2 to the power of i } for _, value := range pow { fmt.Printf(\u0026#34;%d\\n\u0026#34;, value) } } /* \u0026gt;\u0026gt;\u0026gt; 1 \u0026gt;\u0026gt;\u0026gt; 2 \u0026gt;\u0026gt;\u0026gt; 4 \u0026gt;\u0026gt;\u0026gt; 8 \u0026gt;\u0026gt;\u0026gt; 16 \u0026gt;\u0026gt;\u0026gt; 32 \u0026gt;\u0026gt;\u0026gt; 64 \u0026gt;\u0026gt;\u0026gt; 128 \u0026gt;\u0026gt;\u0026gt; 256 \u0026gt;\u0026gt;\u0026gt; 512 */ Map In previous examples, whether an array or slice, numeric values were used as indexes, while maps can use strings as indexes. They are used in a Key-Value combination.\nWhen a map is uninitialized or lacks assignments, the initial value of the map (zero value) is nil.\nA nil map lacks keys and cannot have keys added.\nUsing the make function initializes a map of a specified type and returns it.\npackage main import \u0026#34;fmt\u0026#34; type Vertex struct { Lat, Long float64 } var m map[string]Vertex func main() { m = make(map[string]Vertex) m[\u0026#34;Bell Labs\u0026#34;] = Vertex{ 40.68433, -74.39967, } fmt.Println(m[\u0026#34;Bell Labs\u0026#34;]) } /* \u0026gt;\u0026gt;\u0026gt; {40.68433 -74.39967} */ Maps are similar to structs, but keys are mandatory.\npackage main import \u0026#34;fmt\u0026#34; type Vertex struct { Lat, Long float64 } var m = map[string]Vertex{ \u0026#34;Bell Labs\u0026#34;: {40.68433, -74.39967}, \u0026#34;Google\u0026#34;: {37.42202, -122.08408}, } func main() { fmt.Println(m) } /* \u0026gt;\u0026gt;\u0026gt; map[Bell Labs:{40.68433 -74.39967} Google:{37.42202 -122.08408}] */ If the top-level type is just a type name, it can be omitted.\npackage main import \u0026#34;fmt\u0026#34; type Vertex struct { Lat, Long float64 } var m = map[string]Vertex{ \u0026#34;Bell Labs\u0026#34;: {40.68433, -74.39967}, \u0026#34;Google\u0026#34;: {37.42202, -122.08408}, } func main() { fmt.Println(m) } /* \u0026gt;\u0026gt;\u0026gt; map[Bell Labs:{40.68433 -74.39967} Google:{37.42202 -122.08408}] */ Modifying Values in Maps Inserting or modifying elements in a map:\nm[key] = elem Retrieving an element:\nelem = m[key] Deleting an element:\ndelete(m, key) Checking if a certain key exists using two-value assignment:\nelem, ok = m[key] If the key exists in m, ok = true; otherwise, ok = false.\nWhen reading a non-existent key, you get the default value of the map\u0026rsquo;s element type.\nIn cases where elem or ok aren\u0026rsquo;t defined during usage, : can be used for short variable declaration.\nelem, ok := m[key] In practice:\npackage main import \u0026#34;fmt\u0026#34; func main() { m := make(map[string]int) m[\u0026#34;Answer\u0026#34;] = 42 fmt.Println(\u0026#34;The value:\u0026#34;, m[\u0026#34;Answer\u0026#34;]) m[\u0026#34;Answer\u0026#34;] = 48 fmt.Println(\u0026#34;The value:\u0026#34;, m[\u0026#34;Answer\u0026#34;]) delete(m, \u0026#34;Answer\u0026#34;) fmt.Println(\u0026#34;The value:\u0026#34;, m[\u0026#34;Answer\u0026#34;]) v, ok := m[\u0026#34;Answer\u0026#34;] fmt.Println(\u0026#34;The value:\u0026#34;, v, \u0026#34;Present?\u0026#34;, ok) } /* \u0026gt;\u0026gt;\u0026gt; The value: 42 \u0026gt;\u0026gt;\u0026gt; The value: 48 \u0026gt;\u0026gt;\u0026gt; The value: 0 \u0026gt;\u0026gt;\u0026gt; The value: 0 Present? false */ Function Function values Functions are also values and can be passed around. They can be used as parameters or return values for functions.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func compute(fn func(float64, float64) float64) float64 { return fn(3, 4) } func main() { hypot := func(x, y float64) float64 { return math.Sqrt(x*x + y*y) } fmt.Println(hypot(5, 12)) fmt.Println(compute(hypot)) fmt.Println(compute(math.Pow)) } /* \u0026gt;\u0026gt;\u0026gt; 13 \u0026gt;\u0026gt;\u0026gt; 5 \u0026gt;\u0026gt;\u0026gt; 81 */ Function Closures A Go function can also be a closure. A closure is a function value that references variables outside its body.\nThe closure function is assigned to a variable and can access the variables.\nIn other words, closure functions are \u0026ldquo;bound\u0026rdquo; to variables.\nIn the example, the adder function returns a closure, so each sum variable has its own closure function.\npackage main import \u0026#34;fmt\u0026#34; func adder() func(int) int { sum := 0 return func(x int) int { sum += x return sum } } func main() { pos, neg := adder(), adder() for i := 0; i \u0026lt; 10; i++ { fmt.Println( pos(i), neg(-2*i), ) } } /* \u0026gt;\u0026gt;\u0026gt; 0 0 \u0026gt;\u0026gt;\u0026gt; 1 -2 \u0026gt;\u0026gt;\u0026gt; 3 -6 \u0026gt;\u0026gt;\u0026gt; 6 -12 \u0026gt;\u0026gt;\u0026gt; 10 -20 \u0026gt;\u0026gt;\u0026gt; 15 -30 \u0026gt;\u0026gt;\u0026gt; 21 -42 \u0026gt;\u0026gt;\u0026gt; 28 -56 \u0026gt;\u0026gt;\u0026gt; 36 -72 \u0026gt;\u0026gt;\u0026gt; 45 -90 */ This example demonstrates that the adder function returns a closure, allowing each sum variable to possess its own enclosed function. Within the main function, pos and neg leverage two distinct closure functions for operations.\n",
    "ref": "/en/blog/202112-a-tour-of-go-06-range-and-map/"
  },{
    "title": "Operating Google Forms with GAS (Google App Script)",
    "date": "",
    "description": "Leveraging GAS (Google App Script) services to utilize Google Forms as a simplified database.",
    "body": "When the back office isn\u0026rsquo;t fully established but the frontend is already live, there\u0026rsquo;s a desire to enable non-programmers to autonomously adjust the website\u0026rsquo;s content.\nAlternatively, in cases where temporary content needs to be recorded without creating a dedicated database table, we often resort to managing this data through Google Forms.\nLet\u0026rsquo;s be clear from the outset: these practices are all makeshift solutions!\nGoogle Forms itself comes with quite a few limitations; if you use it as a formal database, it will bring you a lots of troubles.\nIf it doesn\u0026rsquo;t, it means your service is too trivial to even measure.\nStarting small isn\u0026rsquo;t a problem in the early stages. The flexibility it offers allows non-programmers to modify content themselves before the formal editing interface is in place, saving the hassle of relying on tech personnel to deploy new program versions.\nThere are two ways to retrieve data from Google Forms.\nOne method involves publishing the form online, allowing access to specified output types of form content via a URL, commonly exporting as a CSV file.\nAdvantages:\nSimple operation, everyone can do it. No need for additional programming; once set up, data can be accessed directly through the URL. Disadvantages:\nPublished content gets cached by Google: despite claims of updating after modifications, actual updates may take roughly three to five minutes. If your content requires real-time access for the latest data for decision-making, issues may arise. As for the second method, it\u0026rsquo;s aimed at resolving the aforementioned drawbacks, for which we\u0026rsquo;ll utilize Google\u0026rsquo;s GAS (Google App Script) service.\n1. Creating a Google Form Since we\u0026rsquo;re discussing \u0026ldquo;operating Google Forms,\u0026rdquo; it makes perfect sense to create a Google Form, right?\nSet it as \u0026lsquo;Shared for Viewing.\u0026rsquo;\n2. Setting Up a New Google App Script Project You can create one on the Google App Script Projects page , or directly access it within Google Sheets by opening the Extensions menu.\n3. Writing Google Script Now that we have the Google Form ready and know where to write the code for operations.\nThe next step is to craft the script for execution. While writing, refer to the official reference documentation .\nFind the necessary functions in the documentation to operate the Google Form.\nExplanation of Shared Parameters Here are some commonly used functions explained together:\ne.parameter Retrieves all incoming parameters that subsequent program operations can utilize.\nTo use:\nlet params = e.parameter params.sheetTag Refers to the name of the bottommost worksheet in Google Sheets.\nThis can be passed as a variable to simultaneously write to or read from multiple different worksheets.\nUsage involves calling sheetTag after obtaining all incoming variables:\nlet sheetTag = params.sheetTag params.sheetId Represents the ID of the Google Form, used for identification.\nPassing this as a parameter helps differentiate between the formal or testing environment.\nUse your own ID; I\u0026rsquo;ve removed it from this document!\nUsage includes calling sheetId after retrieving all incoming variables:\nlet sheetId = params.sheetId Data Retrieval Next, let\u0026rsquo;s discuss how to retrieve data.\nThe function used for fetching form data must be named deGet, where all get methods funnel into this function.\nRefer to the built-in methods for:\nRetrieving the last row and column numbers. Obtaining all data within a specified range. Returning data in JSON format. You can explore these inherent methods by referencing the aforementioned official documentation.\nfunction doGet(e) { let params = e.parameter let sheetTag = params.sheetTag let sheetId = params.sheetId let SpreadSheet = SpreadsheetApp.openById(sheetId) let Sheet = SpreadSheet.getSheetByName(sheetTag) let lastRow = Sheet.getLastRow() let lastColumn = Sheet.getLastColumn() // getSheetValues(startRow, startColumn, numRows, numColumns) let values = Sheet.getSheetValues(1, 1, lastRow, lastColumn) let jsonString = JSON.stringify(Object.assign({}, values)) return ContentService.createTextOutput(jsonString) } Once you\u0026rsquo;ve finished writing doGet, let\u0026rsquo;s test if it meets the expected results.\nBefore directly publishing, utilize the built-in testing tools for verification.\nUnder the same Google App Script project, add a new file named debugGet.gs.\nfunction debug() { var e = { parameter:{ sheetId:\u0026#34;1XuNpMRdZasL8TNovZasd13asd41SoEBo0BMzabGqaE1s\u0026#34;, sheetTag:\u0026#34;todo\u0026#34; } } let result = doGet(e) Logger.log(result.getContent()) } Once you\u0026rsquo;ve set up the Google Form to receive data, click \u0026lsquo;Run.\u0026rsquo;\nThe returned results will be displayed in the execution results window through Logger.log.\nWriting Data Passing data as a JSON string will sequentially write into the specified worksheet.\nAn important note: if the data includes URLs, special characters must be escaped when passed.\nUnfortunately, native JavaScript lacks a function to revert this formatting.\nThe workaround involves using the DOM document, which GAS (Google Apps Script) cannot directly call since it operates on the server side.\nTherefore, direct replacement is used.\nConvert incoming string data into a JSON object. Iterate through the object and write the values from the first row to different columns based on their respective indices. function doPost(e) { let params = e.parameter // Initialize Spreadsheet let sheetTag = params.sheetTag let sheetId = params.sheetId let SpreadSheet = SpreadsheetApp.openById(sheetId) let Sheet = SpreadSheet.getSheetByName(sheetTag) let data = JSON.parse(params.data) let lastRow = 0 // Write to Spreadsheet data.forEach(function(value){ with(value) { content = content.replace(/\u0026amp;amp;/g, \u0026#34;\u0026amp;\u0026#34;).replace(/\u0026amp;lt;/g, \u0026#34;\u0026lt;\u0026#34;).replace(/\u0026amp;gt;/g, \u0026#34;\u0026gt;\u0026#34;).replace(/\u0026amp;quot;/g, \u0026#34;\\\u0026#34;\u0026#34;).replace(/\u0026amp;#039;/g, \u0026#34;\u0026#39;\u0026#34;) Sheet.getRange(lastRow + 1, 1).setValue(article_id) Sheet.getRange(lastRow + 1, 2).setValue(content) } lastRow = lastRow + 1 }) return ContentService.createTextOutput(JSON.stringify(true)).setMimeType(ContentService.MimeType.JSON) } The testing method is similar to the \u0026lsquo;Get\u0026rsquo; process; let\u0026rsquo;s add a new file named debugGet.gs.\nfunction debugPost(){ var processing = { parameter:{ sheetId:\u0026#34;1XuNpMRdZasL8TNovZS09Aing41SoEBo0BMzabGqaE1s\u0026#34;, sheetTag:\u0026#34;processing\u0026#34;, data: \u0026#39;[{\u0026#34;action\u0026#34;:\u0026#34;Bass class\u0026#34;,\u0026#34;date\u0026#34;:\u0026#34;2021/09/12\u0026#34;}]\u0026#39;, } } var done = { parameter:{ sheetId:\u0026#34;1XuNpMRdZasL8TNovZS09Aing41SoEBo0BMzabGqaE1s\u0026#34;, sheetTag:\u0026#34;done\u0026#34;, data: \u0026#39;[{\u0026#34;action\u0026#34;:\u0026#34;Return the book\u0026#34;,\u0026#34;date\u0026#34;:\u0026#34;2021/10/09\u0026#34;}]\u0026#39;, } } let result1 = doPost(processing) let result2 = doPost(done) Logger.log(result1.getContent()) Logger.log(result2.getContent()) } We can see that we\u0026rsquo;ve tested two types of incoming parameters: processing and done.\nThe only difference lies in the sheetTag passed, representing different worksheets.\nThe next example is a bit more complex and falls under an extended application:\nInput invitation code and user email. Check whether the invitation code still has available quotas\n(if all subsequent columns of the same invitation code are filled, in other words, if there are no empty columns for data). Verify if the user has already registered with the invitation code. Write data / Return results. function doPost(e) { let params = e.parameter let sheetTag = params.sheetTag let sheetId = params.sheetId let SpreadSheet = SpreadsheetApp.openById(sheetId) let Sheet = SpreadSheet.getSheetByName(sheetTag) let invitation_code = params.invitation_code let email = params.email // Email already exists. let user_target_range = Sheet.createTextFinder(email).findNext() if(user_target_range !== null) { return ContentService.createTextOutput(false) } // Retrieve all invitation_code let ranges = Sheet.createTextFinder(invitation_code).findAll() // Iterate through all the results of invitation_code let target_row = 0 ranges.every(function(range){ let row_num = range.getRowIndex() let column_value = Sheet.getRange(row_num, 2).getValue() // Retrieve the content of the current target\u0026#39;s column B; if empty, set the write-in row number. if(column_value == \u0026#39;\u0026#39;) { target_row = row_num return false } return true }) if(target_row !== 0) { Sheet.getRange(target_row, 2).setValue(email) return ContentService.createTextOutput(true) } return ContentService.createTextOutput(false) // Invitation code has been used. } Testing function\nfunction debugPost(){ let params = { parameter: { sheetId:\u0026#34;1XuNpMRdZasL8TNovZS09Aing41SoEBo0BMzabGqaE1s\u0026#34;, sheetTag:\u0026#34;done\u0026#34;, invitation_code: \u0026#34;5XdxPDdv\u0026#34;, email: \u0026#39;test123@gm.co\u0026#39; } } Logger.log(doPost(params).getContent()); } Calling from the Backend Once testing is successful, you can proceed with publishing. You\u0026rsquo;ll obtain an API URL. I haven\u0026rsquo;t included a screenshot here because Google tends to update its interface constantly.\nThe layout might change within a few days.\nNext, I\u0026rsquo;ll demonstrate how to send requests from PHP to the GAS API.\nprivate function updateSheet(string $invitation_code): void { $auth_user = auth()-\u0026gt;user(); $user_email = $auth_user-\u0026gt;account; $client = new \\GuzzleHttp\\Client(); $response = $client-\u0026gt;request(\u0026#39;POST\u0026#39;, $this-\u0026gt;gas_url, [ \u0026#39;form_params\u0026#39; =\u0026gt; [ \u0026#39;sheetId\u0026#39; =\u0026gt; $this-\u0026gt;sheetId, \u0026#39;sheetTag\u0026#39; =\u0026gt; $this-\u0026gt;sheetTag, \u0026#39;invitation_code\u0026#39; =\u0026gt; $invitation_code, \u0026#39;email\u0026#39; =\u0026gt; $user_email, ] ]); if ($response-\u0026gt;getStatusCode() != Response::HTTP_OK) { throw new AppException(Response::HTTP_INTERNAL_SERVER_ERROR, \u0026#39;The Sheet Die\u0026#39;); } } You can see that we\u0026rsquo;re using form_params to pass in variables; there isn\u0026rsquo;t much else to note here.\nLet me reiterate: this approach is only suitable when your service is small.\nBesides considerations like ACID , you never know when you might step into a sensitive area for Google.\nAfter retrieving data, it\u0026rsquo;s advisable to cache it on your own server rather than executing the GAS with every request.\nTriggering GAS too frequently might lead to situations where the Google Form becomes inaccessible.\n",
    "ref": "/en/blog/202111-operate-google-sheet-via-google-app-script/"
  },{
    "title": "[A Tour of Go Study Notes] 05 Arrays and Slices",
    "date": "",
    "description": "Understanding arrays and slices in the Go language using the official tutorial",
    "body": "Let\u0026rsquo;s use the official tutorial, A Tour of Go , to get familiar with basic Golang usage.\nThis piece introduces the concepts of arrays and slices.\n[n]T represents an array with n elements of type T.\nTherefore, the value types within an array must be consistent, and the length remains fixed.\nvar a [10]int // a is an array of 10 pure integers package main import \u0026#34;fmt\u0026#34; func main() { var a [2]string a[0] = \u0026#34;Hello\u0026#34; a[1] = \u0026#34;World\u0026#34; fmt.Println(a[0], a[1]) fmt.Println(a) primes := [6]int{2, 3, 5, 7, 11, 13} fmt.Println(primes) } /* \u0026gt;\u0026gt;\u0026gt; Hello World \u0026gt;\u0026gt;\u0026gt; [Hello World] \u0026gt;\u0026gt;\u0026gt; [2 3 5 7 11 13] */ It seems using arrays can be quite challenging. Who would know the length from the start?\nDon\u0026rsquo;t worry, Go provides a more convenient way to handle arrays.\nSlices Arrays have a fixed size, whereas slices offer dynamic sizing and are generally more frequently used than arrays.\n[]T represents a slice with elements of type T, accessed via a lower and upper bound to obtain a subset of array elements. Retrieve elements including low but excluding high.\na[low: high] The following example creates a slice a with elements from 1 to 3.\npackage main import \u0026#34;fmt\u0026#34; func main() { primes := [6]int{2, 3, 5, 7, 11, 13} var s []int = primes[1:4] fmt.Println(s) } /* \u0026gt;\u0026gt;\u0026gt; [3 5 7] */ A slice itself does not store any elements but rather references the underlying array.\nTherefore, modifying the content of a slice affects other slices using the same underlying array!\npackage main import \u0026#34;fmt\u0026#34; func main() { names := [4]string{ \u0026#34;John\u0026#34;, \u0026#34;Paul\u0026#34;, \u0026#34;George\u0026#34;, \u0026#34;Ringo\u0026#34;, } fmt.Println(names) a := names[0:2] b := names[1:3] fmt.Println(a, b) b[0] = \u0026#34;XXX\u0026#34; fmt.Println(a, b) fmt.Println(names) } /* \u0026gt;\u0026gt;\u0026gt; [John Paul George Ringo] \u0026gt;\u0026gt;\u0026gt; [John Paul] [Paul George] \u0026gt;\u0026gt;\u0026gt; [John XXX] [XXX George] \u0026gt;\u0026gt;\u0026gt; [John XXX George Ringo] */ Slices can contain any type, even other slices.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) func main() { // A tic-tac-toe game template board := [][]string{ []string{\u0026#34;_\u0026#34;, \u0026#34;_\u0026#34;, \u0026#34;_\u0026#34;}, []string{\u0026#34;_\u0026#34;, \u0026#34;_\u0026#34;, \u0026#34;_\u0026#34;}, []string{\u0026#34;_\u0026#34;, \u0026#34;_\u0026#34;, \u0026#34;_\u0026#34;}, } // Two players taking turns to place O and X board[0][0] = \u0026#34;X\u0026#34; board[2][2] = \u0026#34;O\u0026#34; board[1][2] = \u0026#34;X\u0026#34; board[1][0] = \u0026#34;O\u0026#34; board[0][2] = \u0026#34;X\u0026#34; for i := 0; i \u0026lt; len(board); i++ { fmt.Printf(\u0026#34;%s\\n\u0026#34;, strings.Join(board[i], \u0026#34; \u0026#34;)) } } /* \u0026gt;\u0026gt;\u0026gt; X _ X \u0026gt;\u0026gt;\u0026gt; O _ X \u0026gt;\u0026gt;\u0026gt; _ _ O */ Slice Declaration Slice declaration resembles an array without a length.\nThis is an array declaration.\n[3]bool{true, true, false} This is a slice expression. It creates an array similar to the one above and then constructs a slice that refers to this array.\n[]bool{true, true, false} package main import \u0026#34;fmt\u0026#34; func main() { q := []int{2, 3, 5, 7, 11, 13} fmt.Println(q) r := []bool{true, false, true, true, false, true} fmt.Println(r) s := []struct { i int b bool }{ {2, true}, {3, false}, {5, true}, {7, true}, {11, false}, {13, true}, } fmt.Println(s) } /* \u0026gt;\u0026gt;\u0026gt; [2 3 5 7 11 13] \u0026gt;\u0026gt;\u0026gt; [true false true true false true] \u0026gt;\u0026gt;\u0026gt; [{2 true} {3 false} {5 true} {7 true} {11 false} {13 true}] */ Default Value of Slices The default value of a slice is nil. A nil slice has a length and capacity of 0 and does not reference an underlying array.\npackage main import \u0026#34;fmt\u0026#34; func main() { var s []int fmt.Println(s, len(s), cap(s)) if s == nil { fmt.Println(\u0026#34;nil!\u0026#34;) } } /* \u0026gt;\u0026gt;\u0026gt; [] 0 0 \u0026gt;\u0026gt;\u0026gt; nil! */ Default Behavior of Slices Slices by default ignore the upper and lower bounds.\nOr rather, the default lower bound is 0, and the upper bound is the length of the slice.\nAssuming an array:\nvar a [10]int The following slice contents are equivalent:\na[0:10] a[:10] a[0:] a[:] In the example below, since a slice modifies the referenced array, the contents of s itself are altered.\npackage main import \u0026#34;fmt\u0026#34; func main() { s := []int{2, 3, 5, 7, 11, 13} s = s[1:4] fmt.Println(s) s = s[:2] fmt.Println(s) s = s[1:] fmt.Println(s) } /* \u0026gt;\u0026gt;\u0026gt; [3 5 7] \u0026gt;\u0026gt;\u0026gt; [3 5] \u0026gt;\u0026gt;\u0026gt; [5] */ Slice Length and Capacity Length: the number of elements it contains Capacity: the number of elements in the underlying array starting from the first element of the slice You can obtain the length using len(s) and the capacity using cap(s).\nSlices altered by slicing can be expanded by reslicing.\nGiven enough capacity from the original array length:\npackage main import \u0026#34;fmt\u0026#34; func main() { s := []int{2, 3, 5, 7, 11, 13} printSlice(s) // Slice the slice to give it zero length. s = s[:0] printSlice(s) // Extend its length. s = s[:4] printSlice(s) // Drop its first two values. s = s[2:] printSlice(s) } func printSlice(s []int) { fmt.Printf(\u0026#34;len=%d cap=%d %v\\n\u0026#34;, len(s), cap(s), s) } /* \u0026gt;\u0026gt;\u0026gt; len=6 cap=6 [2 3 5 7 11 13] \u0026gt;\u0026gt;\u0026gt; len=0 cap=6 [] \u0026gt;\u0026gt;\u0026gt; len=4 cap=6 [2 3 5 7] \u0026gt;\u0026gt;\u0026gt; len=2 cap=4 [5 7] */ However, if it exceeds the capacity of the underlying array, it will cause a panic.\nFor instance, if the initial slice length is 6, and we attempt to expand the slice\u0026rsquo;s capacity to 9:\ns := []int{2, 3, 5, 7, 11, 13} s = s[:9] printSlice(s) /* \u0026gt;\u0026gt;\u0026gt; panic: runtime error: slice bounds out of range [:9] with capacity 6 */ Creating Slices using make Slices can be created using make, which is another method to create dynamic arrays.\nThe make function allocates an array of specified length with all elements initialized to zero and returns a reference to a slice referring to this array.\nIf a capacity is desired, the third argument needs to be passed to make.\npackage main import \u0026#34;fmt\u0026#34; func main() { a := make([]int, 5) printSlice(\u0026#34;a\u0026#34;, a) b := make([]int, 0, 5) printSlice(\u0026#34;b\u0026#34;, b) c := b[:2] printSlice(\u0026#34;c\u0026#34;, c) d := c[2:5] printSlice(\u0026#34;d\u0026#34;, d) } func printSlice(s string, x []int) { fmt.Printf(\u0026#34;%s len=%d cap=%d %v\\n\u0026#34;, s, len(x), cap(x), x) } /* \u0026gt;\u0026gt;\u0026gt; a len=5 cap=5 [0 0 0 0 0] \u0026gt;\u0026gt;\u0026gt; b len=0 cap=5 [] \u0026gt;\u0026gt;\u0026gt; c len=2 cap=5 [0 0] \u0026gt;\u0026gt;\u0026gt; d len=3 cap=3 [0 0 0] */ Adding Elements to a Slice Appending new elements to a slice is a common operation.\nIn Golang, we use Append for this purpose, a built-in function you can reference here: Official Documentation The append expression\u0026rsquo;s first argument s is a slice of type T.\nThe rest of the values of type T will be added to the end of the existing slice.\nfunc append(s []T, ...T) []T The resulting slice will contain all the elements from the original slice along with the new added elements.\nIf the underlying array of s is too small to fit all the given values, s will be assigned a larger array, and the returned slice will refer to this new allocated array.\nTo learn more about slice internals, you can read this article: Go Slices: usage and internals package main import \u0026#34;fmt\u0026#34; func main() { var s []int printSlice(s) // Appending an empty slice s = append(s, 0) printSlice(s) // Slice grows as needed s = append(s, 1) printSlice(s) // Multiple elements can be added at once s = append(s, 2, 3, 4) printSlice(s) } /* \u0026gt;\u0026gt;\u0026gt; len=0 cap=0 [] \u0026gt;\u0026gt;\u0026gt; len=1 cap=1 [0] \u0026gt;\u0026gt;\u0026gt; len=2 cap=2 [0 1] \u0026gt;\u0026gt;\u0026gt; len=5 cap=6 [0 1 2 3 4] */ ",
    "ref": "/en/blog/202111-a-tour-of-go-05-array-and-slice/"
  },{
    "title": "[A Tour of Go Study Notes] 04 Pointers and Structs",
    "date": "",
    "description": "Understanding Go language pointers and structs using the official tutorial",
    "body": "Entering the world of Golang, we first explore the basic usage of Golang using the official tutorial: A Tour of Go .\nThis article introduces the usage of pointers and structs.\nPointers Go has pointers, which represent the memory address of a value.\n*T represents a pointer to type T, with a default value of nil.\nvar p *int // p = nil The \u0026amp; symbol points to the \u0026ldquo;memory address\u0026rdquo; of the target.\ni := 42 p := \u0026amp;i // p = 0xc000018038 = memory location of variable i The * operator retrieves the \u0026ldquo;value\u0026rdquo; represented by that pointer.\nfmt.Println(*p) // Retrieves the value represented by pointer p, which is i *p = 21 // Changes i indirectly by setting the value of pointer p This is commonly referred to as \u0026ldquo;dereferencing\u0026rdquo; or \u0026ldquo;indirect referencing.\u0026rdquo;\npackage main import \u0026#34;fmt\u0026#34; func main() { i, j := 42, 2701 p := \u0026amp;i // point to i fmt.Println(*p) // read i through the pointer *p = 21 // set i through the pointer fmt.Println(i) // see the new value of i p = \u0026amp;j // point to j *p = *p / 37 // divide j through the pointer fmt.Println(j) // see the new value of j } /* \u0026gt;\u0026gt;\u0026gt; 42 \u0026gt;\u0026gt;\u0026gt; 21 \u0026gt;\u0026gt;\u0026gt; 73 */ Structs Similar to JavaScript objects, structs are collections of fields.\nVariables store single values; when more complex concepts like coordinates are needed, structs can be used.\npackage main import \u0026#34;fmt\u0026#34; type Vertex struct { X int Y int } func main() { fmt.Println(Vertex{1, 2}) } /* \u0026gt;\u0026gt;\u0026gt; {1 2} */ Even the way to access them is quite similar to JavaScript objects, using . to access the contents within the struct.\nNotice, here X and Y are both capitalized.\nThis signifies that the content is exported and can be accessed externally!\npackage main import \u0026#34;fmt\u0026#34; type Vertex struct { X int Y int } func main() { v := Vertex{1, 2} v.X = 4 fmt.Println(v.X) } /* \u0026gt;\u0026gt;\u0026gt; 4 */ The pointer operations mentioned earlier also apply to struct operations.\npackage main import \u0026#34;fmt\u0026#34; type Vertex struct { X int Y int } func main() { v := Vertex{1, 2} p := \u0026amp;v // p points to the memory location of v, indicating they refer to the same target p.X = 1e9 fmt.Println(v) } /* \u0026gt;\u0026gt;\u0026gt; {1000000000 2} */ Creating Structs package main import \u0026#34;fmt\u0026#34; type Vertex struct { X, Y int } var ( v1 = Vertex{1, 2} // Creates a struct of type Vertex v2 = Vertex{X: 1} // Y:0 because int type defaults to 0 v3 = Vertex{} // X:0, Y:0 p = \u0026amp;Vertex{3, 2} // Creates a struct of type *Vertex ) func main() { fmt.Println(v1, v2, v3, p) } /* \u0026gt;\u0026gt;\u0026gt; {1 2} {1 0} {0 0} \u0026amp;{3 2} */ ",
    "ref": "/en/blog/202111-a-tour-of-go-04-pointer-and-struct/"
  },{
    "title": "[A Tour of Go Study Notes] 03 Flow Control",
    "date": "",
    "description": "Understanding Go language flow control using the official tutorial",
    "body": "Entering the world of Golang, we first use the official tutorial: A Tour of Go to learn the basics of using Golang.\nThis article is a note on flow control, explaining some concepts like if, else, for, switch, and defer.\nfor Go has only one type of loop: for.\nIts structure consists of:\nInitialization: Executed before the first iteration. Condition: Evaluated before each iteration. Post statement: Executed after each iteration. The initialization is often a short variable declaration that exists only within the for loop.\nOnce the condition evaluates to false, the loop stops.\nHere\u0026rsquo;s a basic loop example:\npackage main import \u0026#34;fmt\u0026#34; func main() { sum := 0 for i := 0; i \u0026lt; 10; i++ { sum += i } fmt.Println(sum) } /* \u0026gt;\u0026gt;\u0026gt; 45 */ Initialization and post statement can be omitted:\npackage main import \u0026#34;fmt\u0026#34; func main() { sum := 1 for ; sum \u0026lt; 1000; { sum += sum } fmt.Println(sum) } /* \u0026gt;\u0026gt;\u0026gt; 1024 */ You can even omit everything, creating a while loop-like construct:\npackage main import \u0026#34;fmt\u0026#34; func main() { sum := 1 for sum \u0026lt; 1000 { sum += sum } fmt.Println(sum) } /* \u0026gt;\u0026gt;\u0026gt; 1024 */ For an infinite loop, just omit the condition, and the loop will not end. This makes it easy to create an infinite loop:\npackage main func main() { for { } } if package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func sqrt(x float64) string { if x \u0026lt; 0 { return sqrt(-x) + \u0026#34;i\u0026#34; } return fmt.Sprint(math.Sqrt(x)) } func main() { fmt.Println(sqrt(2), sqrt(-4)) } /* \u0026gt;\u0026gt;\u0026gt; 1.4142135623730951 2i */ Short Statement Similar to for, if has a short statement form, where a simple operation can be executed before the condition.\nRefer to the example below. Note that the variable v is scoped only within the if block:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func pow(x, n, lim float64) float64 { if v := math.Pow(x, n); v \u0026lt; lim { return v } // v cannot be used here return lim } func main() { fmt.Println( pow(3, 2, 10), pow(3, 3, 20), ) } /* \u0026gt;\u0026gt;\u0026gt; 9 20 */ else Variables declared in if can also be used in the corresponding else block:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func pow(x, n, lim float64) float64 { if v := math.Pow(x, n); v \u0026lt; lim { return v } else { fmt.Printf(\u0026#34;%g \u0026gt;= %g\\n\u0026#34;, v, lim) } // v cannot be used here return lim } func main() { fmt.Println( pow(3, 2, 10), pow(3, 3, 20), ) } /* \u0026gt;\u0026gt;\u0026gt; 27 \u0026gt;= 20 \u0026gt;\u0026gt;\u0026gt; 9 20 */ switch One notable difference in Go\u0026rsquo;s switch compared to other languages is that it doesn\u0026rsquo;t require a break statement.\nIt executes the corresponding case, in some languages, it would continue executing subsequent case statements.\nYou can use default at the end to define the default case when none of the conditions match.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; ) func main() { fmt.Print(\u0026#34;Go runs on \u0026#34;) switch os := runtime.GOOS; os { case \u0026#34;darwin\u0026#34;: fmt.Println(\u0026#34;OS X.\u0026#34;) case \u0026#34;linux\u0026#34;: fmt.Println(\u0026#34;Linux.\u0026#34;) default: // freebsd, openbsd, // plan9, windows... fmt.Printf(\u0026#34;%s.\\n\u0026#34;, os) } } /* \u0026gt;\u0026gt;\u0026gt; Go runs on Linux. */ case doesn\u0026rsquo;t necessarily need to be constants or integers; it can even execute calculations:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { fmt.Println(\u0026#34;When\u0026#39;s Saturday?\u0026#34;) today := time.Now().Weekday() switch time.Saturday { case today + 0: fmt.Println(\u0026#34;Today.\u0026#34;) case today + 1: fmt.Println(\u0026#34;Tomorrow.\u0026#34;) case today + 2: fmt.Println(\u0026#34;In two days.\u0026#34;) default: fmt.Println(\u0026#34;Too far away.\u0026#34;) } } /* \u0026gt;\u0026gt;\u0026gt; When\u0026#39;s Saturday? \u0026gt;\u0026gt;\u0026gt; Too far away. */ A switch without a condition is equivalent to switch true, making it more readable than a long if-then-else chain.\nIt matches the first case that evaluates to true, executing its block:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { t := time.Now() switch { case t.Hour() \u0026lt; 12: fmt.Println(\u0026#34;Good morning!\u0026#34;) case t.Hour() \u0026lt; 17: fmt.Println(\u0026#34;Good afternoon.\u0026#34;) default: fmt.Println(\u0026#34;Good evening.\u0026#34;) } } /* \u0026gt;\u0026gt;\u0026gt; Good evening. */ defer defer postpones the execution of a function until the surrounding function returns.\nHowever, the arguments are evaluated immediately, and the function is executed after the outer function completes.\npackage main import \u0026#34;fmt\u0026#34; func main() { defer fmt.Println(\u0026#34;world\u0026#34;) fmt.Println(\u0026#34;hello\u0026#34;) } /* \u0026gt;\u0026gt;\u0026gt; hello \u0026gt;\u0026gt;\u0026gt; world */ Stacking defers What happens when you use defer in a loop?\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;counting\u0026#34;) for i := 0; i \u0026lt; 10; i++ { defer fmt.Println(i) } fmt.Println(\u0026#34;done\u0026#34;) } /* counting done 9 8 7 6 5 4 3 2 1 0 */ First, notice that the two Println outside the loop are executed first.\nThis is easy to understand. Then, enter the loop, iterate from 0 to 9, and each current number is deferred to be executed later.\nThe actual output sequence is from 9 to 0, so you can deduce that defer is a form of stacking. When multiple defer statements coexist, they follow the Last In, First Out (LIFO) order for execution.\n",
    "ref": "/en/blog/202111-a-tour-of-go-03-flow-control-statements/"
  },{
    "title": "[A Tour of Go Study Notes] 02 Variables and Types",
    "date": "",
    "description": "Understanding Go language variables and types using the official tutorial",
    "body": "Entering the world of Golang, we first use the official tutorial: A Tour of Go to learn the basics of using Golang.\nThis article is a note on variables and types.\nVariables In the Golang, var is used to declare variables, and just like the parameter list of a function, the type is defined at the end.\nvar declarations can appear within the package or function scope.\npackage main import \u0026#34;fmt\u0026#34; var c, python, java bool func main() { var i int fmt.Println(i, c, python, java) } /* \u0026gt;\u0026gt;\u0026gt; 0 false false false */ Default Values of Variables When declaring variables, an initial value can be defined.\nIf an initial value is defined, the type can be omitted during declaration.\nGo will infer the variable\u0026rsquo;s type based on the initial value, which is quite clever!\npackage main import \u0026#34;fmt\u0026#34; // Actively defining variable types var i, j int = 1, 2 func main() { // Omitting type, using default values var c, python, java = true, false, \u0026#34;no!\u0026#34; fmt.Println(i, j, c, python, java) } /* \u0026gt;\u0026gt;\u0026gt; 1 2 true false no! */ Short Variable Declaration Within a function, := can be used instead of var to declare variables.\nOutside functions, every statement must begin with a keyword (var, func, etc.) and cannot use :=.\npackage main import \u0026#34;fmt\u0026#34; func main() { var i, j int = 1, 2 k := 3 c, python, java := true, false, \u0026#34;no!\u0026#34; fmt.Println(i, j, k, c, python, java) } /* \u0026gt;\u0026gt;\u0026gt; 1 2 3 true false no! */ Basic Types Go\u0026rsquo;s basic types include:\nbool string int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr byte // alias for uint8 rune // alias for int32 // represents a Unicode code point float32 float64 complex64 complex128 Generally, the lengths of int, uint, and uintptr are the same as the system\u0026rsquo;s bit size.\nFor instance, on a 32-bit system, it\u0026rsquo;s 32 bits; on a 64-bit system, it\u0026rsquo;s 64 bits.\nUse int directly when you need an integer unless there\u0026rsquo;s a specific reason to restrict its length.\nYou can use the fmt.Printf package to print formatted output of variables, including the type %T and value %v.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/cmplx\u0026#34; ) var ( ToBe bool = false MaxInt uint64 = 1\u0026lt;\u0026lt;64 - 1 z complex128 = cmplx.Sqrt(-5 + 12i) ) func main() { fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, ToBe, ToBe) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, MaxInt, MaxInt) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, z, z) } /* \u0026gt;\u0026gt;\u0026gt; Type: bool Value: false \u0026gt;\u0026gt;\u0026gt; Type: uint64 Value: 18446744073709551615 \u0026gt;\u0026gt;\u0026gt; Type: complex128 Value: (2+3i) */ Zero Values If a variable is declared without an initial value, it automatically gets assigned a zero value.\nDifferent types have different zero values:\nNumeric types: 0 Boolean type: false String type: \u0026quot;\u0026quot; (empty string) Type Conversion Use T(v) to convert a value v to type T.\nFor example:\nvar i int = 42 var f float64 = float64(i) var u uint = uint(f) Or use a more concise form:\ni := 42 f := float64(i) u := uint(f) Constants Similar to variable declarations but using const Content can be characters, strings, booleans, or numerical data Cannot use := for declaration Numeric Constants A form of high-precision numerical values The type of the constant is determined based on the context import \u0026#34;fmt\u0026#34; const ( // Creating a large number by shifting 1 left by 100 bits (binary 1 followed by 100 zeros) Big = 1 \u0026lt;\u0026lt; 100 // Then shifting left by 99 bits, becoming binary \u0026#34;10,\u0026#34; which is decimal 2 Small = Big \u0026gt;\u0026gt; 99 ) func needInt(x int) int { return x*10 + 1 } func needFloat(x float64) float64 { return x * 0.1 } func main() { fmt.Println(needInt(Small)) fmt.Println(needFloat(Small)) fmt.Println(needFloat(Big)) } /* \u0026gt;\u0026gt;\u0026gt; 21 \u0026gt;\u0026gt;\u0026gt; 0.2 \u0026gt;\u0026gt;\u0026gt; 1.2676506002282295e+29 */ What happens if we print needInt(Big)?\nconstant 1267650600228229401496703205376 overflows int\nThis occurs because in our system, the longest int is 64-bit (sometimes less), and the numerical constant Big we\u0026rsquo;ve defined is a 100-bit positive number, causing an overflow.\n",
    "ref": "/en/blog/202111-a-tour-of-go-02-variables-and-types/"
  },{
    "title": "[A Tour of Go Study Notes] 01 Packages and Functions",
    "date": "",
    "description": "Explore the fundamentals of Go language's basic package structure and functions through the official tutorial.",
    "body": "Entering the world of Golang for beginners, we start with the basics of Golang using the official tutorial: A Tour of Go .\nThis piece covers the fundamental aspects of basic packages and functions in Golang.\nHello World The unavoidable Hello World starts off our journey.\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World\u0026#34;) } Packages Every Go program consists of packages.\nThe execution begins with the main package.\nImport In the example below, two packages, fmt and math/rand, are imported.\nAccording to convention, the package name matches the last element of the import path.\nFor instance, methods in math/rand start with package rand.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; ) func main() { fmt.Println(\u0026#34;My favorite number is\u0026#34;, rand.Intn(10)) } Apart from grouped imports within the import parentheses, another way to import is by writing multiple lines for imports.\nHowever, grouped imports are considered better practice.\npackage main import \u0026#34;fmt\u0026#34; import \u0026#34;math\u0026#34; func main() { fmt.Printf(\u0026#34;Now you have %g problems.\\n\u0026#34;, math.Sqrt(7)) } Exported Name In Go, if a name starts with an uppercase letter, it indicates that it\u0026rsquo;s exported and can be used externally.\nFor example, Pizza is an exported name, just like Pi in the math package.\nHowever, pizza and pi are not exported names because they don\u0026rsquo;t start with an uppercase letter.\nWhen using a package, you can only access the content of exported names.\nNon-exported content cannot be accessed externally!\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func main() { fmt.Println(math.Pi) } Functions Similar to other languages, a function can accept zero or more parameters.\nThe add function in the example takes two int parameters. Note that the type is defined after the parameter.\npackage main import \u0026#34;fmt\u0026#34; func add(x int, y int) int { return x + y } func main() { fmt.Println(add(42, 13)) } /* \u0026gt;\u0026gt;\u0026gt; 55 */ If consecutive parameters have the same type, you can omit the type declaration for preceding parameters and specify it only for the last one, changing x int, y int to x, y int.\npackage main import \u0026#34;fmt\u0026#34; func add(x, y int) int { return x + y } func main() { fmt.Println(add(42, 13)) } /* \u0026gt;\u0026gt;\u0026gt; 55 */ Returning Multiple Values A function can return any number of results.\npackage main import \u0026#34;fmt\u0026#34; func swap(x, y string) (string, string) { return y, x } func main() { a, b := swap(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) fmt.Println(a, b) } /* \u0026gt;\u0026gt;\u0026gt; world hello */ You can also name the return values.\nNaming the return values at the top of the function treats them as variables within the function.\nA return statement without any arguments returns the named return values directly, known as a \u0026ldquo;naked\u0026rdquo; return.\nNamed return values should have meaningful names and can be used as documentation.\nNaked returns should be used in concise functions as using them in longer functions may impact readability.\npackage main import \u0026#34;fmt\u0026#34; func split(sum int) (x, y int) { x = sum * 4 / 9 y = sum - x return } func main() { fmt.Println(split(19)) } /* \u0026gt;\u0026gt;\u0026gt; 4 5 */ ",
    "ref": "/en/blog/202111-a-tour-of-go-01-packages-and-functions/"
  },{
    "title": "[Git Tutorial] Configuring Global gitignore Locally",
    "date": "",
    "description": "Stop uploading editor configuration files! I really don't want to know what editor you use.",
    "body": "Do you include automatically generated editor configuration files in your .gitignore while developing a project?\nWhy add your personal environment settings to the project?\nFor instance, if you\u0026rsquo;re using PhpStorm, it generates a .idea folder: What is the .idea folder? If you\u0026rsquo;re on an Apple system, you\u0026rsquo;ll encounter .DS_Store: wiki .DS_Store During personal development, it\u0026rsquo;s convenient to add these to .gitignore for a clean and tidy workspace.\nBut when you\u0026rsquo;re part of a team project, this approach becomes impractical.\nTypically, .gitignore is for excluding project settings, keys, rendered content, etc., from version control.\nTherefore, including idea and .DS_Store in gitignore isn\u0026rsquo;t reasonable, as not everyone needs to ignore these files.\nActually, we can configure this locally in the git settings because your machine won\u0026rsquo;t suddenly switch from Mac to Windows.\nExcluding specific files or paths from version control in the local environment sounds much more reasonable.\nOperation Steps 1 Create a Global gitignore Configuration File The filename can be anything; use the terminal to execute the command:\ntouch .gitignore_global 2 Add Files to be Ignored Similar to .gitignore, you can manually open the file for editing or execute:\necho /.idea \u0026gt;\u0026gt; .gitignore_global 3 Add the Configuration File to .gitconfig You can manually edit or use the following command:\ngit config --global core.excludesfile ~/.gitignore_global 4 Check if the Configuration is Successful The simplest way is to open the project and delete the corresponding settings in .gitignore.\nHowever, let\u0026rsquo;s review what we\u0026rsquo;ve just configured.\nYou can use commands to view or directly open the file.\nUsing commands seems more sophisticated, so let\u0026rsquo;s give it a try:\nvim .gitignore_global Here, you can see the list of files we want to ignore in the configuration file.\nSome might have been added during the initialization of Git. Just ensure our added content is written in the file:\n*~ .DS_Store /.idea Next, check if the git configuration file has been set:\nvim .gitconfig You can see in the excluded file settings, it\u0026rsquo;s specified to the ignore list configuration file we just created:\n[core] excludesfile = {path to .gitignore_global} By doing this, we\u0026rsquo;ve completed the \u0026ldquo;Local Global gitignore Configuration\u0026rdquo; task. Reference: git gitignore ",
    "ref": "/en/blog/202111-git-global-gitignore/"
  },{
    "title": "[Discord Bot] 02. Dice Rolling Bot",
    "date": "",
    "description": "Create a Discord chat bot for rolling dice using Python3",
    "body": "Do you know about TTRPG? One of the most well-known is probably Dungeons \u0026amp; Dragons (D\u0026amp;D).\nAfter watching hololive EN play, I thought, \u0026ldquo;Let\u0026rsquo;s create a dice rolling bot and give it a try.\u0026rdquo;\nRequirements Discussion Ability to roll multiple dice Customizable number of sides on the dice Success determination calculation Command format\n!roll 5D10\u0026gt;8 This means: roll 5 ten-sided dice, and success is determined if the result is greater than or equal to 8.\nCode # Import the Discord.py library import discord # Import the random library import random # Obtain the Discord client object for operations client = discord.Client() # Invoke the event library @client.event # When the bot is ready, display a message in the terminal async def on_ready(): print(f\u0026#39;Logged in as {client.user}\u0026#39;) # Invoke the event library @client.event # When a message is received async def on_message(message): # Exclude messages sent by the bot itself to avoid an infinite loop if message.author == client.user: return # Default error message error = [] # Process input text content = message.content.replace(\u0026#39; \u0026#39;, \u0026#39;\u0026#39;).lower() # If the message starts with \u0026#34;!roll\u0026#34; if message.content.startswith(\u0026#39;!roll\u0026#39;): content = content.replace(\u0026#39;!roll\u0026#39;, \u0026#39;\u0026#39;) # Dice quantity calculation dice_count = content.split(\u0026#39;d\u0026#39;)[0] try: dice_count = int(dice_count) except ValueError: error.append(\u0026#39;The number of dice rolled must be an integer!\u0026#39;) # Dice type determination content = content.split(\u0026#39;d\u0026#39;)[1] dice_type = content.split(\u0026#39;\u0026gt;\u0026#39;)[0] try: dice_type = int(dice_type) except ValueError: error.append(\u0026#39;Dice type must be an integer!\u0026#39;) # Success determination if \u0026#39;\u0026gt;\u0026#39; in content: success = content.split(\u0026#39;\u0026gt;\u0026#39;)[1] try: success = int(success) except ValueError: error.append(\u0026#39;Success condition must be an integer!\u0026#39;) else: success = 0 if len(error) == 0: success_count = 0 result_msg = \u0026#39;\u0026#39; # Roll the dice results = [random.randint(1, dice_type) for _ in range(dice_count)] for result in results: if success \u0026gt; 0 and result \u0026gt;= success: success_count += 1 result_msg += f\u0026#39;`{result}`, \u0026#39; await message.channel.send(result_msg) if success \u0026gt; 0: await message.channel.send(f\u0026#39;Success: `{success_count}`\u0026#39;) else: await message.channel.send(error) # TOKEN obtained on the \u0026#34;BOT\u0026#34; page in Discord Developer client.run(\u0026#39;\u0026#39;) Code Explanation Basic usage of discord.py has been covered before, so we won\u0026rsquo;t repeat it here. We\u0026rsquo;ll focus on explaining the logic.\nInput Message Detection We use\nmessage.content.startswith(\u0026#39;!roll\u0026#39;) to only respond to messages starting with \u0026lsquo;!roll\u0026rsquo;. To ensure case insensitivity and handle spaces, we use\nmessage.content.replace(\u0026#39; \u0026#39;, \u0026#39;\u0026#39;).lower() to remove spaces and convert the message to lowercase.\nNow, the program processes the message as if the user had input \u0026lsquo;!roll5d10\u0026rsquo;.\nDice Quantity Calculation The logic is similar to before. We extract the number of dice by splitting the content around \u0026rsquo;d\u0026rsquo; and taking the first value.\nWe use\ncontent.replace(\u0026#39;!roll\u0026#39;, \u0026#39;\u0026#39;) to remove \u0026lsquo;!roll\u0026rsquo; as it\u0026rsquo;s only a trigger for the bot and doesn\u0026rsquo;t affect the dice rolling result.\nWe use\ncontent.split(\u0026#39;d\u0026#39;)[0] to get the number of dice. We then try to convert it to an integer, catching a ValueError if it\u0026rsquo;s not a valid number.\nDice Type Determination Similar to quantity calculation, but this time we extract the logic between \u0026rsquo;d\u0026rsquo; and \u0026lsquo;\u0026gt;\u0026rsquo; to determine the type of dice.\nAgain, we try to convert it to an integer and catch a ValueError if needed.\nSuccess Determination We check if \u0026lsquo;\u0026gt;\u0026rsquo; is present in the content. If it is, we extract the success condition after \u0026lsquo;\u0026gt;\u0026rsquo;.\nWe try to convert it to an integer, catching a ValueError if needed.\nRolling the Dice If there are no errors, we roll the dice using\nresults = [random.randint(1, dice_type) for _ in range(dice_count)] We iterate through the results, formatting them for Discord output, and count successful rolls if a success condition is specified.\nFinally, we send the result and success count to Discord. If there are errors, we send the error message instead.\nImprovement Directions While the bot\u0026rsquo;s requirements are met, there are areas for optimization.\nFunction Splitting The logic is currently written together.\nIt would be better to split it into separate functions for easier testing and future adjustments.\nInput Logic Optimization The input logic is too simple, and unexpected user inputs might not be handled correctly.\nFor example, \u0026lsquo;!roll6D10d5\u0026rsquo; will currently be processed the same as \u0026lsquo;!roll6D10\u0026rsquo;.\nProper handling of such cases should be considered.\n",
    "ref": "/en/blog/202110-make-discord-chatbot-02-roll-dice-bot/"
  },{
    "title": "[Discord Bot] 01. Creating a Basic Bot",
    "date": "",
    "description": "Creating a basic Discord chat bot using Python3",
    "body": "Just for fun, let\u0026rsquo;s write something I haven\u0026rsquo;t written before.\nLately, I\u0026rsquo;ve been active on Discord, so I\u0026rsquo;ve decided to create a Discord ChatBot.\nAdding a Discord Application Go to Discord Developers Applications and log in to your Discord account.\nClick on New Application, name it, and then click Create. You can change the name later.\nNavigate to Bot on the left and click on Add Bot.\nClick on the OAuth2 tab on the left.\nIn SCOPES, select bot. Under Bot Permissions, choose Administrator. The link for inviting the bot to your server is shown below.\nInstalling pip and Discord.py Use the following command to install the Discord.py library.\npython3 -m pip install -U discord.py Bot Code We\u0026rsquo;ve just created a bot and added it to our server. The next step is to start the bot and make it do something.\nAs an example, let\u0026rsquo;s write the simplest bot and see how it performs.\nroll_dice.py\n# Import the Discord.py library import discord # Obtain the Discord client object for operations client = discord.Client() # Invoke the event library @client.event # When the bot is ready, display a message in the terminal async def on_ready(): print(f\u0026#39;Logged in as {client.user}\u0026#39;) # Invoke the event library @client.event # When a message is received async def on_message(message): # Exclude messages sent by the bot itself to avoid an infinite loop if message.author == client.user: return # If we say \u0026#34;Who is the bot,\u0026#34; the bot will respond with \u0026#34;Who called me?\u0026#34; if message.content == \u0026#39;Who is the bot\u0026#39;: await message.channel.send(\u0026#39;Who called me?\u0026#39;) client.run(\u0026#39;MY APP TOKEN\u0026#39;) The explanation is provided in the example. The only thing to explain is where to get the token.\nNo more talking, let\u0026rsquo;s see the image!\nYou can obtain the TOKEN on the BOT page in Discord Developer.\nIt won\u0026rsquo;t be directly displayed on the screen; just click Copy.\nIf you insist on viewing it, click Click to Reveal Token, and it will be displayed.\nIf you think the TOKEN is compromised, you can use Regenerate to generate a new one.\nStarting the Bot Open your terminal and run the Python file you just created.\npython roll_dice.py Now, test whether the command triggers the bot.\nLooks like it was successfully called!\n",
    "ref": "/en/blog/202110-make-discord-chatbot-01-set-up/"
  },{
    "title": "Guarding Code Quality with GrumPHP",
    "date": "",
    "description": "Check if your code smells before you make a mistake.",
    "body": "In a collaborative development environment, Code Review is a crucial process.\nIf someone writes code that violates rules or fails tests, it can lead to project errors.\nManual syntax checks or running tests manually are time-consuming and not fashionable.\nThese tasks should be automated, and that\u0026rsquo;s where GrumPHP comes in.\nWhat is GrumPHP GrumPHP is an open-source Composer package that utilizes git hooks.\nWhen someone changes the code and commits it, GrumPHP checks the code content based on predefined tasks.\nIf the tests fail, the commit is aborted.\nGrumPHP comes with a set of common tasks built-in, allowing you to use it with minimal custom configuration.\nYou can find most of the tasks you might need on the Tasks page in the official documentation.\nLet\u0026rsquo;s go through the installation process and add four commonly used tasks.\nInstalling GrumPHP Let\u0026rsquo;s use the partially developed LaraPeko package as an example.\nThe term \u0026ldquo;project\u0026rdquo; mentioned here refers to this package.\nFirst, navigate to the project directory and execute:\ncomposer require --dev phpro/grumphp An interactive prompt will appear, asking if you want to create the configuration file grumphp.yml with various options.\nYou can choose any option or skip this step. The next step involves installing yamllint.\nIf you chose yamllint in the previous step, you\u0026rsquo;re good to go.\nYou can test if the checks are working by running:\nvendor/bin/grumphp run If you run:\ncomposer install and see the message \u0026ldquo;GrumPHP is sniffing your commits!\u0026rdquo; it means GrumPHP is now sniffing your commits.\nAdding Check Tasks Let\u0026rsquo;s gradually install and explain four basic check tasks:\nyamllint PHPLint Phpunit Phpcs If you didn\u0026rsquo;t create grumphp.yml in the previous step, create the file in the project root directory with the following content:\ngrumphp: tasks: { } You can find an example grumphp.yml configuration file on the official GitHub page, and the Parameters file provides detailed explanations and settings.\n1. yamllint Checks the validity of the yaml file structure in your project, useful for configuration files in formats such as Drone.\nAccording to the YamlLint documentation , no additional configuration is required.\nJust add yamllint to the tasks list in grumphp.yml:\ngrumphp: tasks: { yamllint: null, } 2. PHPLint Checks for syntax errors, such as missing commas that might result in red syntax error warnings in IDEs.\nFollow the official documentation for the PHPLint task: PHPLint .\nStart by installing the required dependency only in the test environment:\ncomposer require --dev php-parallel-lint/php-parallel-lint Add the phplint task to grumphp.yml:\ngrumphp: tasks: { yamllint: null, phplint: ~, } Refer to the documentation for many adjustable rules. For simplicity, we\u0026rsquo;ll stick with the default values.\n3. Phpunit Runs tests in a specified path. Fails the task if any tests fail.\nFollow the documentation for the Phpunit task: Phpunit . Install Phpunit in the test environment:\ncomposer require --dev phpunit/phpunit Create the phpunit.xml configuration file as required:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;phpunit xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:noNamespaceSchemaLocation=\u0026#34;./vendor/phpunit/phpunit/phpunit.xsd\u0026#34; bootstrap=\u0026#34;vendor/autoload.php\u0026#34; colors=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;testsuites\u0026gt; \u0026lt;testsuite name=\u0026#34;Unit\u0026#34;\u0026gt; \u0026lt;directory suffix=\u0026#34;Test.php\u0026#34;\u0026gt;./tests/Unit\u0026lt;/directory\u0026gt; \u0026lt;/testsuite\u0026gt; \u0026lt;testsuite name=\u0026#34;Feature\u0026#34;\u0026gt; \u0026lt;directory suffix=\u0026#34;Test.php\u0026#34;\u0026gt;./tests/Feature\u0026lt;/directory\u0026gt; \u0026lt;/testsuite\u0026gt; \u0026lt;/testsuites\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;whitelist processUncoveredFilesFromWhitelist=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;directory suffix=\u0026#34;.php\u0026#34;\u0026gt;./app\u0026lt;/directory\u0026gt; \u0026lt;/whitelist\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/phpunit\u0026gt; Test if the tasks are executed by running the following command.\nSince we configured Phpunit to look for tests in specific folders, ensure those folders exist in the project.\nvendor/bin/grumphp run You should see a green happy face indicating that the tests passed.\nIf we modify a test method to make it fail:\npublic function testGetAhoy() { $larapeko = new LaraPeko(); $this-\u0026gt;assertEquals(\u0026#39;ahoy\u0026#39;, \u0026#39;DD\u0026#39;); } And run the command again, you will see a red sad face along with an error message, making it clear which test failed.\n4. Phpcs Checks if the submitted PHP code adheres to PSR standards .\nInstall the required dependency following the Phpcs task documentation: Phpcs .\ncomposer require --dev squizlabs/php_codesniffer Create a phpcs.xml configuration file to set the rules you want to check. Here, we use the basic PSR-12 rules:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;ruleset name=\u0026#34;PSR12\u0026#34;\u0026gt; \u0026lt;description\u0026gt;The PSR12 coding standard.\u0026lt;/description\u0026gt; \u0026lt;rule ref=\u0026#34;PSR12\u0026#34;/\u0026gt; \u0026lt;file\u0026gt;app/\u0026lt;/file\u0026gt; \u0026lt;file\u0026gt;config/\u0026lt;/file\u0026gt; \u0026lt;file\u0026gt;routes/\u0026lt;/file\u0026gt; \u0026lt;exclude-pattern\u0026gt;public/index.php\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern\u0026gt;server.php\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern\u0026gt;laravel-nova-*\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern\u0026gt;vendor\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern\u0026gt;resources\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern\u0026gt;database/\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern \u0026gt;storage/\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern\u0026gt;node_modules/\u0026lt;/exclude-pattern\u0026gt; \u0026lt;exclude-pattern\u0026gt;tests\u0026lt;/exclude-pattern\u0026gt; \u0026lt;/ruleset\u0026gt; Now, run the check:\nvendor/bin/grumphp run You should see that all configured tests have passed.\nIf there are issues violating PSR-12, you will see a frowning face and details about the problems.\nAutomatic Listening All the tasks we did above were manual executions. It would be better if the code quality check happens automatically.\nTo achieve this, we can modify the composer.json file by adding scripts that run during the Composer process.\nAdd the following script to the composer.json file to initialize GrumPHP during the post-installation command:\n\u0026#34;scripts\u0026#34;: { \u0026#34;post-install-cmd\u0026#34;: [ \u0026#34;@php ./vendor/bin/grumphp git:init\u0026#34; ] }, This way, whether you run git commit in the command line or perform the commit through a GUI, the defined GrumPHP tasks will be triggered before the commit.\nBelow is an example of a failure status message triggered by the phpcs task through a GUI.\n",
    "ref": "/en/blog/202107-grumphp-a-php-code-quality-tool/"
  },{
    "title": "Develop Your Own Laravel Package",
    "date": "",
    "description": "Guide on developing and testing Laravel packages locally and installing them using composer in a local project.",
    "body": "The Laravel ecosystem offers numerous convenient packages for various purposes.\nHowever, what if you can\u0026rsquo;t find a suitable package or need to create a company-specific one?\nThis article demonstrates how to develop a package locally and install it in a Laravel project using composer, ensuring smooth development and testing.\nSome articles suggest creating a packages folder within an existing Laravel project for package development.\nHowever, this approach has drawbacks. Imagine deleting your project one day—your package project would be deleted too.\nAdditionally, this method doesn\u0026rsquo;t facilitate testing the installation via composer. This article presents an alternative approach.\n1. Initialize Package Project First, create a folder for your package; for example, we\u0026rsquo;ll use larapeko as a sample.\nmkdir larapeko Navigate to the newly created larapeko folder and run the following command to initialize the package.\ncd larapeko composer init You\u0026rsquo;ll be prompted with a series of questions to configure basic package information.\nAfterward, you\u0026rsquo;ll find the composer.json file in the folder.\n{ \u0026#34;name\u0026#34;: \u0026#34;ray247k/larapeko\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A package for demo peko\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;authors\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Ray\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ray247k@gmail.com\u0026#34; } ], \u0026#34;minimum-stability\u0026#34;: \u0026#34;dev\u0026#34;, \u0026#34;require\u0026#34;: {} } 2. Write Package Code Create a src folder in the larapeko directory to host the package code.\nInside, create a file named LaraPeko.php and set the appropriate namespace.\nFor demonstration purposes, let\u0026rsquo;s start with a simple function.\n\u0026lt;?php # LaraPeko.php namespace Ray247k\\LaraPeko; class LaraPeko { public function sayPeko() { echo \u0026#34;peko peko\\n\u0026#34;; } } 3. ServiceProvider Create a ServiceProvider file in the src directory, named LaraPekoServiceProvider.php.\nIn the register method, use a singleton to register a class named LaraPeko and return a new instance of it.\n\u0026lt;?php # LaraPekoServiceProvider.php namespace Ray247k\\LaraPeko; use Illuminate\\Support\\ServiceProvider; class LaraPekoServiceProvider extends ServiceProvider { public function boot() { } // Register the package public function register() { $this-\u0026gt;app-\u0026gt;singleton(\u0026#39;LaraPeko\u0026#39;, function ($app) { return new LaraPeko(); }); } } 4. Facade Facades provide a static interface to call registered classes directly.\nCreate a Facade file in the src directory, named LaraPekoFacade.php, and use the LaraPeko class registered in the ServiceProvider as the Laravel Facade object.\n\u0026lt;?php # LaraPekoFacade.php namespace Ray247k\\LaraPeko; use Illuminate\\Support\\Facades\\Facade; class LaraPekoFacade extends Facade { protected static function getFacadeAccessor() { return \u0026#39;LaraPeko\u0026#39;; } } 5. Default Config and Usage Sometimes, users may want to customize parameters through a configuration file.\nCreate a config folder in the package directory and add a configuration file, lara_peko.php.\n\u0026lt;?php # lara_peko.php return [ \u0026#39;best_girl\u0026#39; =\u0026gt; \u0026#39;Yagoo\u0026#39;, ]; Edit the LaraPekoServiceProvider.php file to include publishing the configuration file.\n\u0026lt;?php # LaraPekoServiceProvider.php namespace Ray247k\\LaraPeko; use Illuminate\\Support\\ServiceProvider; class LaraPekoServiceProvider extends ServiceProvider { public function boot() { $source = realpath($raw = __DIR__.\u0026#39;/../config/lara_peko.php\u0026#39;) ?: $raw; $this-\u0026gt;publishes([ $source =\u0026gt; config_path(\u0026#39;lara_peko.php\u0026#39;), ]); } // Register the package public function register() { $configPath = __DIR__ . \u0026#39;/../config/lara_peko.php\u0026#39;; $this-\u0026gt;mergeConfigFrom($configPath, \u0026#39;lara_peko\u0026#39;); $this-\u0026gt;app-\u0026gt;singleton(\u0026#39;LaraPeko\u0026#39;, function ($app) { return new LaraPeko(); }); } } After running php artisan vendor:publish, choose the corresponding number for the package when prompted.\nThis will generate a configuration template file in the project\u0026rsquo;s config directory.\nIf you are sure which package\u0026rsquo;s config file you want to create, you can directly specify the provider tag.\nphp artisan vendor:publish --provider=\u0026#34;Ray247k\\LaraPeko\\LaraPekoServiceProvider\u0026#34; In the register method, the default and new configuration files are merged.\nIf users modify the project\u0026rsquo;s configuration file, it will override the default configuration, allowing for customization.\nClear Configuration Cache When the configuration changes, use the following commands to clear the project\u0026rsquo;s configuration cache.\nphp artisan config:clear # Clear the configuration cache php artisan cache:clear # Clear the general cache Using Config Parameters To use the configuration in your code, employ the config() method.\nIn the LaraPeko.php file, create a getBestGirl method to retrieve the best_girl parameter from the configuration.\n\u0026lt;?php # LaraPeko.php namespace Ray247k\\LaraPeko; class LaraPeko { public function sayPeko() { echo \u0026#34;peko peko\\n\u0026#34;; } public function getBestGirl() { echo config(\u0026#39;lara_peko.best_girl\u0026#39;); } } 6. Package Auto-discovery Introduced in Laravel 5.5, this feature allows users to reduce the steps required during package installation by configuring the package settings. The goal is to eliminate manual editing of the config/app.php file\u0026rsquo;s providers and aliases arrays after running composer install.\nKeywords: Package Auto-discovery, Laravel Extension Package Auto-discovery.\nProcedure: Add the following content to the autoload and extra sections in the package\u0026rsquo;s composer.json.\nDue to length constraints, only the added parts are shown:\n{ \u0026#34;autoload\u0026#34;: { \u0026#34;psr-4\u0026#34;: { \u0026#34;Ray\\\\LaraPeko\\\\\u0026#34;: \u0026#34;src\u0026#34; } }, \u0026#34;extra\u0026#34;: { \u0026#34;laravel\u0026#34;: { \u0026#34;providers\u0026#34;: [ \u0026#34;Ray\\\\LaraPeko\\\\LaraPekoServiceProvider\u0026#34; ], \u0026#34;aliases\u0026#34;: { \u0026#34;LaraPeko\u0026#34;: \u0026#34;Ray\\\\LaraPeko\\\\LaraPekoFacade\u0026#34; } } } } Through the laravel definition in the extra section, a data entry is added to the providers and aliases arrays for registration.\nThe autoload uses the PSR-4 rule to point the specified namespace to the folder where our package code is located (src).\n7. Package Dependencies If your package depends on another package, such as the commonly used Guzzle, include the required item in the package\u0026rsquo;s composer.json:\n{ \u0026#34;name\u0026#34;: \u0026#34;ray247k/larapeko\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A package for demo peko\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;authors\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Ray\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;ray247k@gmail.com\u0026#34; } ], \u0026#34;minimum-stability\u0026#34;: \u0026#34;dev\u0026#34;, \u0026#34;require\u0026#34;: { \u0026#34;php\u0026#34;: \u0026#34;\u0026gt;=7.3\u0026#34;, \u0026#34;laravel/framework\u0026#34;: \u0026#34;5.5.*||^6.0|^7.0|^8.0\u0026#34;, \u0026#34;guzzlehttp/guzzle\u0026#34;: \u0026#34;^6.3\u0026#34; } } In addition to requiring Guzzle, we have specified the required PHP version and Laravel framework version.\n8. Package Testing Setting up a Test Project After the above process, our package is almost configured. Next, we\u0026rsquo;ll enter the testing phase.\nSince we intend to use it in a Laravel project, let\u0026rsquo;s create a new Laravel project for testing, named \u0026ldquo;lara-ahoy\u0026rdquo;:\nlaravel new lara-ahoy You can check the installed Laravel version with the following command:\ncd \u0026lt;project directory\u0026gt; php artisan -V # Laravel Framework 6.20.30 Including the Package in the Project Add Package Entry To use the package in the project, use the following command to add the package\u0026rsquo;s version storage location to the composer.json:\ncomposer config repositories.ray247k path ../larapeko This assumes that the package (larapeko) and the test project (lara-ahoy) are in the same directory.\nAfter running the command, open the composer.json file in the lara-ahoy project, and you\u0026rsquo;ll see a new section:\n{ \u0026#34;repositories\u0026#34;: { \u0026#34;ray247k\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;path\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;../larapeko\u0026#34; } } } You can manually add this section if you prefer not to use the command.\nIf the package is in a remote repository, change the type to vcs and add the URL:\ncomposer config repositories.ray247k vcs https://github.com/ray247k/larapeko Or manually adjust the repositories setting in composer.json:\n{ \u0026#34;repositories\u0026#34;: { \u0026#34;ray247k\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;vcs\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://github.com/ray247k/larapeko\u0026#34; } } } The remote URL can use either https or ssh. If the repository is private, use ssh\n(e.g., git@github.com:ray247k/larapeko.git).\nAdd Dependency You can use the following command:\ncomposer require ray247k/larapeko @dev Or manually add the following content to the require section in composer.json:\n{ \u0026#34;require\u0026#34;: { \u0026#34;ray247k/larapeko\u0026#34;: \u0026#34;@dev\u0026#34; } } If an error occurs during the require command due to an issue with a dependency\u0026rsquo;s version, you can use the --ignore-platform-reqs parameter to ignore the platform:\ncomposer require ray247k/larapeko --ignore-platform-reqs After adding the local dependency, it will automatically fetch the latest code each time it is used.\nIf it doesn\u0026rsquo;t automatically fetch or if you prefer to run composer require every time to update, adjust the symlink setting in the test project\u0026rsquo;s composer.json under the repositories section:\n{ \u0026#34;repositories\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;path\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;../../packages/my-package\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;symlink\u0026#34;: false } } ] } Install the Package Install the package using:\ncomposer install In the output, you should see that the package has been discovered and installed:\nDiscovered Package: ray247k/larapeko Package manifest generated successfully. If a version error occurs similar to during the require command, use the --ignore-platform-reqs parameter:\ncomposer install --ignore-platform-reqs Open the composer.lock file in the lara-ahoy project to verify that the package has been successfully installed.\nIf you have previously run the project, you must clear the existing autoload cache:\ncomposer dump-autoload Testing the Package In the previous steps, we installed our larapeko package into the test project lara-ahoy.\nNow, we\u0026rsquo;ll test whether the package has been correctly integrated using two methods: file testing and unit testing.\nMethod 1: File Testing Create a file named test-autoload.php directly in the Laravel project\u0026rsquo;s root directory.\nIn this file, load the autoload and call the package method:\n\u0026lt;?php # test-autoload.php use Ray247k\\LaraPeko\\LaraPeko; require_once \u0026#39;./vendor/autoload.php\u0026#39;; LaraPeko::sayPeko(); Run the test file in the lara-ahoy project directory:\nphp test-autoload.php If the autoload is successful, you should see the result of the sayPeko() method, even if a PHP Deprecated message appears:\nAlthough a PHP Deprecated message is displayed, the default string content is still successfully printed.\nThis indicates that the Facade aliases registered in the ServiceProvider were called successfully!\nTo suppress the PHP Deprecated message, add error_reporting(0); to the test-autoload.php file:\n\u0026lt;?php error_reporting(0); use Ray247k\\LaraP eko\\LaraPeko; require_once \u0026#39;./vendor/autoload.php\u0026#39;; LaraPeko::sayPeko(); Now, you won\u0026rsquo;t see the PHP Deprecated message:\nMethod 2: Unit Testing Next, let\u0026rsquo;s use PHPUnit to perform unit tests. Run the following command in the project directory:\nphp vendor/bin/phpunit If you encounter an error related to an invalid value for PHP_VERSION, consider using Homebrew to install PHP or running the tests in a Docker environment.\nIf php vendor/bin/phpunit works, proceed to build the unit test file.\nAdd a testing method getAhoy() to LaraPeko.php:\npublic function getAhoy() { return \u0026#34;ahoy\u0026#34;; } Create a tests folder inside the package project and add a test file named LaraPekoTest.php:\n\u0026lt;?php namespace Ray247k\\LaraPeko; use PHPUnit\\Framework\\TestCase; class LaraPekoTest extends TestCase { /** * @test * * @return void */ public function testClassInstance() { $this-\u0026gt;assertInstanceOf(LaraPekoTest::class, new LaraPekoTest); } public function testGetAhoy() { $larapeko = new LaraPeko(); $this-\u0026gt;assertEquals(\u0026#39;ahoy\u0026#39;, $larapeko-\u0026gt;getAhoy()); } } This test checks whether the class instance is correctly created and whether the getAhoy() method returns \u0026ldquo;ahoy\u0026rdquo;.\nRun the test using the following command:\nphp vendor/bin/phpunit vendor/ray247k/larapeko/tests You should see the PHPUnit results:\nNote: Suppose there is a private function testMethod() in the LaraPeko package:\nprivate function testMethod() { return \u0026#39;Haha\u0026#39;; } To test a private function, you can use the bindTo() method within a closure.\nAfter creating an object, manually inject it into the closure object to replace $this in the closure.\nThis way, it\u0026rsquo;s like directly calling the testMethod() method using the LaraPeko object:\n/** * @test * * @return void */ public function testMethodTest() { $contentSegment = new LaraPeko(); $closure = function () { return $this-\u0026gt;testMethod(); }; $closure_bind = $closure-\u0026gt;bindTo($contentSegment, $contentSegment); $this-\u0026gt;assertEquals(\u0026#39;Haha\u0026#39;, $closure_bind()); } This allows testing private methods effectively!\nThe above is a comprehensive guide for developing your own Laravel package.\nIf you have any further questions or topics to discuss, feel free to let me know. I\u0026rsquo;m happy to assist!\n",
    "ref": "/en/blog/202107-laravel-package-development/"
  },{
    "title": "Setting up Laravel Development Environment with Docker",
    "date": "",
    "description": "Introduction to building a basic Laravel development environment using Docker",
    "body": "In the past, developing Laravel projects locally mostly involved installing the required services directly on the machine.\nLater, the recommended development environment by the official documentation, Homestead , became a popular choice.\nSubsequently, Laradock emerged as a containerized PHP development environment, pre-configured with commonly used containers. Laradock can be considered the harbinger of Laravel\u0026rsquo;s container era.\nIn the latest version of Laravel 8 , container environment is directly integrated.\nLaravel Sail is a lightweight CLI that provides a container for Laravel projects built with PHP, MySQL, and Redis.\nIt allows usage of the container without requiring prior Docker experience.\nBut why bother learning Docker and setting up the environment on your own when Laravel Sail is available?\nWell, because it didn\u0026rsquo;t exist initially.\nWhile local development may not encounter situations requiring configuration file adjustments, you may face scenarios where services you need are not provided by Laravel Sail .\nIf you want to switch your online environment to use Docker or need to know how to build an environment with Docker, understanding Docker and configuring related services becomes necessary.\nAfter a lengthy introduction, let\u0026rsquo;s get to the main content.\nSetting up Laravel Project Environment Use docker-compose to construct the Laravel project environment. A Laravel project typically requires the following services:\nnginx php redis PostgreSQL For the database, we\u0026rsquo;ll use PostgreSQL, but if you prefer MySQL, the configuration is also provided in the code.\nChoose one when starting!\n1. docker-compose.yml Start by creating a project folder, let\u0026rsquo;s call it docker_env for clarity. Inside the docker_env folder, create a file named docker-compose.yml to serve as the configuration file for Docker-compose services.\nComplete Code # docker-compose.yml version: \u0026#39;3\u0026#39; networks: server: data: services: nginx: container_name: docker_env_nginx image: nginx:1.18 # stable version networks: - server ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./nginx/conf.d/:/etc/nginx/conf.d/ - ./nginx/ssl/:/ssl/ - \u0026lt;local project path\u0026gt;:/var/www/html/\u0026lt;project name\u0026gt; restart: always php: container_name: docker_env_php build: ./php/ expose: - 9000 networks: - server - data volumes: - /trp/web/home/:/var/trp/ restart: always redis: container_name: docker_env_redis image: redis:6.0 # stable version ports: - \u0026#34;6379:6379\u0026#34; networks: - data restart: always postgre: container_name: docker_env_postgre ports: - \u0026#34;5432:5432\u0026#34; image: \u0026#34;postgres:12.6\u0026#34; volumes: - /trp/database/dev/database_data:/var/lib/postgresql/data # persist data even if container shuts down networks: - data environment: POSTGRES_DB: ${DB_NAME} POSTGRES_USER: ${DB_USER} POSTGRES_PASSWORD: ${DB_PASSWORD} restart: always mysql: container_name: docker_env_mysql ports: - \u0026#34;3306:3306\u0026#34; image: mysql:5.7.23 volumes: - /var/lib/mysql networks: - data environment: MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD} MYSQL_USER: ${DB_USER} MYSQL_PASSWORD: ${DB_PASSWORD} Section Explanation Here, we will explain the contents of the services section and complete the necessary configurations.\nIf you simply copy and paste the above, the services are likely to fail.\nnginx Uses nginx as the web server for our application.\nnginx: container_name: docker_env_nginx image: nginx:1.18 # stable version networks: - server ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./nginx/conf.d/:/etc/nginx/conf.d/ - ./nginx/ssl/:/ssl/ - \u0026lt;local project path\u0026gt;:/var/www/html/\u0026lt;project name\u0026gt; restart: always ports defines the mapping of ports between the host machine and the nginx container.\nIn the provided code, it may not be immediately apparent.\nTo clarify, using 8080:80 means mapping the host\u0026rsquo;s 8080 port to the nginx container\u0026rsquo;s 80 port.\nvolumes block indicates files to be mounted into the Docker container.\nThe third line mounts the specified Laravel project into the container\u0026rsquo;s designated directory.\nIf you don\u0026rsquo;t have a Laravel project yet, create one.\nThe second line mounts SSL certificates if you want to use HTTPS locally.\nYou can generate a self-signed certificate and place it in the docker_env/nginx/ssl/ directory.\nWhile this will display as insecure, it will still be an HTTPS URL.\nThe first line mounts the nginx folder in the same level as the docker-compose.yml into /etc/nginx/conf.d/, which is the directory used by the nginx service for configuration files.\nTherefore, we need to create an nginx config file in the docker_env/nginx/ folder.\n# dev.project.com.conf server { listen 80; listen [::]:80; # Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response. return 301 https://$host$request_uri; } server { listen 443 ssl; listen [::]:443 ssl; root /var/www/html/\u0026lt;laravel project name\u0026gt;/public; index index.php index.html index.htm; ssl_certificate /ssl/ssl.crt; ssl_certificate_key /ssl/ssl.key; server_name dev.project.com; location / { try_files $uri $uri/ /index.php$is_args$args; } location ~ \\.php$ { try_files $uri /index.php =404; fastcgi_pass docker_env_php:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } location ~ /\\.ht { deny all; } } In the above config, the part listening on port 80 immediately redirects to port 443 (HTTPS).\nThe root specifies the location of the project folder, and public is the entry point for Laravel projects.\nThe SSL certificate paths correspond to the mounted paths specified in the volumes section.\nThe fastcgi_pass setting specifies the container_name and port of the container with PHP-FPM installed.\nNote that the names and ports used are not fixed!\nphp php: container_name: docker_env_php build: ./php/ expose: - 9000 networks: - server - data volumes: - /trp/web/home/:/var/trp/ restart: always As the PHP container doesn\u0026rsquo;t need an external port, expose is used to make the container\u0026rsquo;s 9000 port accessible within the Docker network. Volume mounting is similar to the nginx service.\nTo use Laravel, some additional PHP extensions need to be installed. This is done with the build command, which rebuilds our own Docker image.\nIt installs necessary Laravel PHP extensions, and the database extension depends on the chosen database.\nThe example below demonstrates the setup for PostgreSQL.\nHere, we also show how to install Composer. Uncomment the relevant lines if needed.\nHowever, since PHP doesn\u0026rsquo;t require pre-compilation, you can make changes directly in the files mounted by the volumes, enabling internal program modifications without installing Composer inside the container.\nUnless there\u0026rsquo;s a specific reason to run commands inside the container, it\u0026rsquo;s not recommended to install Composer in the container.\nCreate a php folder in the docker_env directory and add a file named Dockerfile with the following content:\n# Dockerfile FROM php:7.4-fpm WORKDIR /var RUN apt-get update \u0026amp;\u0026amp; apt-get install -y libpq-dev libpng-dev libzip-dev zip RUN docker-php-ext-install pgsql pdo_pgsql gd zip\\ \u0026amp;\u0026amp; docker-php-ext-enable opcache # To use mysql, install pdo pdo_mysql instead of pgsql pdo_pgsql # Install composer Latest # RUN php -r \u0026#34;copy(\u0026#39;https://getcomposer.org/installer\u0026#39;, \u0026#39;composer-setup.php\u0026#39;);\u0026#34; \\ # \u0026amp;\u0026amp; php -r \u0026#34;if (hash_file(\u0026#39;sha384\u0026#39;, \u0026#39;composer-setup.php\u0026#39;) === \u0026#39;756890a4488ce9024fc62c56153228907f1545c228516cbf63f885e036d37e9a59d27d63f46af1d4d07ee0f76181c7d3\u0026#39;) { echo \u0026#39;Installer verified\u0026#39;; } else { echo \u0026#39;Installer corrupt\u0026#39;; unlink(\u0026#39;composer-setup.php\u0026#39;); } echo PHP_EOL;\u0026#34; \\ # \u0026amp;\u0026amp; php composer-setup.php \\ # \u0026amp;\u0026amp; php -r \u0026#34;unlink(\u0026#39;composer-setup.php\u0026#39;);\u0026#34; \\ # \u0026amp;\u0026amp; mv composer.phar /usr/local/bin/composer database postgre: container_name: docker_env_postgre ports: - \u0026#34;5432:5432\u0026#34; image: \u0026#34;postgres:12.6\u0026#34; volumes: - /trp/database/dev/database_data:/var/lib/postgresql/data # persist data even if container shuts down networks: - data environment: POSTGRES_DB: ${DB_NAME} POSTGRES_USER: ${DB_USER} POSTGRES_PASSWORD: ${DB_PASSWORD} restart: always mysql: container_name: docker_env_mysql ports: - \u0026#34;3306:3306\u0026#34; image: mysql:5.7.23 volumes: - /var/lib/mysql networks: - data environment: MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD} MYSQL_USER: ${DB_USER} MYSQL_PASSWORD: ${DB_PASSWORD} The volumes setting for the database mainly focuses on data persistence.\nEven if the container is shut down, the data is retained. Database-related environment parameters are set in the environment section.\nAs these parameters may include sensitive information, you can create an .env file in the docker_env folder to store them without version control:\n# .env DB_USER=db_user DB_PASSWORD=db_password DB_NAME=my_database The keys on the left side can be called in the docker-compose.yml file using ${key}, allowing retrieval of the values set on the left side.\nStarting the Containers After these steps, with Docker service running, navigate to the docker_env folder and run the command:\ndocker-compose up -d If everything is correct, the containers will start in the background.\nThis completes the goal of \u0026ldquo;Setting up Laravel Environment with Docker!\u0026rdquo;\nTo check the container status, you can use the command:\ndocker ps -a If I were personally writing a new docker-compose.yml configuration, I would first use:\ndocker-compose up to display logs in real-time. This makes debugging much easier.\nCommon issues might include missing volume directories like nginx config, SSL, or the Laravel project.\nLaravel .env Configuration After successfully starting the Docker environment, configure the Laravel project\u0026rsquo;s environment file.\nOpen the .env file in the root directory and modify its content.\nIn the example below, PostgreSQL is used for the database configuration, and it aligns with the settings in the docker-compose.yml file:\n# .env DB_CONNECTION=pgsql DB_HOST=docker_env_postgre DB_PORT=5432 DB_DATABASE=my_database DB_USERNAME=db_user DB_PASSWORD=db_password Now, Laravel can access the database container through the internal Docker network.\nSetting Up Local Test URL With Docker setting up Laravel development environment - Done\nConnecting Laravel to Docker Database - Done\nNow, we need to make sure that we can open the local Laravel project in the browser.\nOpen your terminal, whether it\u0026rsquo;s bash or zsh:\nsudo vim /etc/hosts Add the following line in /etc/hosts:\n127.0.0.1 dev.project.com This line directs requests for dev.project.com to the localhost IP.\nWhen entering dev.project.com in the browser\u0026rsquo;s address bar, the request will be sent to the localhost.\nBut how does this make the Laravel project visible? This is because in the earlier nginx config settings, a server_name was defined:\nserver_name dev.project.com; So, when the Docker nginx service, listening on ports 80 and 443, detects an incoming request that matches the server_name configuration, it processes the request.\nIf you have another nginx config file with default_server appended to the listen port, like this:\nlisten 80 default_server; It means that if no other nginx config rules match, nginx will use this configuration. If all config files don\u0026rsquo;t\nset default_server, then the first config file sorted alphabetically will be used.\nWith this setup, you should be able to access the Laravel project\u0026rsquo;s interface.\nRunning Laravel Commands If you want to run Laravel commands related to the database, such as:\nphp artisan migrate You first need to enter the PHP container:\ndocker exec -ti docker_env_php bash Inside the container, navigate to the project directory and run the command.\nIf successful, you\u0026rsquo;ll see the command executed correctly.\nIf you run the migrate command without entering the container, you\u0026rsquo;ll encounter an error message like:\nSQLSTATE[08006] [7] could not translate host name \u0026#34;docker_env_postgre\u0026#34; to address: nodename nor servname provided, or not known This is because the program doesn\u0026rsquo;t recognize the host \u0026ldquo;docker_env_postgre.\u0026rdquo;\nIf we modify the .env file\u0026rsquo;s DB_HOST to the real location using the docker inspect command, it would work, but this is not a sustainable solution.\nIf you find it cumbersome to enter the container or want to run commands directly in the local project folder, what should you do? Modify the etc/hosts file again by adding:\n127.0.0.1 docker_env_postgre Now you\u0026rsquo;re done! Running migrate in the local project folder will direct it to the local container.\nThe above is the entire content of this guide on \u0026ldquo;Building a Laravel Development Environment with Docker.\u0026rdquo;\nIt took several hours to complete this piece, and it\u0026rsquo;s a culmination of a lot of server knowledge.\nIf you\u0026rsquo;re not familiar with these concepts, it\u0026rsquo;s recommended to understand them before proceeding.\nDue to space limitations, I could only provide a brief explanation of the configurations and skip many details and practices.\nThere are also many aspects not covered, such as Docker network segmentation and usage.\n",
    "ref": "/en/blog/202107-laravel-environment-with-docker/"
  },{
    "title": "Environment Initialization and Server Configuration (Docker + Laravel)",
    "date": "",
    "description": "Setting up the basic Docker and Laravel execution environment on a new server",
    "body": "If you are setting up a new cloud server and want to create a Laravel development environment using Docker on it.\nAnd run Laravel\u0026rsquo;s common composer and artisan commands remotely, here\u0026rsquo;s a guide for you.\nSince these steps are not frequently executed, documenting them makes it convenient for future reference.\nIt also lays the groundwork for the upcoming article on using Docker to build a Laravel environment.\n1. Establish Repository Access Permissions To allow the remote server to access a specific repository, you need to add an SSH key.\nStandard Method: After SSH logging into the remote server, generate an SSH key on the remote machine using the following command:\nssh-keygen Then add the public key to the remote repository.\nUnrecommended Method: Alternatively, you can copy an existing key pair with repository access to the remote server.\nIf the .ssh folder doesn\u0026rsquo;t exist, create it with permissions set to 700.\nmkdir .ssh chmod 700 .ssh cd .ssh vim id_rsa chmod 600 id_rsa vim id_rsa.pub chmod 644 id_rsa.pub 2. Update Linux Packages Since we\u0026rsquo;ll be installing packages later, it\u0026rsquo;s a good practice to update the server\u0026rsquo;s package list.\nsudo apt update 3. Install Docker Since we\u0026rsquo;re creating a Docker environment, it\u0026rsquo;s essential to install Docker.\nsudo apt install docker.io 4. Install Docker-Compose We\u0026rsquo;ll use docker-compose to orchestrate multiple Docker containers and launch a complete service.\nAs we\u0026rsquo;ll be starting various containers like nginx, PHP, redis, mysql for future Laravel projects, managing them with docker-compose will be more convenient.\nsudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 5. Install PHP Packages For Laravel projects, where we don\u0026rsquo;t need compilation, changes made outside the container will reflect inside it.\nTherefore, it\u0026rsquo;s not always necessary to execute commands within the container.\nTo run php artisan and phpunit directly on the host and install/execute composer, install PHP based on your requirements.\napt install php7.4-cli php7.4-gd install php7.4-mbstring php7.4-curl php7.4-xml php7.4-zip If you\u0026rsquo;re unsure about the package names, you can use tab completion after apt install php7.4 to list all packages starting with \u0026ldquo;php7.4,\u0026rdquo; making it easy to find the desired version.\n6. Install PHP Package Manager: Composer Follow the official steps for installation: Download Composer Example (official website should be the primary reference for the correct commands):\nphp -r \u0026#34;copy(\u0026#39;https://getcomposer.org/installer\u0026#39;, \u0026#39;composer-setup.php\u0026#39;);\u0026#34; php -r \u0026#34;if (hash_file(\u0026#39;sha384\u0026#39;, \u0026#39;composer-setup.php\u0026#39;) === \u0026#39;756890a4488ce9024fc62c56153228907f1545c228516cbf63f885e036d37e9a59d27d63f46af1d4d07ee0f76181c7d3\u0026#39;) { echo \u0026#39;Installer verified\u0026#39;; } else { echo \u0026#39;Installer corrupt\u0026#39;; unlink(\u0026#39;composer-setup.php\u0026#39;); } echo PHP_EOL;\u0026#34; php composer-setup.php php -r \u0026#34;unlink(\u0026#39;composer-setup.php\u0026#39;);\u0026#34; Move composer to the environment variable for global usage.\nIt\u0026rsquo;s recommended to perform this step.\nOtherwise, you can only execute commands within the installation path where composer.phar is located.\nsudo mv composer.phar /usr/local/bin/composer To view globally executable commands and their corresponding directories:\necho $PATH 7. Set .sh File as Executable If there are shell scripts to be executed in the project, you must modify the file permissions.\nchmod 755 {target file path} With these steps, you can now execute the following commands on the host machine:\ndocker composer php artisan This completes the foundational setup needed for using Docker to build a Laravel environment in the future.\n",
    "ref": "/en/blog/202107-initialize-server-setting-with-docker-and-laravel/"
  },{
    "title": "[CI/CD with Drone 101] 04 Introduction to Drone Runners",
    "date": "",
    "description": "Building an automated deployment process with Drone, introducing different drone runners.",
    "body": "In our previous post, \u0026ldquo;[CI/CD with Drone 101] 01 Basic Service Setup and GitHub Integration \u0026rdquo; there is an example docker-compose.yml file that creates two Docker runners: drone_runner_docker and drone-runner-ssh.\nHowever, in \u0026ldquo;[CI/CD with Drone 101] 02 Setting Deployment Trigger Conditions (Pipeline) \u0026rdquo; only the Docker runner is used.\nThis post will introduce both runners, along with other runners mentioned on the official website.\nAll runners can be found on the official runner overview page. The default runner is the Docker runner.\nThe runners listed during the article edit were:\nDocker Runner Kubernetes Runner Exec Runner SSH Runner Digital Ocean Runner Macstadium Runner SSH Runner Example: kind: pipeline type: ssh name: default server: host: from_secret: GCP_IP_HOST user: from_secret: TESTING_USER ssh_key: from_secret: TESTING_PRIVATE_SSH_KEY steps: - name: greeting commands: - echo hello world - whoami - pwd - echo DRONE_REPO = ${DRONE_REPO} - echo DRONE_BRANCH = ${DRONE_BRANCH} Explanation When the type is set to ssh, it means using the SSH runner.\nIn the server block, the parameters required for SSH are specified, including the remote host IP location and the username for login. from_secret: SSH_KEY means retrieving the value from the Secrets set in the Drone service backend.\nIf SSH login to the host is required, the id_rsa for the host needs to be filled in here. Additionally, an authorized_keys file with 600 permissions must be created in the user\u0026rsquo;s directory for login, inside the .ssh directory.\nThe file content should be the same as id_rsa.pub so that Drone\u0026rsquo;s SSH runner can correctly use the specified user\u0026rsquo;s SSH key to log in to the remote host.\nThe advantage of this approach is that sensitive information does not need to be known by all project deployers; they just need to know the name of the secrets index being used.\nIf there are future modifications, they can be done in the Drone backend without needing to modify files for each project.\nSecrets Official Documentation Of course, it is also possible to use a password instead of SSH to log in to the remote host. Further configurations can be found in the official documentation .\nDocker Runner This runner uses a specified image to create a container and executes a specified action within the container.\nIt is the default runner and is recommended for beginners.\nIf multiple pipelines need to perform actions on the host\u0026rsquo;s files, this may not be suitable because Docker pipelines run in Docker containers, isolated from the physical host, and do not directly impact the files on the host.\nExample kind: pipeline type: docker name: backend_dev steps: - name: submodules update image: alpine/git commands: - whoami - pwd - date - echo DRONE_REPO = ${DRONE_REPO} - echo DRONE_BRANCH = ${DRONE_BRANCH} - echo DRONE_COMMIT = ${DRONE_COMMIT} - echo DRONE_BUILD_NUMBER = ${DRONE_BUILD_NUMBER} - git submodule update --init --recursive trigger: branch: - dev event: - push Other Runners Exec Runner Use Cases Not suitable for projects that need to run outside containers, for example, MacOS projects.\nWhen Not to Use Since the Exec runner does not isolate with the host, all operations are directly performed on the service\u0026rsquo;s constructed host.\nIf the project and the Drone service are not on the same host, or if dangerous commands are added to the .drone.yml in the project, it can lead to tragedy. Therefore, generally, Docker runners are used by default.\nAdditionally, the Exec runner is in Beta and is not recommended for use in a production environment.\nExample In the example below, commands are directly executed on the host when the pipeline is triggered!\n--- kind: pipeline type: exec name: default steps: - name: backend commands: - go build - go test - name: frontend commands: - npm install - npm test These are the commonly used Drone runners, and it is recommended to use the Docker runner.\nMost operations needed have readily available Docker images.\nUnless absolutely necessary, avoid using the other two runners to directly manipulate the host file system or execute commands directly on the host.\nI haven\u0026rsquo;t used Kubernetes yet, so it is not covered in this introduction.\nIf there is an opportunity in the future, I will provide additional information.\n",
    "ref": "/en/blog/202107-drone-cicd-4-advanced-runner/"
  },{
    "title": "[CI/CD with Drone 101] 03 Deployment Schedule Configuration and Permission Management",
    "date": "",
    "description": "Tutorial on configuring deployment schedules and managing permissions through Drone for automated deployment.",
    "body": "Utilizing Drone to establish a custom automated deployment service.\nDrone is a CI/CD system tool developed in Golang.\nThis tutorial focuses on configuring deployment schedules and managing permissions.\nScheduled Execution (Cron Job) Setting up schedules was quite challenging initially. If setting up the service itself took three days, the schedule configuration took about half a month with constant document checking, forum discussions, testing, and debugging.\nMany details were only addressed by the author on Gitter or in forum threads, without being included in the official documentation.\nAdditionally, certain features were not supported for those using Bitbucket.\nCustom Execution Time Service Limitations In Drone 1.0, customizing the schedule on the interface is not allowed due to potential errors (source ).\nAlso, the minimum repeat execution time is once every half an hour (source ).\nConfiguration in Project Files The author suggests defining the cron job name in the project\u0026rsquo;s .drone.yml file using the CLI, rather than using expressions (Cron job definition ).\nTimezone in Expressions is Fixed The official documentation mentions that the timezone for scheduling is fixed to UTC (i.e., +0 timezone), so manual conversion is necessary (Cron Timezones ).\nThe current implementation calculates the execution time based on UTC as opposed to local time.\nHowever, the official documentation also notes that custom schedule times may not be triggered at precise times and may have a slight deviation (reference ).\nCreating a Cron Job While many tutorials use the Drone CLI, it was discovered on the official website that an API can also be used for creating cron jobs (API drone cron create ).\nTo do this, obtain the token from the User Settings on the Drone page to use as the header in the Post request.\nIn Postman, create a POST /api/repos/{owner}/{repo}/cron request and add the token to the Headers tab, where the token key is Authorization and the corresponding value has the format Bearer {a string of alphanumeric characters}.\nThe Body is then used to set the cron job name, execution conditions, and the branch to be executed.\n{ \u0026#34;name\u0026#34;: \u0026#34;every2hour\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;0 0 */2 * * *\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34; } In this example, the condition is 0 0 8-16,*/2 * * *, meaning \u0026ldquo;execute every two hours from 8 AM to 4 PM\u0026rdquo;.\nAlternatively, it can be set as 0 0 10,12,14,16 * * * to \u0026ldquo;execute every day at 10 AM, 12 PM, 2 PM, and 4 PM\u0026rdquo;.\nAn online tool can be used to generate conditions based on requirements.\nNote! The author uses a time format with six digits, including seconds!\nViewing Cron Jobs Cron List After creation, you can use GET /api/repos/{owner}/{repo}/cron to view the list of all cron jobs.\nCron Info Using GET /api/repos/{owner}/{repo}/cron/{name} to view detailed information about a specific cron job.\nReturning to the Drone backend, accessing the settings page for the project reveals the cron job created using the API!\nWhen the scheduled time arrives, the pipeline triggered by the specified cron job name in the .drone.yml file will be executed.\nUpdating Cron Jobs To update a cron job, use PATCH /api/repos/{owner}/{repo}/cron/{name}.\nThe body includes the changes, such as switching the branch to dev or adjusting the execution time to a fixed time each day.\n{ \u0026#34;name\u0026#34;: \u0026#34;every2hour\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;0 0 2,4,6,8 * * *\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;dev\u0026#34; } Deleting Cron Jobs Cron jobs created previously can be deleted using DELETE /api/repos/{owner}/{repo}/cron/{name}.\nHowever, the author prefers using the UI interface to click on DELETE.\nAnother option is the Drone CLI, which was not used this time but is provided for reference.\nDrone CLI Cron CLI drone cron add Permission Control If no user is set, the service is open to the public on the online URL (official explanation ).\nDrone registration is open by default. This means any user can register an account and use the system.\nAdmin Permission Configuration Admin rights must be set up in the drone_server to manage other accounts (DRONE_USER_CREATE ).\n- DRONE_USER_CREATE=username:ray247k,admin:true User List Configuration Restrict access by setting a user list (official documentation ). Use organizations or user IDs in the list.\n- DRONE_USER_FILTER=ray247k,lemon,JohnDoe However, if Bitbucket is used as the version control platform, organization restriction is not supported (forum thread explanation ). Therefore, anyone with write permissions can access the settings page.\nTesting with GitHub shows that organization-based access control is possible and the setting page is not open to everyone.\n",
    "ref": "/en/blog/202106-drone-cicd-3-advanced-cron-job/"
  },{
    "title": "[CI/CD with Drone 101] 02 Setting Deployment Trigger Conditions (Pipeline)",
    "date": "",
    "description": "Setting up deployment trigger conditions (pipeline) through Drone for automated deployment.",
    "body": "Utilizing Drone to establish a custom automated deployment service.\nDrone is a CI/CD system tool developed in Golang.\nIn this article, we\u0026rsquo;ll configure multiple trigger conditions to initiate different deployment steps under various circumstances.\nMultiple Trigger Conditions In some projects, different events need to be performed on specific branches.\nThe following configuration can be referred to for such scenarios.\n.drone.yml\n--- ################################################ # dev: Update on every push to the branch # ################################################ kind: pipeline type: docker name: backend_dev steps: - name: submodules update image: alpine/git commands: - date - echo DRONE_REPO = ${DRONE_REPO} - echo DRONE_BRANCH = ${DRONE_BRANCH} - echo DRONE_COMMIT = ${DRONE_COMMIT} - echo DRONE_BUILD_NUMBER = ${DRONE_BUILD_NUMBER} - git submodule update --init --recursive - name: composer install image: composer:1.10.19 commands: - composer install --ignore-platform-reqs trigger: branch: - dev event: - push --- ################################################ # stage: Execution based on schedule set # ################################################ kind: pipeline type: docker name: backend_stage steps: - name: submodules update image: alpine/git commands: - date - echo DRONE_REPO = ${DRONE_REPO} - echo DRONE_BRANCH = ${DRONE_BRANCH} - echo DRONE_COMMIT = ${DRONE_COMMIT} - echo DRONE_BUILD_NUMBER = ${DRONE_BUILD_NUMBER} - git submodule update --init --recursive - name: composer install image: composer:1.10.19 commands: - composer install --ignore-platform-reqs trigger: event: - cron cron: - every2hour --- ################################################ # production: Deploy using git tag triggering # ################################################ kind: pipeline type: docker name: backend_production steps: - name: submodules update image: alpine/git commands: - date - echo DRONE_REPO = ${DRONE_REPO} - echo DRONE_TAG = ${DRONE_TAG} - echo DRONE_COMMIT = ${DRONE_COMMIT} - echo DRONE_BUILD_NUMBER = ${DRONE_BUILD_NUMBER} - git submodule update --init --recursive - name: composer install image: composer:1.10.19 commands: - composer install --no-dev --ignore-platform-reqs trigger: event: - tag If the dev branch is pushed, only the dev_check_info and dev_run_update actions will be executed.\nConversely, actions for pushing to the testing branch will wait until the specified schedule every2hour is triggered before execution.\nThis setup allows for different operations based on different branches, which is useful when, for example, deciding whether to include debugging mode in the startup commands.\nIf different environments are deployed on different servers or have different deployment processes, this configuration proves to be convenient.\nEach pipeline operates independently, executing different steps based on the trigger settings.\nPushing code to the dev branch will only trigger the backend_dev pipeline and its internal steps.\nAlternatively, tagging in version control, when a new tag is pushed and detected by the webhook, triggers the backend_production.\nCron Jobs can also be established for scheduled executions, as demonstrated by the backend_stage setting, which listens for the every2hour schedule.\nFor more condition settings, refer to the official documentation: Pipelines Conditions Learn more about Pipelines: Pipelines Overview ",
    "ref": "/en/blog/202106-drone-cicd-2-advanced-pipeline-configuration/"
  },{
    "title": "[CI/CD with Drone 101] 01 Basic Service Setup and GitHub Integration",
    "date": "",
    "description": "Setting up automated deployment process with Drone, including basic service configuration and GitHub integration",
    "body": "Utilizing Drone to establish your own automated deployment service.\nDrone is a CI/CD system tool developed in Golang.\nThis article covers the configuration of basic services and integration with GitHub.\nWhy use Drone Supports major Git repositories: GitHub, GitLab, Bitbucket, Gitea, etc.\nIf your project uses Git version control, integrating with Drone is straightforward. CI process is described in a YAML file, easy to understand, and flexible to adjust.\nAnyone with experience in Docker Compose should be able to get started quickly. Developed in Go, the service starts up very quickly. Docker-based service, can be quickly ported to different platforms.\nEspecially suitable for running in Kubernetes cluster environments. Drone\u0026rsquo;s community has various plugins available, and custom plugins can be created in various languages.\nJust package it into an image to easily integrate it into the CI process. Divided into server and runner, the server is responsible for collecting events from Git repositories.\nWhen it detects conditions defined in the pipeline, such as a push to a specific branch, it triggers the pipeline process, assigning it to different types of runners for execution.\nCommon runners include Docker runner, Kubernetes runner, SSH runner, etc.\nBasic Setup Locally use docker-compose to set up the Drone 1.10 service (as version 1.0 does not have an SSH runner).\nUse ngrok to allow external connections to reach localhost on port 8089, serving as the callback URL for GitHub webhook.\nNote: Ngrok is restarted several times in between; some screenshots may have different ngrok URLs.\nAny URL ending with ngrok.io in the article is the externally accessible URL provided by ngrok service.\nSTEP 1 docker-compose.yml version: \u0026#39;3.7\u0026#39; services: drone-server: container_name: drone_server image: drone/drone:1.10 ports: - 8089:80 volumes: - /var/lib/drone:/data - /var/run/docker.sock:/var/run/docker.sock restart: always environment: - DRONE_GITHUB_CLIENT_ID=${DRONE_GITHUB_CLIENT_ID} - DRONE_GITHUB_CLIENT_SECRET=${DRONE_GITHUB_CLIENT_SECRET} - DRONE_AGENTS_ENABLED=true - DRONE_RPC_SECRET=${DRONE_RPC_SECRET} - DRONE_SERVER_HOST=${DRONE_SERVER_HOST} - DRONE_SERVER_PROTO=${DRONE_SERVER_PROTO} - DRONE_TLS_AUTOCERT=${DRONE_TLS_AUTOCERT} - DRONE_CRON_INTERVAL=1m - DRONE_USER_CREATE=${DRONE_USER_CREATE} - DRONE_USER_FILTER=${DRONE_USER_FILTER} drone-ssh-runner: container_name: drone-runner-ssh image: drone/drone-runner-ssh depends_on: - drone-server environment: - DRONE_RPC_HOST=${DRONE_SERVER_HOST} - DRONE_RPC_SECRET=${DRONE_RPC_SECRET} - DRONE_RPC_PROTO=${DRONE_SERVER_PROTO} - DRONE_RUNNER_CAPACITY=${DRONE_RUNNER_CAPACITY} drone-docker-runner: container_name: drone_runner_docker image: drone/drone-runner-docker:1 depends_on: - drone-server volumes: - /var/run/docker.sock:/var/run/docker.sock environment: - DRONE_RPC_HOST=${DRONE_SERVER_HOST} - DRONE_RPC_SECRET=${DRONE_RPC_SECRET} - DRONE_RPC_PROTO=${DRONE_SERVER_PROTO} - DRONE_RUNNER_CAPACITY=${DRONE_RUNNER_CAPACITY} You can save some sensitive information in a .env file at the same level as docker-compose.yml.\nFor example, ${DRONE_GITHUB_CLIENT_SECRET} can be kept outside of version control.\nFor details on the configuration parameters, refer to the official documentation .\nSet DRONE_CRON_INTERVAL=1m to make the Drone server check for pending cron jobs every minute. The default is 30m, so some users may experience delayed execution of cron jobs after service startup. It\u0026rsquo;s not a delay in execution but rather the behavior of the Drone server not triggering cron detection promptly: Reference .\nDefine the parameters directly in the .env file. Note that DRONE_SERVER_HOST does not need to include the protocol.\n.env # Configuration for GITHUB OAuth App DRONE_GITHUB_CLIENT_ID= DRONE_GITHUB_CLIENT_SECRET= # Admin account settings DRONE_USER_CREATE=username:ray247k,admin:true # People with access permissions; for GitHub, you can use an organization for configuration # Not applicable for Bitbucket DRONE_USER_FILTER=ray247k,lemon # Drone Server URL; using ngrok here as an example DRONE_SERVER_HOST=56662cd578da.ngrok.io # If using a reverse proxy with self-signed certificates, set this to http and false DRONE_SERVER_PROTO=https DRONE_TLS_AUTOCERT=true # Random password for security DRONE_RPC_SECRET={random_password} # Number of concurrent jobs; must not be 0 DRONE_RUNNER_CAPACITY=3 # Interval for checking pending cron jobs DRONE_CRON_INTERVAL=1m Generate DRONE_RPC_SECRET using the following command:\nopenssl rand -hex 16 STEP 2 If you already have a URL available, you can skip this step.\nExpose IP with Ngrok for External Connections Bind the localhost port 8089 of the recently set up Drone server to a specific ngrok URL.\nThis URL will need to be copied and pasted into the GitHub OAuth App settings later.\nngrok http 8089 STEP 3 - GitHub OAuth App Configuration Navigate to the GitHub OAuth App settings page to create a new OAuth application.\nProfile Picture (top-right)-\u0026gt;Settings-\u0026gt;Developer settings tab on the left-\u0026gt;OAuth Apps\nStart by creating a new OAuth App.\nEnter the ngrok URL you just created and register the OAuth application.\nSet the callback URL to /login.\nAfter creating, it\u0026rsquo;s time to integrate Drone CI with GitHub.\nSTEP 4 Modify Drone Configuration Open the GitHub APP settings you just created.\nModify the DRONE_GITHUB_CLIENT_ID and DRONE_GITHUB_CLIENT_SECRET in the docker-compose.yml file to match the values of Client ID and Client Secret on the GitHub App settings page.\nAfter making the changes, restart the Drone service using docker-compose.yml, and access your ngrok URL to check the setup.\nOnce you successfully authenticate, it means your Drone CI integration is complete! Congratulations!\nSTEP 5 Activate Services Following the previous steps, you should be able to open your ngrok URL and see the Drone dashboard.\nUpon entering, you\u0026rsquo;ll find all your repositories.\nChoose any repository, click on it, and press the \u0026ldquo;ACTIVATE REPOSITORY\u0026rdquo; button in the center to enable the webhook for the project.\nTo confirm, you can open the \u0026ldquo;Repository -\u0026gt; Settings -\u0026gt; Webhooks\u0026rdquo; on GitHub, and you should see a webhook in use.\nClicking on it will reveal more settings or allow you to view the requests sent by the webhook.\n",
    "ref": "/en/blog/202106-drone-cicd-1-basic-usage/"
  },{
    "title": "Common Docker Commands",
    "date": "",
    "description": "Introduction to basic Docker operations and commonly used commands",
    "body": "Some commonly used commands and related parameters on Docker and their usage.\nIn actual use, there are definitely more commands than those listed here.\nBasic Service Commands docker stats Display container resource usage statistics docker start Start one or more stopped containers docker stop Stop one or more running containers docker restart Restart one or more containers Log Commands $ docker logs [OPTIONS] CONTAINER Options: --details Show extra details -f, --follow Follow log output in real-time --since string Show logs since a specific timestamp or relative time, e.g., 42m (42 minutes) --tail string Number of lines to show from the end of the logs, default is all -t, --timestamps Show timestamps --until string Show logs until a specific timestamp or relative time, e.g., 42m (42 minutes) Usage Examples View logs after a specific timestamp\n$ docker logs -t --since=\u0026#34;2018-02-08T13:23:37\u0026#34; {CONTAINER_ID} Display logs after a specific timestamp and only the last 100 lines\n$ docker logs -f -t --since=\u0026#34;2018-02-08\u0026#34; --tail=100 {CONTAINER_ID} View logs for the last 30 minutes\n$ docker logs --since 30m {CONTAINER_ID} View logs within a specific time range\n$ docker logs -t --since=\u0026#34;2018-02-08T13:23:37\u0026#34; --until \u0026#34;2018-02-09T12:23:37\u0026#34; CONTAINER_ID Running Containers Create a container from a local environment image. If the image does not exist locally, it will be pulled from the repository.\nPrivate repos require a \u0026ldquo;login\u0026rdquo; action. The image format can be [IMAGE NAME]:[TAG].\ndocker run Examples: # Run nginx docker run -idt nginx # Run nginx and expose port 8080 docker run -idt -p 8080:80 nginx # Run nginx on port 8080, automatically restart on failure docker run -idt -p 8080:80 --restart on-failure nginx # Run nginx and remove after it exits docker run --rm -idt nginx Commonly Used Flags `-d` detach: Run container in the background `-i` interactive: Keep STDIN open even if not attached `-t` allocate a pseudo-TTY `-p` expose port (e.g., -p 8080:80) `-p Host Port:Docker Port` `--restart` restart policy (always, none, on-failure) `--rm` automatically remove the container when it exits Container-related Commands List all running containers\ndocker ps List all containers (running and stopped)\ndocker ps -a Execute a command in a running container\ndocker exec -ti nginx bash Get metadata of a container or image\ndocker inspect {container name} Remove a Docker container\ndocker rm [CONTAINER] Image-related Commands List all images\ndocker images Remove a specified image\ndocker rmi [IMAGE] If you want to expose the service to others\ndocker run -i -t -d -p 80:80 nginx If the service encounters unexpected interruption\ndocker run -i -t -d -p 80:80 --restart always nginx If you want to link with node.js\ndocker run -i -d -t -p 80:80 --link node nginx Volume-related Commands List all volumes\ndocker volume ls Remove a specified volume\ndocker volume rm [OPTIONS] VOLUME [VOLUME...] Remove \u0026ldquo;all\u0026rdquo; volumes\ndocker volume prune Other Commands docker build Build a Docker image from a Dockerfile -f manually specify the Dockerfile name\n# Build an image in the current directory using Dockerfile docker build . -t image-name # Build an image in the current directory using Dockerfile-alt docker build . -t image-name -f Dockerfile-alt docker push / pull Push or pull a Docker image to/from a repository Login to Docker repository first using docker login.\ndocker login Login to a Docker repository to push or pull images.\n",
    "ref": "/en/blog/202106-docker-common-commands/"
  },{
    "title": "About",
    "date": "",
    "description": "About Byte Ebi",
    "body": "Familiar with PHP including CodeIgniter, WordPress, Laravel.\nKnow a little about frontend and have experience with container.\nNow I am using Golang most of the time. And studying for AWS services.\nMake some small tool in my free time.\nFeel free to contact me via LinkedIn. I wanna have some foreign friends.\nLove Japanese culture after I star watching Vtuber.\nIf you want to talk about that, please do not hesitate to contact me.\n",
    "ref": "/en/about/"
  }]
