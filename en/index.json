[{
    "title": "Different between pointer and value for struct type in golang",
    "date": "",
    "description": "The different between pointer and value for struct type in golang with examples",
    "body": "I always had a question \u0026ldquo;Should I use value for type in struct? Or using pointer?\u0026rdquo;\nAfter reading many different projects on the web, it seems that they all have different practices.\nUntil I recently encountered a problem that gave me some ideas.\nIn our projects, we often start by defining the Request incoming Struct\nThen use json.Unmarshal to convert the incoming json string into Struct for subsequent operations\nPreviously, the default value for fields was false, so we pass by value.\nIn this case, the default value is true and the problem happened.\nTake the first look at the code and execution results\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type John struct { Name string `json:\u0026#34;name\u0026#34;` CanRun bool `json:\u0026#34;canRun\u0026#34;` CanFly bool `json:\u0026#34;canFly\u0026#34;` } type Doe struct { Name string `json:\u0026#34;name\u0026#34;` CanRun *bool `json:\u0026#34;canRun\u0026#34;` CanFly *bool `json:\u0026#34;canFly\u0026#34;` } func main() { var param = []byte(`{\u0026#34;Name\u0026#34;:\u0026#34;John Doe\u0026#34;, \u0026#34;canRun\u0026#34;:true}`) defult := true var john = new(John) json.Unmarshal(param, \u0026amp;john) jsondata, _ := json.Marshal(john) fmt.Println(string(jsondata)) var doe = new(Doe) json.Unmarshal(param, \u0026amp;doe) doejsondata, _ := json.Marshal(doe) fmt.Println(string(doejsondata)) if doe.CanFly == nil { doe.CanFly = \u0026amp;defult } doejsondata, _ = json.Marshal(doe) fmt.Println(string(doejsondata)) } In the example, there are two Structs.\nJohn, whose Type is a Bollinger pass by value, and Doe, whose Type is a Bollinger pass by pointer.\nIf the CanFly field is \u0026ldquo;optional\u0026rdquo; and the default value is \u0026ldquo;true\u0026rdquo;\nThe param in the example code play as a request without CanFly parameter\nAssign the variables of type \u0026ldquo;John\u0026rdquo; and \u0026ldquo;Doe\u0026rdquo; Use json.Unmarshal to convert the incoming json string to Struct Convert to json strings via json.Marshal for easy reading and printing on the screen The first line of the output is \u0026ldquo;John\u0026rdquo;, which using the boolean value as type.\nWhen calling new(John) will automatically bring in the zero value .\nAnd the default zero value of boolean is false.\nAnd Name and CanRnu are overwritten with zero value because they are passed in.\nBut because CanFly is optional and not passed in by the user, it is still false\nWe can\u0026rsquo;t identify the false is from user or from zero value.\nIf you use \u0026ldquo;pointer\u0026rdquo; as the type in Struct, you can see that the second line of CanFly is null.\nSo you can set the default value to \u0026ldquo;true\u0026rdquo; later on.\nThis is what I learned about the default value problem when converting json strings to Struct, and how to solve it with a pointer.\n",
    "ref": "/en/blog/202306-different-between-pointer-and-value-for-struct-type-in-golang/"
  },{
    "title": "Using FFmpeg to convert media files",
    "date": "",
    "description": "Convert .ts video files to .mp4",
    "body": "Sometimes, we can use certain methods to download the .M3U8 file and then merge it into a .ts file.\nHowever, computers usually cannot directly play this format.\nWe can use FFmpeg to convert it into a common .mp4 file for easy playback.\nInstallation First, we need to download FFmpeg. If you are a Mac user, you can install it using Homebrew .\nbrew install ffmpeg To check if the installation is successful, you can use the following command:\nffmpeg -version Convert media files Once you have installed it, the next step is file conversion.\nUse the following command and replace the source .ts file and the destination path with your own target path:\nffmpeg -i /source_path/vedio.ts -acodec copy -vcodec copy -f mp4 /target_path/new_vedio.mp4 After executing the command, you will be able to see the converted file in the target path!\nWith FFmpeg, you no longer need to install additional software or rely on online file conversion services.\n",
    "ref": "/en/blog/202301-ffmpeg-video-convert/"
  },{
    "title": "[AWS 101] What are SNS and SQS",
    "date": "",
    "description": "Briefly note what AWS SNS and SQS services do and their use cases",
    "body": "Amazon Simple Queue Service (SQS) and Amazon Simple Notification Service (SNS) may look similar.\nBut in reality, they are quite different.\nTo better clarify the differences between the two, let\u0026rsquo;s compare the notes on both services side by side.\nAWS SNS Full name is: Amazon Simple Notification Service\nThe publisher system can use Topics to distribute event messages to a large number of subscriber systems for parallel processing.\nThe subscribers include:\nAmazon SQS AWS Lambda function HTTPS endpoint Amazon Kinesis Data Firehose The architecture looks like this:\nFeatures: Send real-time messages to all subscribers. Do not retain sent messages. Follow the Publish–subscribe pattern. AWS SQS Full name is: Amazon Simple Queue Service (SQS)\nIt is a fully managed message queue service.\nMessages are sent to a queue and processed by the recipient who retrieves them.\nIt is a Serverless solution, but the executor can be either Lambda function or non-Serverless projects like Laravel.\nType Two types of queues are provided to cater to different application requirements:\nStandard Queue The application can handle messages that arrive more than once and are not ordered sequentially.\nUnlimited Throughput: Standard queues support nearly unlimited transactions per second (TPS) for each API action. At-Least-Once Delivery: A message is guaranteed to be delivered at least once, but occasionally, multiple copies of the message may be delivered. Best-Effort Ordering: Messages are occasionally not delivered in the exact order they were sent. FIFO Queue When the order of operations and events is crucial or duplicates are not acceptable:\nHigh Throughput: By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). Exactly-Once Processing: Messages are delivered exactly once and remain available until the consumer processes and deletes them. First-In-First-Out Delivery: Messages are strictly delivered and received in the order they were sent (First In, First Out - FIFO). Poll Short polling This method is similar to someone repeatedly asking you if there is any job available, even if there is no job.\nYou will get immediate results after each inquiry, even if the queue is empty.\nHowever, to ensure you receive the latest status, you need to keep querying continuously.\nLong polling After making a request, you will not receive an immediate response unless there is a timeout or a message in the queue.\nThis approach minimizes additional polling to reduce costs while ensuring the fastest possible reception of new messages.\nWhen the queue is empty, a long-polling request for the next message can wait for a maximum of 20 seconds.\nFeatures: The receiver actively polls messages. A queue can only be associated with a single consumer. The message is deleted only after the consumer responds with the completion of the processing. Difference between SNS and SQS SNS When new content is updated, event notifications are sent to all subscribers.\nIt follows a push-based architecture where messages are automatically pushed to the subscribers.\nIn simple terms, it is a broadcast notification. SQS Separating queue tasks from the codebase involves a Pull-Based architecture\nConsumers are responsible for pulling messages from the queue and processing them on their own.\nIn simple terms, it is a Queue Here\u0026rsquo;s an example of how these features and architectures can be combined: 1. Methods for creating an SNS Topic and SQS: Access the SNS service and create an SNS topic. Access the SQS service and create an SQS queue. Subscribe the newly created SQS queue to the SNS topic just created to retrieve podcast content. In the SQS Dashboard, select the target SQS queue. Click on the Action dropdown menu and choose Subscribe to Amazon SNS topic. Select the topic and click \u0026ldquo;Save.\u0026rdquo; 2. Simulate sending a message from SNS 到 SNS Topic 頁面並選擇 Topic 後，點選右上角Publish message 輸入測試用的內容，並按下Publish message 3. Confirm if the subscription message has been received After publishing a message in SNS, all endpoints subscribed to the topic should receive it.\nOpen the SQS page and select the SQS queue that is subscribed to the topic. Click on Send and receive message. 往Scroll down to the \u0026ldquo;Receive messages\u0026rdquo; section and select Poll for Messages. Once successful, you will see a list of messages appearing below, and you can click on them to view detailed content. Reference source: Send Fanout Event Notifications ",
    "ref": "/en/blog/202210-aws101-what-is-sns-and-sqs/"
  },{
    "title": "Serverless 101",
    "date": "",
    "description": "Introducing What is Serverless with AWS Services",
    "body": "Since our company\u0026rsquo;s services are built on AWS and heavily rely on various serverless computing services,\nI had no prior experience in this area. This series serves as a compilation of notes for me,\nand its content may be subject to constant revisions in the future.\nAlthough it\u0026rsquo;s called \u0026ldquo;serverless,\u0026rdquo; servers still exist in reality; they are simply deployed and maintained by the cloud platform.\nFor users, all they need to do is write the code without having to worry about server tuning.\nBackend engineers often have to manage servers on the side, and they know how painful it is.\nHowever, with the Serverless architecture, infrastructure management can be delegated to cloud service providers simply by writing configuration files (and pulling out the magical card from the boss).\nCommon cloud service providers all have their own Serverless services\nAWS：Lambda Microsoft Azure：Functions Google：Cloud Functions The Serverless-related services provided by AWS can be broadly categorized as follows:\nServerless ≠ Lambda Function That is a common misconception.\nIn addition to Lambda Functions, Serverless also encompasses the composition of event sources and other resources.\nThis includes APIs, databases, and event triggers that work together to execute tasks.\nAnd AWS provides AWS Serverless Application Model(AWS SAM） It is an open source architecture for building serverless applications\nPros Basically, the infrastructure is outsourced to a cloud service provider to help you solve\nHigh availability: Increase program traffic flexibility and stability through automatic scaling and load balancing Reduced server idle performance: charge according to usage, use as much as you want instead of keeping the host on all the time Reduced server setup and maintenance labor costs Security: No need to worry about the overall security of the server Cons It primarily stems from the issues arising from not having servers running continuously.\nDebugging can be a bit challenging and requires the use of log processing mechanisms like CloudWatch. Can\u0026rsquo;t handle Schedule job =\u0026gt; Lambda not always running There is a startup time involved when executing Lambdas. Request cannot be guaranteed to run on the same Lambda =\u0026gt; Lambdas are stateless, state synchronization must be handled through other services. Architecture By using Lambda to write different functions, it replaces the traditional monolithic codebase that runs on virtual servers.\nThis can be seen as an implementation of Function as a Service (FaaS) paradigm.\nThe general flow is as follows:\nEvent Source -\u0026gt; Lambda Function -\u0026gt; Service Event Source: An event or trigger occurs, such as an API request, database update, or message in a queue. This event acts as the source of the operation. Lambda Function: The event is passed to a Lambda function, which is a small piece of code responsible for processing the event and performing the necessary actions or computations. The Lambda function is executed in a serverless environment. Service: The Lambda function interacts with other services or resources, such as databases, APIs, or external systems, to complete the required operations. These services provide the necessary functionality and data processing capabilities. For a simple to-do list application, the system architecture diagram using Serverless would look like this\nThe key focus would be on the dashed section labeled ToDo App on the right side of the diagram.\nThis section aligns with the flow of Event Source -\u0026gt; Lambda Function -\u0026gt; Service mentioned above.\n",
    "ref": "/en/blog/202210-what-is-serverless/"
  },{
    "title": "Interacting with smart contracts using Golang",
    "date": "",
    "description": "Generating a Go package with abigen tool to interact with smart contracts and perform contract operations",
    "body": "Previously, I was calling Smart Contracts through PHP, which raised two issues.\nOne was the slowness, and the other was the lack of maintenance of the package.\nFortunately, Golang provides an official package that is both fast and solves these two problems.\nThis article explains how to convert existing Smart Contract ABIs into a Go package for interacting with contracts.\nPreparation To set up the necessary packages on Mac, you need to install the following dependencies through Homebrew beforehand.\nbrew update brew tap ethereum/ethereum brew install ethereum brew install solidity brew install protobuf Otherwise, you may encounter issues in the subsequent steps.\nPlease install solc\nPlease install protoc\nInstall go-ethereum Next, you need to install the conversion tool we will be using.\nThe official package comes with the abigen command, which allows you to convert the ABI into a Go package.\ngit clone https://github.com/ethereum/go-ethereum.git cd go-ethereum make devtools Generate Go library file Once the installation is complete, you will need the smart contract ABI to perform the conversion.\nHowever, the topic of what a smart contract ABI is falls outside the scope of this article.\nIf you require a more in-depth explanation of smart contract operations in the future, it can be covered separately.\nOnce you have obtained the abi.json file for the smart contract, you can proceed with the following command execution:\nabigen --abi=\u0026#34;./erc721.abi.json\u0026#34; --type=\u0026#34;erc721\u0026#34; --pkg=erc721 --out=\u0026#34;erc721.go\u0026#34; flags description usage \u0026ndash;abi file path for smart contract abi.json ./erc721.abi.json \u0026ndash;type type name in struct erc721 \u0026ndash;pkg go package name in output file erc721 \u0026ndash;out output file name erc721.go As a result, you will have a file named erc721.go, and upon opening it, you will find that the package name is erc721.\nBy comparing the functions inside the file, you will notice that they correspond to the functions in the abi.\nNow, you can interact with the contract by importing and referencing the erc721 package.\nUsing the package in Golang. To interact with the generated erc721 package in Go, you need to first import the package in your code.\nSince blockchain operations are essential, you should also import go-ethereum. Here\u0026rsquo;s an example:\nimport ( erc721 \u0026#34;project/package-erc721/path\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/common\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/ethclient\u0026#34; ) Then interacting with smart contracts using go-ethereum.\nThe following example demonstrates the interaction with the TotalSupply function in a smart contract.\nPassing a smart contract\u0026rsquo;s contract address string: hexAddress.\nYou can obtain the total supply issued by the contract, and this example focuses on an ERC721 NFT contract.\ninfuraUrl := \u0026#34;https://mainnet.infura.io/v3/38ad7d4b...97aa1d5c583\u0026#34; client, err := ethclient.Dial(infuraUrl) if err != nil { return nil, err } defer client.Close() address := common.HexToAddress(hexAddress) instance, err := erc721.NewErc721(address, client) if err != nil { return nil, err } totalSupply, err := instance.TotalSupply(nil) if err != nil { return nil, err } return totalSupply, nil This post illustrates how to use abigen to generate a corresponding Go package for a Smart Contract.\nSo far, the functions called, including Owner, OwnerOf, and TotalSupply, have successfully retrieved data from the blockchain.\nThere are other applications to explore in the future as opportunities arise for further operations.\n",
    "ref": "/en/blog/202207-go-ethereum-abigen/"
  },{
    "title": "Create a Redis service on GCP",
    "date": "",
    "description": "Set up a Redis service using Google Cloud MemoryStore.",
    "body": "GCP offers the Google Cloud MemoryStore service for creating Redis or Memcached caching machine services.\nThe pricing is similar to that of setting up a database.\nI thought it would be as inexpensive as using virtual machines (VMs).\nCreating Redis Go to the MemoryStore page and click on either \u0026ldquo;Redis\u0026rdquo; or \u0026ldquo;Memcached\u0026rdquo; to begin the setup process.\nIn this example, for Redis, click on \u0026ldquo;Create Instance\u0026rdquo; to proceed.\nEnter some basic configurations, which mainly involve adjusting the size and name.\nAfter configuring the options as prompted, click on \u0026ldquo;Create Instance\u0026rdquo; and then wait for it to be created.\nAt this point, there will be a short waiting period, so you can take a break in the meantime.\nThis is the method to create Redis on GCP.\nOnce created, you only need to map the endpoint in your application to the IP of the Redis instance to start using it.\nAlternatively, you can also use third-party GUI tools like Another Redis Desktop Manager to access via IP address.\n",
    "ref": "/en/blog/202206-gcp-redis/"
  },{
    "title": "Make your computer be temporarily accessible on the internet. - Ngrok",
    "date": "",
    "description": "Use Ngrok to temporarily obtain a publicly accessible URL for your computer.",
    "body": "In development, it is often encountered that testing on the local machine requires a publicly accessible URL for services.\nOr webhooks to use as a callback. Moreover, these services often require HTTPS certification.\nAre there any other options besides setting up a cloud server for this purpose?\nNgrok As a forwarding server, it can redirect external requests to a specified port on your machine.\nThe principle is to connect to the Ngrok cloud server, which exposes the address you specify on your local machine.\nNgrok then provides a public URL for accessing the content.\nThe advantages are that it is fast, provides HTTPS services for enhanced security, and you can even set up password protection.\nOfficial documentation and download links Install Mac brew install ngrok Linux Determine the hardware architecture of your host machine.\ncat /proc/cpuinfo Download the specified file from the official website and follow the steps to install.\nOr just install using snap\nsudo apt update sudo apt install snapd sudo snap install ngrok Start service Enter the command to start and listen on port 8080.\nngrok http 8080 You will then be able to see the publicly accessible URL.\n註冊 ngrok You can use the service without registering, but after a period of time, the connection will be terminated.\nAnd upon restarting, a new URL will be assigned.\nHowever, when testing webhooks or providing the URL to others, having to reassign the URL means reconfiguring webhooks or notifying others, which can be inconvenient.\nAfter logging into your Ngrok account, go to the Your Authtoken page.\nCopy the Authtoken and then enter it in the terminal using the following command:\nngrok config add-authtoken {Your Authtoken} Seeing the following message indicates that the authentication process is complete.\nAuthtoken saved to configuration file: /Users/user_name/.ngrok2/ngrok.yml After completing the registration, you can use the Ngrok service without worrying about the connection being terminated after a while.\n",
    "ref": "/en/blog/202204-ngrok/"
  },{
    "title": "Scalable Server 02 Load Balancing",
    "date": "",
    "description": "Configuring Load Balancing on GCP for Enhanced Server Flexibility",
    "body": "In the previous post Scalable Server 01 Auto Scaling , it was mentioned that we need to configure a set of Load Balancers to distribute internet requests to different server hosts.\nThis allows us to dynamically adjust the number of hosts based on traffic and maintain stable service operations.\nThe previously created instance group consists of individual hosts with unique IPs.\nHowever, using load balancing allows us to conceal the individual instance IPs and redirect traffic to new machines as traffic increases.\nThis ensures a certain level of security and system stability, preventing the original hosts from crashing due to excessive load.\nCreate a Load Balancing First, locate Network Services in the sidebar menu and select Load Balancing.\nAfter pressing Create Load Balancer on the top toolbar, select HTTP(S) Load Balancer.\nSince the traffic is coming from the internet, select From the internet to VM or serverless service.\nBackend Service configuration Next, select Create a backend service\nChoose port number 80 for internal communication, as our internal servers do not have SSL certificates attached.\nFrontend Service configuration Next, go to Frontend Configuration.\nIf we want to allow access via a fixed set of IP addresses, we\u0026rsquo;ll need to create a set of IP addresses here.\nYou can either pre-create them or reserve a set of fixed IP addresses through on-screen prompts.\nIf you have an SSL certificate, you can also configure it here.\nAfter saving, wait for it to be created.\nThen, click into the details of the newly created load balancer.\nIn the Frontend section, you\u0026rsquo;ll find an IP address.\nPaste this into your browser to access the service you just configured!\nSo, our load balancer is now successfully set up!\nWith the automatic scaling feature, when traffic increases and triggers the scaling rules, new virtual machines will be launched.\nThe load balancer will distribute requests, helping us handle the traffic and maintain stable service operations.\nReference:\n[GCP 教學] 打造彈性、快速且安全的雲端基本服務架構 – 負載平衡 Load Balancer 和 Instance Group GCP VM 雲端主機最基本防護 - Load Balance ",
    "ref": "/en/blog/202204-gcp-load-balancing/"
  },{
    "title": "Introducing Laravel Sail and Basic Operations",
    "date": "",
    "description": "Using Laravel Sail to launch the development environment with ease and joy.",
    "body": "In the context of Laravel development environment setup, both official and unofficial sources offer a plethora of approaches.\nThis time, we will be introducing a package introduced after Laravel 8: Laravel Sail .\nIn the past, to save beginners time in the initial setup of environments, there have been numerous tools for development environments.\nFor example, Laravel Homestead , built using virtual machines, was one option.\nThere were also Docker-based development environments like laradock .\nOr creating a Laravel development environment using Docker , as I personally do.\nIn Laravel 8 and onwards, Laravel Sail has been integrated as a built-in package.\nCompared to Laravel Homestead, it requires fewer resources as it utilizes Docker.\nIn contrast to laradock, which also operates within a Docker environment, Laravel Sail offers simpler configuration, and in some cases, even requires no configuration, achieving a plug-and-play experience.\nYou need to install Docker before start using Laravel Sail\nInstall Sail In Laravel 9, Sail is now included as a built-in feature, allowing you to directly launch services through commands.\nphp artisan sail:install For older versions of Laravel that do not have Sail, you can install it using Composer.\ncomposer require laravel/sail --dev Upon executing the command, an interactive interface will prompt you to select the desired services.\nWhich services would you like to install? [mysql]: [0] mysql [1] pgsql [2] mariadb [3] redis [4] memcached [5] meilisearch [6] minio [7] mailhog [8] selenium \u0026gt; 0,3,7 Sail scaffolding installed successfully. After making your selections, a docker-compose.yml file will be generated in the project\u0026rsquo;s root directory.\nYou can then use a command to launch the Sail service.\n./vendor/bin/sail up 設定指令別名 If you\u0026rsquo;re feeling a bit lazy and don\u0026rsquo;t want to type out the entire command, you can set up an alias for it.\nOpen either vim ~/.bashrc or vim ~/.zshrc (depending on your terminal).\nAdd the following line to the file:\nalias sail=\u0026#34;./vendor/bin/sail\u0026#34; After that, you can directly execute Sail commands within the project using the sail alias.\nThe following operations are performed assuming you\u0026rsquo;ve added the alias.\nSail command The usage is similar to Docker commands:\nsail up -d：Start and run in the background sail stop：Stop sail down：Stop and remove containers sail build --no-cache：Rebuild containers, ignoring all caches for a complete rebuild. Adjust docker image The default image includes the basic Laravel environment.\nIf you need to make adjustments, such as installing additional PHP extensions, you will need to export the relevant configurations.\nphp artisan sail:publish After executing the command, a docker folder will be added to the project, containing container settings and Dockerfiles.\nExecute command Sail provides a convenient way to call various commands by adding sail in front of them.\nEssentially, you prepend sail to the desired commands.\nExecute PHP command sail php --version Execute Composer command sail composer install # composer install Execute Artisan command sail artisan queue:work # php artisan queue:work sail artisan schedule:work # php artisan schedule:work Execute shell command sail shell myShell.sh # sh myShell.sh By following these simple steps, you can easily set up a local development environment and execute commands as well.\nLaravel Sail significantly reduces many of the development hassles!\n",
    "ref": "/en/blog/202204-laravel-sail/"
  },{
    "title": "Jenkins CI/CD 02 Basic Pipeline Setup and GitHub Integration",
    "date": "",
    "description": "Creating a basic pipeline workflow and triggering it through GitHub webhooks",
    "body": "In Jenkins , the build process is known as a pipeline.\nIt can also be triggered through webhooks, offering various triggering and execution methods.\nBelow, we\u0026rsquo;ll demonstrate how to integrate GitHub webhooks and provide common trigger condition examples.\nCreating the First Pipeline Navigate to Open Blue Ocean in the sidebar to create a new pipeline.\nFollow the prompts to create an access token. In this demonstration, I had to generate a new token because I deleted the old one, resulting in a Token deleted error message.\nClick Create an access token here to open the personal access token creation page on GitHub.\nYou can also find it by going to\nGitHub \u0026gt; Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new token\nThe necessary permissions are typically pre-filled; simply click Generate token.\nCopy the generated token and paste it into Jenkins.\nIf you forget to do so at this point, you\u0026rsquo;ll need to regenerate the token.\nOnce verification is complete, you can select a project.\nClick \u0026ldquo;Create,\u0026rdquo; and Jenkins will scan all branches within the GitHub repository.\nConnecting Webhooks to Listen for Events 進入專案的 Settings \u0026gt; Webhook \u0026gt; Add webhook\n在Payload URL填入jenkins主機網址/github-webhook/\n如果 port 不是使用 80 或 443，則要完整輸入。例如 Jenkins 預設使用的是8080port\nNow, let\u0026rsquo;s make some changes to the project and commit and push them to GitHub.\nYou might notice that Jenkins isn\u0026rsquo;t responding.\nThis is because we haven\u0026rsquo;t configured Jenkins to listen to GitHub events.\nHow can we do that? Through webhooks!\nWebhooks transmit events to a specified URL when events configured in the webhook settings are triggered.\nTo allow Jenkins to receive GitHub events, you must bind the webhook to your project.\nThis is typically automated in Drone when you build a project, but not in Jenkins.\nGo to your project\u0026rsquo;s Settings \u0026gt; Webhook \u0026gt; Add webhook.\nIn the Payload URL field, enter your Jenkins server\u0026rsquo;s URL followed by /github-webhook/.\nIf your port isn\u0026rsquo;t 80 or 443, you should include the complete URL.\nFor example, if Jenkins is using the default port 8080:\nClick \u0026ldquo;Add webhook\u0026rdquo; to create it and then test the connection.\nIf the test fails, you can adjust the settings and click the three dots on the right to Redeliver for a retest.\nFrom now on, every time a push event occurs(based on the webhook settings), Jenkins will be notified.\nThis way, we achieve the goal of triggering a Jenkins build process when new content is pushed to GitHub.\nListening for Tag Events In version releases, we often tag the current version.\nIn my previous experience with Drone, when a tag was pushed to the repository, the corresponding event was sent via webhook. However, in Jenkins, it\u0026rsquo;s not that simple.\nI struggled with this for a while until I found the solution in the official documentation: When using tags in Jenkins Pipeline .\nBy default, Jenkins doesn\u0026rsquo;t set up tag listening.\nYou need to access your pipeline settings, navigate to Branch Sources, and in the Behaviours section at the bottom, check Discover tags to enable tag listening.\nI\u0026rsquo;m still not entirely sure why this design choice was made.\nBasic Pipeline Example Here, I\u0026rsquo;ll provide a basic demonstration of several commonly used trigger conditions.\nDetails on what actions to take after triggering will be discussed later.\nStart by creating a file named jenkinsfile in the project\u0026rsquo;s root directory.\nJenkins uses this file as the default pipeline configuration.\n// jenkinsfile pipeline { agent any stages { stage(\u0026#39;Example Build\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } stage(\u0026#39;Example Deploy\u0026#39;) { when { branch \u0026#39;production\u0026#39; } steps { echo \u0026#39;Deploying\u0026#39; } } stage(\u0026#39;Example Tag Deploy\u0026#39;) { when { buildingTag() } steps { echo \u0026#34;Building $TAG_NAME\u0026#34; } } stage(\u0026#39;Example Tag Deploy\u0026#39;) { when { tag \u0026#34;release-*\u0026#34; } steps { echo \u0026#34;Building $TAG_NAME\u0026#34; } } } } Here, we demonstrate four trigger conditions:\nNo when statement: This stage triggers regardless of the conditions. when branch: This stage triggers only when a specified branch is pushed. when buildingTag(): This stage triggers when any tag is created. when tag \u0026quot;release-*: This stage triggers only when tags conforming to the format \u0026ldquo;release-*\u0026rdquo; are created. These four conditions provide a foundation for various pipeline workflows.\nFor more usage options, consult the official documentation: Pipeline Syntax .\nAfter reviewing the documentation, you might find that Drone is preferable.\nAs Jenkins offers a multitude of options, making it easy to get lost.\nDifferent developer have different preferences for using the UI or command line.\nHowever, the fundamental concepts remain the same, and experience in one can be applied to the other.\nThis concludes the basic introduction to Jenkins pipelines.\n",
    "ref": "/en/blog/202203-jenkins-cicd-2-basic-pipeline/"
  },{
    "title": "Scalable Server 01 Auto Scaling",
    "date": "",
    "description": "Configuring Auto Scaling on GCP for Enhanced Server Flexibility",
    "body": "Create image First, you need to stop the specified VM.\nNext, in the sidebar, navigate to Storage -\u0026gt; Images, then click on it to select Create Image.\nMake sure not to choose Virtual Machines -\u0026gt; achine Image as it will lead to confusion later on!\nIn the beginning, I mistakenly pressed Create Machine Image from the VM section and got stuck in this alternate world, which wasted my whole afternoon.\nThe simplest way to determine this is by not stopping the VM in the first step.\nWhen creating an image, if it doesn\u0026rsquo;t prompt you to stop, then you\u0026rsquo;re doing it wrong.\nFollowing the instructions and successfully creating the image, you can proceed to the next step.\nCreate Instance templates Go to Instance Templates, then click on Create Instance Template.\nChoose the hardware specifications or hardware rules This section is where you determine the specifications required for each new instance when launched within the instance group. The setup here resembles the configuration during VM creation. Additionally, you can configure the firewall settings to open ports 80 and 443 if your network service requires them.\nSelect the VM image as the boot template The difference here is that we are using the recently created image as the template.\nSo, under the boot disk section, click on Change -\u0026gt; Custom image and select the image you\u0026rsquo;ve just created.\nExecute commands automatically at startup Sometimes, when you start a new VM, you need to run certain commands to start services like Docker.\nWithout these commands at startup, your VM will only be powered on but won\u0026rsquo;t have services like the Docker daemon running.\nClick on the Management section to expand its contents, and you\u0026rsquo;ll find instructions for automation.\nYou can specify boot scripts that will run when the instance starts up or restarts. Boot scripts are useful for installing software, updating items, and ensuring that services run smoothly within the virtual machine.\nYou can include commands in this section to ensure that when the VM starts, the necessary services are initiated.\n#! /bin/bash sudo systemctl start docker sudo docker start laravel-app nginx redis sudo docker exec -i laravel-app /var/www/html/artisan migrate sudo docker exec -i laravel-app /var/www/html/artisan l5-swagger:generate --all sudo docker exec -i laravel-app /usr/bin/supervisord -c ./builds/docker/beta/php81/horizon.conf sudo docker exec -i laravel-app /var/www/html/artisan queue:work \u0026amp; Official documentation: Using startup scripts on Linux VMs Instance Group When the triggering conditions are met, the instance group will automatically configure machines based on the previously set instance template.\nTo begin, open the Instance Groups and click on Create Instance Group.\nOn the interface, you will see three options on the left.\nThe interface might have changed since the existing online articles were published.\nNew managed instance group (stateless) New managed instance group (stateful) New unmanaged instance group Among these options, only the two under Managed Instance Group have Auto Scaling functionality.\nAdditionally, the Unmanaged Instance Group allows you to add existing VM instances without the need to create an instance template first.\nReference documentation: Using managed instance groups Select the instance template you want to use and specify the Auto Scaling rules. Once done, click on the Create button at the bottom to complete the creation of the instance group and the Auto Scaling configuration.\nConfigure instance group health checks and automatic healing To ensure that newly started instances are available, automatically shut down any non-functional instances, and replace them with new ones, open the instance group\u0026rsquo;s editing page, and scroll down to the Autohealing section.\nSelect Create Health Check and configure it by providing a name and other settings. For example, if you choose the HTTP protocol, you can send requests to specific routes, while the default TCP option simply checks for normal transmission.\nAfter configuring the health check, let it run, and then go back to the instance group\u0026rsquo;s detailed page to check the health report. If everything is fine, it should indicate that everything is healthy without any issues.\nRolling Update When a new image is created, you\u0026rsquo;ll need to create a new instance template first.\nThen, you need to go back to the instance group and edit it to use the new instance template.\nThis ensures that new instances launched in the future will use the updated template.\nModifying the template won\u0026rsquo;t automatically restart existing instances.\nIf you want to achieve seamless updates for online services, you might consider implementing a rolling update.\nTo perform a rolling update, go to the Instance Groups, select the newly created instance group, and look for a button like UPDATE VMS. The button name may vary, so please check for a similar option.\nNext, select the new instance template and in the Update Settings, choose Automatic for the Update type.\nThis ensures that not only new VMs but also existing ones are replaced with the new template.\nYou will see that it starts a new instance using the new instance template.\nOnce the new instance template has been successfully started, it will proceed to delete the old instances.\nThis ensures a seamless upgrade without impacting your online services.\nAt this point, the service is not fully functional. The instance group is still a collection of individual, independent hosts. While it has the ability to automatically scale the number of hosts, it doesn\u0026rsquo;t inherently handle request routing or load balancing.\nIn accordance with the diagram below. Currently, we have only completed the Instance Group and the subsequent steps.\nWe also need to use the Load Balancing service to provide a fixed IP address responsible for receiving all requests.\nThrough Load Balancing, we can distribute traffic to individual hosts within the instance group.\nWe will talk about this in the further post.\n",
    "ref": "/en/blog/202203-gcp-auto-scaling/"
  },{
    "title": "Jenkins CI/CD 01 Installation",
    "date": "",
    "description": "Setup Jenkins CI/CD step by step",
    "body": "Create Your Own Automated Deployment Service with Jenkins Jenkins is a Java-based CI/CD system tool that we are exploring in this tutorial,\nfollowing our previous introduction to Drone If you plan to use Docker for installation and require Docker commands within Jenkins, you\u0026rsquo;ll need to address Docker-in-Docker issues.\nFor those less familiar with Docker and operating systems, it is recommended to follow the method described below, installing Jenkins directly on your host machine.\nBelow, we\u0026rsquo;ll go through the step-by-step installation process as per the official installation documentation First, ensure that you have Java installed:\njava -version sudo apt install openjdk-11-jre Next, install the Jenkins service on your host machine, opting for the stable version (LTS - Long Term Support):\ncurl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc \u0026gt; /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install jenkins Start the Jenkins service using the following command, and then access it via your web browser at localhost:8080.\nYou\u0026rsquo;ll be prompted to enter the initial admin password:\nsudo systemctl enable --now jenkins Retrieve the initial admin password using the following command:\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword Plugin Installation By default, Jenkins does not support pipelines, so you\u0026rsquo;ll need to install a pipeline plugin.\nIn this tutorial, we\u0026rsquo;re using Blue Ocean .\nTo use Docker as an agent in your build process, you\u0026rsquo;ll also need to install two additional extensions:\nDocker plugin Docker Pipeline You can search for and install these plugins in the Jenkins plugin management section:\nAdditionally, ensure that Docker is properly installed on your Jenkins host machine.\nDocker Command Permissions To configure Docker command permissions, first switch to the Jenkins user:\nsudo su jenkins Then, execute any Docker command, e.g., docker ps.\nIf you encounter a permission denied message like:\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post\nAvoid running sudo chmod 777 /var/run/docker.sock.\nThe correct approach is to add the Jenkins user to the Docker user group:\ncat /etc/passwd # List all users sudo groupadd docker # Create a Docker user group (usually created when Docker is installed) sudo usermod -aG docker jenkins # Add the Jenkins user to the Docker group Restart the Jenkins service. You can trigger the restart by entering the following URL in your web browser:\nhttp://jenlins_url/restart\nClick the restart button, and once it\u0026rsquo;s back up, it will load the new configuration.\nNow you can run Docker commands as the Jenkins user.\n",
    "ref": "/en/blog/202203-jenkins-cicd-1-installation/"
  },{
    "title": "Laravel Queue Usage",
    "date": "",
    "description": "Using queues to defer processing of tasks that don't need immediate attention.",
    "body": "The purpose of queues is to defer processing of a time-consuming task that doesn\u0026rsquo;t require immediate completion. Like sending emails, users don\u0026rsquo;t need to wait for the email to be sent successfully before the next step.\nAllowing the application to respond more quickly to web requests and smooth user experience.\nPreparation Laravel supports multiple queue drivers\nDatabase Redis Amazon SQS If testing locally, you can configure it in the .env file as follows:\nQUEUE_CONNECTION=sync This will execute immediately after sending the task, making it more convenient for testing Queue-related code.\nIn the following example, we will demonstrate using database as the driver.\nQUEUE_CONNECTION=database Create Queue table Because Queue is a feature provided by Laravel.\nYou can directly create a database table named Jobs to record pending Queue information using a command.\nphp artisan queue:table php artisan migrate Create Job files You can create it manually or by using a command.\nphp artisan make:job SyncBlockchainNftItemJob At this point, a file will be generated in the specified path: app/Jobs/SyncBlockchainNftItemJob.php\nWrite the Job program logic. Just modify the SyncBlockchainNftItemJob.php we created earlier.\nAnd the main functionality should be written inside the handle() method, like this\npublic function handle() { if ($this-\u0026gt;userId) { Artisan::call(\u0026#34;nft-items:sync $this-\u0026gt;userId\u0026#34;); } } Call an Artisan command to execute something.\nThe required parameters can be initialized in the __construct at the beginning of the Job program file and can be used as input parameters when creating Queue tasks in the future.\nprotected $userId; public function __construct($userId) { $this-\u0026gt;userId = $userId; } Call the Job to create a task. Now that the Job is ready, how do you call it from the Controller?\nAfter including SyncBlockchainNftItemJob, you can use dispatch wherever you want to create a task and assign it to the specified Job.\n$this-\u0026gt;dispatch(new SyncBlockchainNftItemJob($user-\u0026gt;id)); // Alternatively, you can keep it even simpler. SyncBlockchainNftItemJob::dispatch($user-\u0026gt;id); Starting Queue Worker If sync was not used as the driver earlier, the Queue won\u0026rsquo;t execute!\nYou need to use a command to instruct the Queue to start working!\nphp artisan queue:work It\u0026rsquo;s important to note that once Queue Workers are started, they won\u0026rsquo;t automatically update when there are code changes.\nDuring the deployment phase, remember to use a command to restart the Queue worker. Otherwise, it will continue running the old version of the code!\nphp artisan queue:restart Check status ps -ef|grep queue:work Check execution status Executing the Controller\u0026rsquo;s code, you\u0026rsquo;ll notice that when the Queue is triggered, a new record is added to the jobs table.\nAnd during execution, the terminal will display corresponding information.\nIf you see Processed, it means the task has been completed, and at this point, the record in jobs will be removed.\nSupervisor When the Queue is running, various situations can lead to critical errors, preventing the execution of tasks.\nIn such cases, it\u0026rsquo;s recommended by the official documentation to use Supervisor for management.\nIn the event that the Queue unexpectedly stops operating, Supervisor will restart the Queue service based on the configuration file, ensuring that Jobs can run continuously!\nInstall from docker image FROM php:7.4-fpm RUN apt-get install supervisor CMD /var/www/html/_scripts/cron.sh In the last line of our CMD, we executed a cron.sh as the entry point, which will be used later.\nSupervisor Config Place the file wherever you like.\n# supervisord.conf [program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /var/www/html/artisan queue:work --sleep=90 --tries=3 autostart=true autorestart=true startsecs=5 user=root numprocs=3 redirect_stderr=true stdout_logfile=/var/www/html/storage/logs/supervisord.log stopwaitsecs=3600 [supervisord] logfile=/var/log/supervisor/supervisord_main.log pidfile=/var/www/html/storage/logs/supervisord.pid [supervisorctl] Start Supervisor Create the _scripts/cron.sh file in the project and mount it to the container\u0026rsquo;s /var/www/html path.\n#!/usr/bin/env bash cd /var/www/html supervisord -c ./docker/cron/supervisord.conf # Start supervisord using the configuration file located at the specified path php /var/www/html/artisan schedule:work # Simultaneously initiate the cron job. When the Dockerfile executes a shell script via CMD, it can be considered as\nsh _scripts/cron.sh\nIn cron.sh, we\u0026rsquo;ve done three things:\nNavigate to the container\u0026rsquo;s /var/www/html path. Start the supervisord service using the specified config. Execute php artisan schedule:work. This way, both supervisord and the cron job service are initiated.\nCheck Supervisor status You can use a command to confirm whether supervisord is running.\npgrep -fl supervisord # or ps aux|grep supervisord If an error message appears indicating that the ps or pgrep commands do not exist, you will need to install the package using a command.\napt-get update \u0026amp;\u0026amp; apt-get install procps If you see supervisord listed, it means it\u0026rsquo;s already running.\n",
    "ref": "/en/blog/202203-laravel-queue/"
  },{
    "title": "Successfully launched a VM on GCP on the first try",
    "date": "",
    "description": "Step-by-step guide to launching your first VM instance and configuring it on Google Cloud Platform (GCP).",
    "body": "How to launch a VM virtual machine on Google Cloud Platform (GCP)?\nIt may sound complex, but in practice, it\u0026rsquo;s quite straightforward.\nIf you haven\u0026rsquo;t had any experience with setting up a host before, this time, we\u0026rsquo;ll document the steps for you.\nCreate a new VM instance There\u0026rsquo;s not much to be concerned about. Once you\u0026rsquo;re in the GCP console and on the VM instances page, simply choose the desired specifications, give it a name, and click \u0026lsquo;Create\u0026rsquo;.\nAssigning a static external IP address To ensure that our service has a consistent IP address, we can change the external IP type from ephemeral to static.\nPlease note that converting an ephemeral IP to a static IP may incur additional charges.\nFirst, select the VM instance you want to modify.\nClick on the ellipsis menu (the three dots), and choose View network details.\nNext, from the left-hand menu, select the \u0026lsquo;External IP addresses\u0026rsquo; tab.\nIn the Users column, locate the VM instance you just created.\nYou\u0026rsquo;ll notice that the Name field for the target is currently empty.\nClick on Reserve on the left-hand side, and at this point, it will prompt you to enter a name.\nOnce you\u0026rsquo;ve provided a name, your VM instance\u0026rsquo;s IP address will remain unchanged even after service restarts.\nOpening external ports Your virtual machine is up and running, and you\u0026rsquo;ve also secured a static external IP.\nNow, it\u0026rsquo;s time to establish a connection!\nYou enter the IP address in your browser and\u0026hellip; can\u0026rsquo;t access it! Huh?\nCommonly used network service ports By default, when you create a VM, only port 22 for SSH connections is open.\nIf you set up firewall rules during the creation, it will apply the http-server and https-server network tags, allowing your VM to receive requests on ports 80 and 443.\nYou can check the \u0026lsquo;Network tags\u0026rsquo; section in the VM\u0026rsquo;s information page.\nIf you find that these ports are not open, and you wish to enable them, click on the \u0026lsquo;Edit\u0026rsquo; button for the VM, and navigate to the Firewall settings.\nSelect the http-server and https-server rules that were not checked during initial setup.\nAfter successfully applying these changes, your VM will be able to receive incoming requests on ports 80 and 443.\nOther port If your service requires the use of a different port, you will need to configure it manually.\nLet\u0026rsquo;s use port 8080 as an example for configuration.\nCreate a firewall rule Select the VM instance you want to modify, and from the rightmost three-dot menu, choose \u0026lsquo;View network details\u0026rsquo; to access the \u0026lsquo;Virtual Private Cloud network.\nThen, on the left-hand menu, select \u0026lsquo;Firewall\u0026rsquo; and click on \u0026lsquo;Create firewall rule\u0026rsquo;.\nFor the most part, you can use the default values. Determine if you need to customize based on the provided explanations.\nThere are only a few areas that require adjustments.\nTarget tags These are network tags used when selecting firewall rules later on.\nThey serve the same purpose as the http-server tag when configuring port 80, acting as identifiers.\nSource IPv4 range This is where you specify trusted IP locations.\nIf you want to make it publicly accessible, set it to 0.0.0.0/0 to accept requests from all sources.\nProtocol and Ports To demonstrate opening multiple ports simultaneously, we\u0026rsquo;ve added port 8089.\nThis allows you to specify the external availability of both TCP ports 8080 and 8089.\nYou can observe that ports are separated by a comma ,.\nIf you use a dash -, you can specify a range of ports to be opened.\nApplying the Firewall Rule Go back to Compute Engine and select the VM instance where you want to apply the rule. Click the \u0026lsquo;Edit\u0026rsquo; button.\nManually enter the Target tags you filled out when creating the firewall rule in the Network tags field.\nAfter saving, the rule will be applied.\n",
    "ref": "/en/blog/202201-gcp-vm-basic/"
  },{
    "title": "About",
    "date": "",
    "description": "About Byte Ebi",
    "body": "Familiar with PHP including CodeIgniter, WordPress, Laravel.\nKnow a little about frontend and have experience with container.\nNow I am using Golang most of the time. And studying for AWS services.\nMake some small tool in my free time.\nFeel free to contact me via LinkedIn. I wanna have some foreign friends.\nLove Japanese culture after I star watching Vtuber.\nIf you want to talk about that, please do not hesitate to contact me.\n",
    "ref": "/en/about/"
  }]
