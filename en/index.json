[{
    "title": "Different between pointer and value for struct type in golang",
    "date": "",
    "description": "The different between pointer and value for struct type in golang with examples",
    "body": "I always had a question \u0026ldquo;Should I use value for type in struct? Or using pointer?\u0026rdquo;\nAfter reading many different projects on the web, it seems that they all have different practices.\nUntil I recently encountered a problem that gave me some ideas.\nIn our projects, we often start by defining the Request incoming Struct\nThen use json.Unmarshal to convert the incoming json string into Struct for subsequent operations\nPreviously, the default value for fields was false, so we pass by value.\nIn this case, the default value is true and the problem happened.\nTake the first look at the code and execution results\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type John struct { Name string `json:\u0026#34;name\u0026#34;` CanRun bool `json:\u0026#34;canRun\u0026#34;` CanFly bool `json:\u0026#34;canFly\u0026#34;` } type Doe struct { Name string `json:\u0026#34;name\u0026#34;` CanRun *bool `json:\u0026#34;canRun\u0026#34;` CanFly *bool `json:\u0026#34;canFly\u0026#34;` } func main() { var param = []byte(`{\u0026#34;Name\u0026#34;:\u0026#34;John Doe\u0026#34;, \u0026#34;canRun\u0026#34;:true}`) defult := true var john = new(John) json.Unmarshal(param, \u0026amp;john) jsondata, _ := json.Marshal(john) fmt.Println(string(jsondata)) var doe = new(Doe) json.Unmarshal(param, \u0026amp;doe) doejsondata, _ := json.Marshal(doe) fmt.Println(string(doejsondata)) if doe.CanFly == nil { doe.CanFly = \u0026amp;defult } doejsondata, _ = json.Marshal(doe) fmt.Println(string(doejsondata)) } In the example, there are two Structs.\nJohn, whose Type is a Bollinger pass by value, and Doe, whose Type is a Bollinger pass by pointer.\nIf the CanFly field is \u0026ldquo;optional\u0026rdquo; and the default value is \u0026ldquo;true\u0026rdquo;\nThe param in the example code play as a request without CanFly parameter\nAssign the variables of type \u0026ldquo;John\u0026rdquo; and \u0026ldquo;Doe\u0026rdquo; Use json.Unmarshal to convert the incoming json string to Struct Convert to json strings via json.Marshal for easy reading and printing on the screen The first line of the output is \u0026ldquo;John\u0026rdquo;, which using the boolean value as type.\nWhen calling new(John) will automatically bring in the zero value .\nAnd the default zero value of boolean is false.\nAnd Name and CanRnu are overwritten with zero value because they are passed in.\nBut because CanFly is optional and not passed in by the user, it is still false\nWe can\u0026rsquo;t identify the false is from user or from zero value.\nIf you use \u0026ldquo;pointer\u0026rdquo; as the type in Struct, you can see that the second line of CanFly is null.\nSo you can set the default value to \u0026ldquo;true\u0026rdquo; later on.\nThis is what I learned about the default value problem when converting json strings to Struct, and how to solve it with a pointer.\n",
    "ref": "/en/blog/202306-different-between-pointer-and-value-for-struct-type-in-golang/"
  },{
    "title": "Using FFmpeg to convert media files",
    "date": "",
    "description": "Convert .ts video files to .mp4",
    "body": "Sometimes, we can use certain methods to download the .M3U8 file and then merge it into a .ts file.\nHowever, computers usually cannot directly play this format.\nWe can use FFmpeg to convert it into a common .mp4 file for easy playback.\nInstallation First, we need to download FFmpeg. If you are a Mac user, you can install it using Homebrew .\nbrew install ffmpeg To check if the installation is successful, you can use the following command:\nffmpeg -version Convert media files Once you have installed it, the next step is file conversion.\nUse the following command and replace the source .ts file and the destination path with your own target path:\nffmpeg -i /source_path/vedio.ts -acodec copy -vcodec copy -f mp4 /target_path/new_vedio.mp4 After executing the command, you will be able to see the converted file in the target path!\nWith FFmpeg, you no longer need to install additional software or rely on online file conversion services.\n",
    "ref": "/en/blog/202301-ffmpeg-video-convert/"
  },{
    "title": "[AWS 101] What are SNS and SQS",
    "date": "",
    "description": "Briefly note what AWS SNS and SQS services do and their use cases",
    "body": "Amazon Simple Queue Service (SQS) and Amazon Simple Notification Service (SNS) may look similar.\nBut in reality, they are quite different.\nTo better clarify the differences between the two, let\u0026rsquo;s compare the notes on both services side by side.\nAWS SNS Full name is: Amazon Simple Notification Service\nThe publisher system can use Topics to distribute event messages to a large number of subscriber systems for parallel processing.\nThe subscribers include:\nAmazon SQS AWS Lambda function HTTPS endpoint Amazon Kinesis Data Firehose The architecture looks like this:\nFeatures: Send real-time messages to all subscribers. Do not retain sent messages. Follow the Publish–subscribe pattern. AWS SQS Full name is: Amazon Simple Queue Service (SQS)\nIt is a fully managed message queue service.\nMessages are sent to a queue and processed by the recipient who retrieves them.\nIt is a Serverless solution, but the executor can be either Lambda function or non-Serverless projects like Laravel.\nType Two types of queues are provided to cater to different application requirements:\nStandard Queue The application can handle messages that arrive more than once and are not ordered sequentially.\nUnlimited Throughput: Standard queues support nearly unlimited transactions per second (TPS) for each API action. At-Least-Once Delivery: A message is guaranteed to be delivered at least once, but occasionally, multiple copies of the message may be delivered. Best-Effort Ordering: Messages are occasionally not delivered in the exact order they were sent. FIFO Queue When the order of operations and events is crucial or duplicates are not acceptable:\nHigh Throughput: By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). Exactly-Once Processing: Messages are delivered exactly once and remain available until the consumer processes and deletes them. First-In-First-Out Delivery: Messages are strictly delivered and received in the order they were sent (First In, First Out - FIFO). Poll Short polling This method is similar to someone repeatedly asking you if there is any job available, even if there is no job.\nYou will get immediate results after each inquiry, even if the queue is empty.\nHowever, to ensure you receive the latest status, you need to keep querying continuously.\nLong polling After making a request, you will not receive an immediate response unless there is a timeout or a message in the queue.\nThis approach minimizes additional polling to reduce costs while ensuring the fastest possible reception of new messages.\nWhen the queue is empty, a long-polling request for the next message can wait for a maximum of 20 seconds.\nFeatures: The receiver actively polls messages. A queue can only be associated with a single consumer. The message is deleted only after the consumer responds with the completion of the processing. Difference between SNS and SQS SNS When new content is updated, event notifications are sent to all subscribers.\nIt follows a push-based architecture where messages are automatically pushed to the subscribers.\nIn simple terms, it is a broadcast notification. SQS Separating queue tasks from the codebase involves a Pull-Based architecture\nConsumers are responsible for pulling messages from the queue and processing them on their own.\nIn simple terms, it is a Queue Here\u0026rsquo;s an example of how these features and architectures can be combined: 1. Methods for creating an SNS Topic and SQS: Access the SNS service and create an SNS topic. Access the SQS service and create an SQS queue. Subscribe the newly created SQS queue to the SNS topic just created to retrieve podcast content. In the SQS Dashboard, select the target SQS queue. Click on the Action dropdown menu and choose Subscribe to Amazon SNS topic. Select the topic and click \u0026ldquo;Save.\u0026rdquo; 2. Simulate sending a message from SNS 到 SNS Topic 頁面並選擇 Topic 後，點選右上角Publish message 輸入測試用的內容，並按下Publish message 3. Confirm if the subscription message has been received After publishing a message in SNS, all endpoints subscribed to the topic should receive it.\nOpen the SQS page and select the SQS queue that is subscribed to the topic. Click on Send and receive message. 往Scroll down to the \u0026ldquo;Receive messages\u0026rdquo; section and select Poll for Messages. Once successful, you will see a list of messages appearing below, and you can click on them to view detailed content. Reference source: Send Fanout Event Notifications ",
    "ref": "/en/blog/202210-aws101-what-is-sns-and-sqs/"
  },{
    "title": "Serverless 101",
    "date": "",
    "description": "Introducing What is Serverless with AWS Services",
    "body": "Since our company\u0026rsquo;s services are built on AWS and heavily rely on various serverless computing services,\nI had no prior experience in this area. This series serves as a compilation of notes for me,\nand its content may be subject to constant revisions in the future.\nAlthough it\u0026rsquo;s called \u0026ldquo;serverless,\u0026rdquo; servers still exist in reality; they are simply deployed and maintained by the cloud platform.\nFor users, all they need to do is write the code without having to worry about server tuning.\nBackend engineers often have to manage servers on the side, and they know how painful it is.\nHowever, with the Serverless architecture, infrastructure management can be delegated to cloud service providers simply by writing configuration files (and pulling out the magical card from the boss).\nCommon cloud service providers all have their own Serverless services\nAWS：Lambda Microsoft Azure：Functions Google：Cloud Functions The Serverless-related services provided by AWS can be broadly categorized as follows:\nServerless ≠ Lambda Function That is a common misconception.\nIn addition to Lambda Functions, Serverless also encompasses the composition of event sources and other resources.\nThis includes APIs, databases, and event triggers that work together to execute tasks.\nAnd AWS provides AWS Serverless Application Model(AWS SAM） It is an open source architecture for building serverless applications\nPros Basically, the infrastructure is outsourced to a cloud service provider to help you solve\nHigh availability: Increase program traffic flexibility and stability through automatic scaling and load balancing Reduced server idle performance: charge according to usage, use as much as you want instead of keeping the host on all the time Reduced server setup and maintenance labor costs Security: No need to worry about the overall security of the server Cons It primarily stems from the issues arising from not having servers running continuously.\nDebugging can be a bit challenging and requires the use of log processing mechanisms like CloudWatch. Can\u0026rsquo;t handle Schedule job =\u0026gt; Lambda not always running There is a startup time involved when executing Lambdas. Request cannot be guaranteed to run on the same Lambda =\u0026gt; Lambdas are stateless, state synchronization must be handled through other services. Architecture By using Lambda to write different functions, it replaces the traditional monolithic codebase that runs on virtual servers.\nThis can be seen as an implementation of Function as a Service (FaaS) paradigm.\nThe general flow is as follows:\nEvent Source -\u0026gt; Lambda Function -\u0026gt; Service Event Source: An event or trigger occurs, such as an API request, database update, or message in a queue. This event acts as the source of the operation. Lambda Function: The event is passed to a Lambda function, which is a small piece of code responsible for processing the event and performing the necessary actions or computations. The Lambda function is executed in a serverless environment. Service: The Lambda function interacts with other services or resources, such as databases, APIs, or external systems, to complete the required operations. These services provide the necessary functionality and data processing capabilities. For a simple to-do list application, the system architecture diagram using Serverless would look like this\nThe key focus would be on the dashed section labeled ToDo App on the right side of the diagram.\nThis section aligns with the flow of Event Source -\u0026gt; Lambda Function -\u0026gt; Service mentioned above.\n",
    "ref": "/en/blog/202210-what-is-serverless/"
  },{
    "title": "Interacting with smart contracts using Golang",
    "date": "",
    "description": "Generating a Go package with abigen tool to interact with smart contracts and perform contract operations",
    "body": "Previously, I was calling Smart Contracts through PHP, which raised two issues.\nOne was the slowness, and the other was the lack of maintenance of the package.\nFortunately, Golang provides an official package that is both fast and solves these two problems.\nThis article explains how to convert existing Smart Contract ABIs into a Go package for interacting with contracts.\nPreparation To set up the necessary packages on Mac, you need to install the following dependencies through Homebrew beforehand.\nbrew update brew tap ethereum/ethereum brew install ethereum brew install solidity brew install protobuf Otherwise, you may encounter issues in the subsequent steps.\nPlease install solc\nPlease install protoc\nInstall go-ethereum Next, you need to install the conversion tool we will be using.\nThe official package comes with the abigen command, which allows you to convert the ABI into a Go package.\ngit clone https://github.com/ethereum/go-ethereum.git cd go-ethereum make devtools Generate Go library file Once the installation is complete, you will need the smart contract ABI to perform the conversion.\nHowever, the topic of what a smart contract ABI is falls outside the scope of this article.\nIf you require a more in-depth explanation of smart contract operations in the future, it can be covered separately.\nOnce you have obtained the abi.json file for the smart contract, you can proceed with the following command execution:\nabigen --abi=\u0026#34;./erc721.abi.json\u0026#34; --type=\u0026#34;erc721\u0026#34; --pkg=erc721 --out=\u0026#34;erc721.go\u0026#34; flags description usage \u0026ndash;abi file path for smart contract abi.json ./erc721.abi.json \u0026ndash;type type name in struct erc721 \u0026ndash;pkg go package name in output file erc721 \u0026ndash;out output file name erc721.go As a result, you will have a file named erc721.go, and upon opening it, you will find that the package name is erc721.\nBy comparing the functions inside the file, you will notice that they correspond to the functions in the abi.\nNow, you can interact with the contract by importing and referencing the erc721 package.\nUsing the package in Golang. To interact with the generated erc721 package in Go, you need to first import the package in your code.\nSince blockchain operations are essential, you should also import go-ethereum. Here\u0026rsquo;s an example:\nimport ( erc721 \u0026#34;project/package-erc721/path\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/common\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/ethclient\u0026#34; ) Then interacting with smart contracts using go-ethereum.\nThe following example demonstrates the interaction with the TotalSupply function in a smart contract.\nPassing a smart contract\u0026rsquo;s contract address string: hexAddress.\nYou can obtain the total supply issued by the contract, and this example focuses on an ERC721 NFT contract.\ninfuraUrl := \u0026#34;https://mainnet.infura.io/v3/38ad7d4b...97aa1d5c583\u0026#34; client, err := ethclient.Dial(infuraUrl) if err != nil { return nil, err } defer client.Close() address := common.HexToAddress(hexAddress) instance, err := erc721.NewErc721(address, client) if err != nil { return nil, err } totalSupply, err := instance.TotalSupply(nil) if err != nil { return nil, err } return totalSupply, nil This post illustrates how to use abigen to generate a corresponding Go package for a Smart Contract.\nSo far, the functions called, including Owner, OwnerOf, and TotalSupply, have successfully retrieved data from the blockchain.\nThere are other applications to explore in the future as opportunities arise for further operations.\n",
    "ref": "/en/blog/202207-go-ethereum-abigen/"
  },{
    "title": "Create a Redis service on GCP",
    "date": "",
    "description": "Set up a Redis service using Google Cloud MemoryStore.",
    "body": "GCP offers the Google Cloud MemoryStore service for creating Redis or Memcached caching machine services.\nThe pricing is similar to that of setting up a database.\nI thought it would be as inexpensive as using virtual machines (VMs).\nCreating Redis Go to the MemoryStore page and click on either \u0026ldquo;Redis\u0026rdquo; or \u0026ldquo;Memcached\u0026rdquo; to begin the setup process.\nIn this example, for Redis, click on \u0026ldquo;Create Instance\u0026rdquo; to proceed.\nEnter some basic configurations, which mainly involve adjusting the size and name.\nAfter configuring the options as prompted, click on \u0026ldquo;Create Instance\u0026rdquo; and then wait for it to be created.\nAt this point, there will be a short waiting period, so you can take a break in the meantime.\nThis is the method to create Redis on GCP.\nOnce created, you only need to map the endpoint in your application to the IP of the Redis instance to start using it.\nAlternatively, you can also use third-party GUI tools like Another Redis Desktop Manager to access via IP address.\n",
    "ref": "/en/blog/202206-gcp-redis/"
  },{
    "title": "Make your computer be temporarily accessible on the internet. - Ngrok",
    "date": "",
    "description": "Use Ngrok to temporarily obtain a publicly accessible URL for your computer.",
    "body": "In development, it is often encountered that testing on the local machine requires a publicly accessible URL for services.\nOr webhooks to use as a callback. Moreover, these services often require HTTPS certification.\nAre there any other options besides setting up a cloud server for this purpose?\nNgrok As a forwarding server, it can redirect external requests to a specified port on your machine.\nThe principle is to connect to the Ngrok cloud server, which exposes the address you specify on your local machine.\nNgrok then provides a public URL for accessing the content.\nThe advantages are that it is fast, provides HTTPS services for enhanced security, and you can even set up password protection.\nOfficial documentation and download links Install Mac brew install ngrok Linux Determine the hardware architecture of your host machine.\ncat /proc/cpuinfo Download the specified file from the official website and follow the steps to install.\nOr just install using snap\nsudo apt update sudo apt install snapd sudo snap install ngrok Start service Enter the command to start and listen on port 8080.\nngrok http 8080 You will then be able to see the publicly accessible URL.\n註冊 ngrok You can use the service without registering, but after a period of time, the connection will be terminated.\nAnd upon restarting, a new URL will be assigned.\nHowever, when testing webhooks or providing the URL to others, having to reassign the URL means reconfiguring webhooks or notifying others, which can be inconvenient.\nAfter logging into your Ngrok account, go to the Your Authtoken page.\nCopy the Authtoken and then enter it in the terminal using the following command:\nngrok config add-authtoken {Your Authtoken} Seeing the following message indicates that the authentication process is complete.\nAuthtoken saved to configuration file: /Users/user_name/.ngrok2/ngrok.yml After completing the registration, you can use the Ngrok service without worrying about the connection being terminated after a while.\n",
    "ref": "/en/blog/202204-ngrok/"
  },{
    "title": "Introducing Laravel Sail and Basic Operations",
    "date": "",
    "description": "Using Laravel Sail to launch the development environment with ease and joy.",
    "body": "In the context of Laravel development environment setup, both official and unofficial sources offer a plethora of approaches.\nThis time, we will be introducing a package introduced after Laravel 8: Laravel Sail .\nIn the past, to save beginners time in the initial setup of environments, there have been numerous tools for development environments.\nFor example, Laravel Homestead , built using virtual machines, was one option.\nThere were also Docker-based development environments like laradock .\nOr creating a Laravel development environment using Docker , as I personally do.\nIn Laravel 8 and onwards, Laravel Sail has been integrated as a built-in package.\nCompared to Laravel Homestead, it requires fewer resources as it utilizes Docker.\nIn contrast to laradock, which also operates within a Docker environment, Laravel Sail offers simpler configuration, and in some cases, even requires no configuration, achieving a plug-and-play experience.\nYou need to install Docker before start using Laravel Sail\nInstall Sail In Laravel 9, Sail is now included as a built-in feature, allowing you to directly launch services through commands.\nphp artisan sail:install For older versions of Laravel that do not have Sail, you can install it using Composer.\ncomposer require laravel/sail --dev Upon executing the command, an interactive interface will prompt you to select the desired services.\nWhich services would you like to install? [mysql]: [0] mysql [1] pgsql [2] mariadb [3] redis [4] memcached [5] meilisearch [6] minio [7] mailhog [8] selenium \u0026gt; 0,3,7 Sail scaffolding installed successfully. After making your selections, a docker-compose.yml file will be generated in the project\u0026rsquo;s root directory.\nYou can then use a command to launch the Sail service.\n./vendor/bin/sail up 設定指令別名 If you\u0026rsquo;re feeling a bit lazy and don\u0026rsquo;t want to type out the entire command, you can set up an alias for it.\nOpen either vim ~/.bashrc or vim ~/.zshrc (depending on your terminal).\nAdd the following line to the file:\nalias sail=\u0026#34;./vendor/bin/sail\u0026#34; After that, you can directly execute Sail commands within the project using the sail alias.\nThe following operations are performed assuming you\u0026rsquo;ve added the alias.\nSail command The usage is similar to Docker commands:\nsail up -d：Start and run in the background sail stop：Stop sail down：Stop and remove containers sail build --no-cache：Rebuild containers, ignoring all caches for a complete rebuild. Adjust docker image The default image includes the basic Laravel environment.\nIf you need to make adjustments, such as installing additional PHP extensions, you will need to export the relevant configurations.\nphp artisan sail:publish After executing the command, a docker folder will be added to the project, containing container settings and Dockerfiles.\nExecute command Sail provides a convenient way to call various commands by adding sail in front of them.\nEssentially, you prepend sail to the desired commands.\nExecute PHP command sail php --version Execute Composer command sail composer install # composer install Execute Artisan command sail artisan queue:work # php artisan queue:work sail artisan schedule:work # php artisan schedule:work Execute shell command sail shell myShell.sh # sh myShell.sh By following these simple steps, you can easily set up a local development environment and execute commands as well.\nLaravel Sail significantly reduces many of the development hassles!\n",
    "ref": "/en/blog/202204-laravel-sail/"
  },{
    "title": "Laravel Queue Usage",
    "date": "",
    "description": "Using queues to defer processing of tasks that don't need immediate attention.",
    "body": "The purpose of queues is to defer processing of a time-consuming task that doesn\u0026rsquo;t require immediate completion. Like sending emails, users don\u0026rsquo;t need to wait for the email to be sent successfully before the next step.\nAllowing the application to respond more quickly to web requests and smooth user experience.\nPreparation Laravel supports multiple queue drivers\nDatabase Redis Amazon SQS If testing locally, you can configure it in the .env file as follows:\nQUEUE_CONNECTION=sync This will execute immediately after sending the task, making it more convenient for testing Queue-related code.\nIn the following example, we will demonstrate using database as the driver.\nQUEUE_CONNECTION=database Create Queue table Because Queue is a feature provided by Laravel.\nYou can directly create a database table named Jobs to record pending Queue information using a command.\nphp artisan queue:table php artisan migrate Create Job files You can create it manually or by using a command.\nphp artisan make:job SyncBlockchainNftItemJob At this point, a file will be generated in the specified path: app/Jobs/SyncBlockchainNftItemJob.php\nWrite the Job program logic. Just modify the SyncBlockchainNftItemJob.php we created earlier.\nAnd the main functionality should be written inside the handle() method, like this\npublic function handle() { if ($this-\u0026gt;userId) { Artisan::call(\u0026#34;nft-items:sync $this-\u0026gt;userId\u0026#34;); } } Call an Artisan command to execute something.\nThe required parameters can be initialized in the __construct at the beginning of the Job program file and can be used as input parameters when creating Queue tasks in the future.\nprotected $userId; public function __construct($userId) { $this-\u0026gt;userId = $userId; } Call the Job to create a task. Now that the Job is ready, how do you call it from the Controller?\nAfter including SyncBlockchainNftItemJob, you can use dispatch wherever you want to create a task and assign it to the specified Job.\n$this-\u0026gt;dispatch(new SyncBlockchainNftItemJob($user-\u0026gt;id)); // Alternatively, you can keep it even simpler. SyncBlockchainNftItemJob::dispatch($user-\u0026gt;id); Starting Queue Worker If sync was not used as the driver earlier, the Queue won\u0026rsquo;t execute!\nYou need to use a command to instruct the Queue to start working!\nphp artisan queue:work It\u0026rsquo;s important to note that once Queue Workers are started, they won\u0026rsquo;t automatically update when there are code changes.\nDuring the deployment phase, remember to use a command to restart the Queue worker. Otherwise, it will continue running the old version of the code!\nphp artisan queue:restart Check status ps -ef|grep queue:work Check execution status Executing the Controller\u0026rsquo;s code, you\u0026rsquo;ll notice that when the Queue is triggered, a new record is added to the jobs table.\nAnd during execution, the terminal will display corresponding information.\nIf you see Processed, it means the task has been completed, and at this point, the record in jobs will be removed.\nSupervisor When the Queue is running, various situations can lead to critical errors, preventing the execution of tasks.\nIn such cases, it\u0026rsquo;s recommended by the official documentation to use Supervisor for management.\nIn the event that the Queue unexpectedly stops operating, Supervisor will restart the Queue service based on the configuration file, ensuring that Jobs can run continuously!\nInstall from docker image FROM php:7.4-fpm RUN apt-get install supervisor CMD /var/www/html/_scripts/cron.sh In the last line of our CMD, we executed a cron.sh as the entry point, which will be used later.\nSupervisor Config Place the file wherever you like.\n# supervisord.conf [program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /var/www/html/artisan queue:work --sleep=90 --tries=3 autostart=true autorestart=true startsecs=5 user=root numprocs=3 redirect_stderr=true stdout_logfile=/var/www/html/storage/logs/supervisord.log stopwaitsecs=3600 [supervisord] logfile=/var/log/supervisor/supervisord_main.log pidfile=/var/www/html/storage/logs/supervisord.pid [supervisorctl] Start Supervisor Create the _scripts/cron.sh file in the project and mount it to the container\u0026rsquo;s /var/www/html path.\n#!/usr/bin/env bash cd /var/www/html supervisord -c ./docker/cron/supervisord.conf # Start supervisord using the configuration file located at the specified path php /var/www/html/artisan schedule:work # Simultaneously initiate the cron job. When the Dockerfile executes a shell script via CMD, it can be considered as\nsh _scripts/cron.sh\nIn cron.sh, we\u0026rsquo;ve done three things:\nNavigate to the container\u0026rsquo;s /var/www/html path. Start the supervisord service using the specified config. Execute php artisan schedule:work. This way, both supervisord and the cron job service are initiated.\nCheck Supervisor status You can use a command to confirm whether supervisord is running.\npgrep -fl supervisord # or ps aux|grep supervisord If an error message appears indicating that the ps or pgrep commands do not exist, you will need to install the package using a command.\napt-get update \u0026amp;\u0026amp; apt-get install procps If you see supervisord listed, it means it\u0026rsquo;s already running.\n",
    "ref": "/en/blog/202203-laravel-queue/"
  },{
    "title": "About",
    "date": "",
    "description": "About Byte Ebi",
    "body": "Familiar with PHP including CodeIgniter, WordPress, Laravel.\nKnow a little about frontend and have experience with container.\nNow I am using Golang most of the time. And studying for AWS services.\nMake some small tool in my free time.\nFeel free to contact me via LinkedIn. I wanna have some foreign friends.\nLove Japanese culture after I star watching Vtuber.\nIf you want to talk about that, please do not hesitate to contact me.\n",
    "ref": "/en/about/"
  }]
