[{
    "title": "Different between pointer and value for struct type in golang",
    "date": "",
    "description": "The different between pointer and value for struct type in golang with examples",
    "body": "I always had a question \u0026ldquo;Should I use value for type in struct? Or using pointer?\u0026rdquo;\nAfter reading many different projects on the web, it seems that they all have different practices.\nUntil I recently encountered a problem that gave me some ideas.\nIn our projects, we often start by defining the Request incoming Struct\nThen use json.Unmarshal to convert the incoming json string into Struct for subsequent operations\nPreviously, the default value for fields was false, so we pass by value.\nIn this case, the default value is true and the problem happened.\nTake the first look at the code and execution results\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type John struct { Name string `json:\u0026#34;name\u0026#34;` CanRun bool `json:\u0026#34;canRun\u0026#34;` CanFly bool `json:\u0026#34;canFly\u0026#34;` } type Doe struct { Name string `json:\u0026#34;name\u0026#34;` CanRun *bool `json:\u0026#34;canRun\u0026#34;` CanFly *bool `json:\u0026#34;canFly\u0026#34;` } func main() { var param = []byte(`{\u0026#34;Name\u0026#34;:\u0026#34;John Doe\u0026#34;, \u0026#34;canRun\u0026#34;:true}`) defult := true var john = new(John) json.Unmarshal(param, \u0026amp;john) jsondata, _ := json.Marshal(john) fmt.Println(string(jsondata)) var doe = new(Doe) json.Unmarshal(param, \u0026amp;doe) doejsondata, _ := json.Marshal(doe) fmt.Println(string(doejsondata)) if doe.CanFly == nil { doe.CanFly = \u0026amp;defult } doejsondata, _ = json.Marshal(doe) fmt.Println(string(doejsondata)) } In the example, there are two Structs.\nJohn, whose Type is a Bollinger pass by value, and Doe, whose Type is a Bollinger pass by pointer.\nIf the CanFly field is \u0026ldquo;optional\u0026rdquo; and the default value is \u0026ldquo;true\u0026rdquo;\nThe param in the example code play as a request without CanFly parameter\nAssign the variables of type \u0026ldquo;John\u0026rdquo; and \u0026ldquo;Doe\u0026rdquo; Use json.Unmarshal to convert the incoming json string to Struct Convert to json strings via json.Marshal for easy reading and printing on the screen The first line of the output is \u0026ldquo;John\u0026rdquo;, which using the boolean value as type.\nWhen calling new(John) will automatically bring in the zero value .\nAnd the default zero value of boolean is false.\nAnd Name and CanRnu are overwritten with zero value because they are passed in.\nBut because CanFly is optional and not passed in by the user, it is still false\nWe can\u0026rsquo;t identify the false is from user or from zero value.\nIf you use \u0026ldquo;pointer\u0026rdquo; as the type in Struct, you can see that the second line of CanFly is null.\nSo you can set the default value to \u0026ldquo;true\u0026rdquo; later on.\nThis is what I learned about the default value problem when converting json strings to Struct, and how to solve it with a pointer.\n",
    "ref": "/en/blog/202306-different-between-pointer-and-value-for-struct-type-in-golang/"
  },{
    "title": "Using FFmpeg to convert media files",
    "date": "",
    "description": "Convert .ts video files to .mp4",
    "body": "Sometimes, we can use certain methods to download the .M3U8 file and then merge it into a .ts file.\nHowever, computers usually cannot directly play this format.\nWe can use FFmpeg to convert it into a common .mp4 file for easy playback.\nInstallation First, we need to download FFmpeg. If you are a Mac user, you can install it using Homebrew .\nbrew install ffmpeg To check if the installation is successful, you can use the following command:\nffmpeg -version Convert media files Once you have installed it, the next step is file conversion.\nUse the following command and replace the source .ts file and the destination path with your own target path:\nffmpeg -i /source_path/vedio.ts -acodec copy -vcodec copy -f mp4 /target_path/new_vedio.mp4 After executing the command, you will be able to see the converted file in the target path!\nWith FFmpeg, you no longer need to install additional software or rely on online file conversion services.\n",
    "ref": "/en/blog/202301-ffmpeg-video-convert/"
  },{
    "title": "[AWS 101] What are SNS and SQS",
    "date": "",
    "description": "Briefly note what AWS SNS and SQS services do and their use cases",
    "body": "Amazon Simple Queue Service (SQS) and Amazon Simple Notification Service (SNS) may look similar.\nBut in reality, they are quite different.\nTo better clarify the differences between the two, let\u0026rsquo;s compare the notes on both services side by side.\nAWS SNS Full name is: Amazon Simple Notification Service\nThe publisher system can use Topics to distribute event messages to a large number of subscriber systems for parallel processing.\nThe subscribers include:\nAmazon SQS AWS Lambda function HTTPS endpoint Amazon Kinesis Data Firehose The architecture looks like this:\nFeatures: Send real-time messages to all subscribers. Do not retain sent messages. Follow the Publish–subscribe pattern. AWS SQS Full name is: Amazon Simple Queue Service (SQS)\nIt is a fully managed message queue service.\nMessages are sent to a queue and processed by the recipient who retrieves them.\nIt is a Serverless solution, but the executor can be either Lambda function or non-Serverless projects like Laravel.\nType Two types of queues are provided to cater to different application requirements:\nStandard Queue The application can handle messages that arrive more than once and are not ordered sequentially.\nUnlimited Throughput: Standard queues support nearly unlimited transactions per second (TPS) for each API action. At-Least-Once Delivery: A message is guaranteed to be delivered at least once, but occasionally, multiple copies of the message may be delivered. Best-Effort Ordering: Messages are occasionally not delivered in the exact order they were sent. FIFO Queue When the order of operations and events is crucial or duplicates are not acceptable:\nHigh Throughput: By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). Exactly-Once Processing: Messages are delivered exactly once and remain available until the consumer processes and deletes them. First-In-First-Out Delivery: Messages are strictly delivered and received in the order they were sent (First In, First Out - FIFO). Poll Short polling This method is similar to someone repeatedly asking you if there is any job available, even if there is no job.\nYou will get immediate results after each inquiry, even if the queue is empty.\nHowever, to ensure you receive the latest status, you need to keep querying continuously.\nLong polling After making a request, you will not receive an immediate response unless there is a timeout or a message in the queue.\nThis approach minimizes additional polling to reduce costs while ensuring the fastest possible reception of new messages.\nWhen the queue is empty, a long-polling request for the next message can wait for a maximum of 20 seconds.\nFeatures: The receiver actively polls messages. A queue can only be associated with a single consumer. The message is deleted only after the consumer responds with the completion of the processing. Difference between SNS and SQS SNS When new content is updated, event notifications are sent to all subscribers.\nIt follows a push-based architecture where messages are automatically pushed to the subscribers.\nIn simple terms, it is a broadcast notification. SQS Separating queue tasks from the codebase involves a Pull-Based architecture\nConsumers are responsible for pulling messages from the queue and processing them on their own.\nIn simple terms, it is a Queue Here\u0026rsquo;s an example of how these features and architectures can be combined: 1. Methods for creating an SNS Topic and SQS: Access the SNS service and create an SNS topic. Access the SQS service and create an SQS queue. Subscribe the newly created SQS queue to the SNS topic just created to retrieve podcast content. In the SQS Dashboard, select the target SQS queue. Click on the Action dropdown menu and choose Subscribe to Amazon SNS topic. Select the topic and click \u0026ldquo;Save.\u0026rdquo; 2. Simulate sending a message from SNS 到 SNS Topic 頁面並選擇 Topic 後，點選右上角Publish message 輸入測試用的內容，並按下Publish message 3. Confirm if the subscription message has been received After publishing a message in SNS, all endpoints subscribed to the topic should receive it.\nOpen the SQS page and select the SQS queue that is subscribed to the topic. Click on Send and receive message. 往Scroll down to the \u0026ldquo;Receive messages\u0026rdquo; section and select Poll for Messages. Once successful, you will see a list of messages appearing below, and you can click on them to view detailed content. Reference source: Send Fanout Event Notifications ",
    "ref": "/en/blog/202210-aws101-what-is-sns-and-sqs/"
  },{
    "title": "Serverless 101",
    "date": "",
    "description": "Introducing What is Serverless with AWS Services",
    "body": "Since our company\u0026rsquo;s services are built on AWS and heavily rely on various serverless computing services,\nI had no prior experience in this area. This series serves as a compilation of notes for me,\nand its content may be subject to constant revisions in the future.\nAlthough it\u0026rsquo;s called \u0026ldquo;serverless,\u0026rdquo; servers still exist in reality; they are simply deployed and maintained by the cloud platform.\nFor users, all they need to do is write the code without having to worry about server tuning.\nBackend engineers often have to manage servers on the side, and they know how painful it is.\nHowever, with the Serverless architecture, infrastructure management can be delegated to cloud service providers simply by writing configuration files (and pulling out the magical card from the boss).\nCommon cloud service providers all have their own Serverless services\nAWS：Lambda Microsoft Azure：Functions Google：Cloud Functions The Serverless-related services provided by AWS can be broadly categorized as follows:\nServerless ≠ Lambda Function That is a common misconception.\nIn addition to Lambda Functions, Serverless also encompasses the composition of event sources and other resources.\nThis includes APIs, databases, and event triggers that work together to execute tasks.\nAnd AWS provides AWS Serverless Application Model(AWS SAM） It is an open source architecture for building serverless applications\nPros Basically, the infrastructure is outsourced to a cloud service provider to help you solve\nHigh availability: Increase program traffic flexibility and stability through automatic scaling and load balancing Reduced server idle performance: charge according to usage, use as much as you want instead of keeping the host on all the time Reduced server setup and maintenance labor costs Security: No need to worry about the overall security of the server Cons It primarily stems from the issues arising from not having servers running continuously.\nDebugging can be a bit challenging and requires the use of log processing mechanisms like CloudWatch. Can\u0026rsquo;t handle Schedule job =\u0026gt; Lambda not always running There is a startup time involved when executing Lambdas. Request cannot be guaranteed to run on the same Lambda =\u0026gt; Lambdas are stateless, state synchronization must be handled through other services. Architecture By using Lambda to write different functions, it replaces the traditional monolithic codebase that runs on virtual servers.\nThis can be seen as an implementation of Function as a Service (FaaS) paradigm.\nThe general flow is as follows:\nEvent Source -\u0026gt; Lambda Function -\u0026gt; Service Event Source: An event or trigger occurs, such as an API request, database update, or message in a queue. This event acts as the source of the operation. Lambda Function: The event is passed to a Lambda function, which is a small piece of code responsible for processing the event and performing the necessary actions or computations. The Lambda function is executed in a serverless environment. Service: The Lambda function interacts with other services or resources, such as databases, APIs, or external systems, to complete the required operations. These services provide the necessary functionality and data processing capabilities. For a simple to-do list application, the system architecture diagram using Serverless would look like this\nThe key focus would be on the dashed section labeled ToDo App on the right side of the diagram.\nThis section aligns with the flow of Event Source -\u0026gt; Lambda Function -\u0026gt; Service mentioned above.\n",
    "ref": "/en/blog/202210-what-is-serverless/"
  },{
    "title": "Interacting with smart contracts using Golang",
    "date": "",
    "description": "Generating a Go package with abigen tool to interact with smart contracts and perform contract operations",
    "body": "Previously, I was calling Smart Contracts through PHP, which raised two issues.\nOne was the slowness, and the other was the lack of maintenance of the package.\nFortunately, Golang provides an official package that is both fast and solves these two problems.\nThis article explains how to convert existing Smart Contract ABIs into a Go package for interacting with contracts.\nPreparation To set up the necessary packages on Mac, you need to install the following dependencies through Homebrew beforehand.\nbrew update brew tap ethereum/ethereum brew install ethereum brew install solidity brew install protobuf Otherwise, you may encounter issues in the subsequent steps.\nPlease install solc\nPlease install protoc\nInstall go-ethereum Next, you need to install the conversion tool we will be using.\nThe official package comes with the abigen command, which allows you to convert the ABI into a Go package.\ngit clone https://github.com/ethereum/go-ethereum.git cd go-ethereum make devtools Generate Go library file Once the installation is complete, you will need the smart contract ABI to perform the conversion.\nHowever, the topic of what a smart contract ABI is falls outside the scope of this article.\nIf you require a more in-depth explanation of smart contract operations in the future, it can be covered separately.\nOnce you have obtained the abi.json file for the smart contract, you can proceed with the following command execution:\nabigen --abi=\u0026#34;./erc721.abi.json\u0026#34; --type=\u0026#34;erc721\u0026#34; --pkg=erc721 --out=\u0026#34;erc721.go\u0026#34; flags description usage \u0026ndash;abi file path for smart contract abi.json ./erc721.abi.json \u0026ndash;type type name in struct erc721 \u0026ndash;pkg go package name in output file erc721 \u0026ndash;out output file name erc721.go As a result, you will have a file named erc721.go, and upon opening it, you will find that the package name is erc721.\nBy comparing the functions inside the file, you will notice that they correspond to the functions in the abi.\nNow, you can interact with the contract by importing and referencing the erc721 package.\nUsing the package in Golang. To interact with the generated erc721 package in Go, you need to first import the package in your code.\nSince blockchain operations are essential, you should also import go-ethereum. Here\u0026rsquo;s an example:\nimport ( erc721 \u0026#34;project/package-erc721/path\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/common\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/ethclient\u0026#34; ) Then interacting with smart contracts using go-ethereum.\nThe following example demonstrates the interaction with the TotalSupply function in a smart contract.\nPassing a smart contract\u0026rsquo;s contract address string: hexAddress.\nYou can obtain the total supply issued by the contract, and this example focuses on an ERC721 NFT contract.\ninfuraUrl := \u0026#34;https://mainnet.infura.io/v3/38ad7d4b...97aa1d5c583\u0026#34; client, err := ethclient.Dial(infuraUrl) if err != nil { return nil, err } defer client.Close() address := common.HexToAddress(hexAddress) instance, err := erc721.NewErc721(address, client) if err != nil { return nil, err } totalSupply, err := instance.TotalSupply(nil) if err != nil { return nil, err } return totalSupply, nil This post illustrates how to use abigen to generate a corresponding Go package for a Smart Contract.\nSo far, the functions called, including Owner, OwnerOf, and TotalSupply, have successfully retrieved data from the blockchain.\nThere are other applications to explore in the future as opportunities arise for further operations.\n",
    "ref": "/en/blog/202207-go-ethereum-abigen/"
  },{
    "title": "Create a Redis service on GCP",
    "date": "",
    "description": "Set up a Redis service using Google Cloud MemoryStore.",
    "body": "GCP offers the Google Cloud MemoryStore service for creating Redis or Memcached caching machine services.\nThe pricing is similar to that of setting up a database.\nI thought it would be as inexpensive as using virtual machines (VMs).\nCreating Redis Go to the MemoryStore page and click on either \u0026ldquo;Redis\u0026rdquo; or \u0026ldquo;Memcached\u0026rdquo; to begin the setup process.\nIn this example, for Redis, click on \u0026ldquo;Create Instance\u0026rdquo; to proceed.\nEnter some basic configurations, which mainly involve adjusting the size and name.\nAfter configuring the options as prompted, click on \u0026ldquo;Create Instance\u0026rdquo; and then wait for it to be created.\nAt this point, there will be a short waiting period, so you can take a break in the meantime.\nThis is the method to create Redis on GCP.\nOnce created, you only need to map the endpoint in your application to the IP of the Redis instance to start using it.\nAlternatively, you can also use third-party GUI tools like Another Redis Desktop Manager to access via IP address.\n",
    "ref": "/en/blog/202206-gcp-redis/"
  },{
    "title": "Make your computer be temporarily accessible on the internet. - Ngrok",
    "date": "",
    "description": "Use Ngrok to temporarily obtain a publicly accessible URL for your computer.",
    "body": "In development, it is often encountered that testing on the local machine requires a publicly accessible URL for services.\nOr webhooks to use as a callback. Moreover, these services often require HTTPS certification.\nAre there any other options besides setting up a cloud server for this purpose?\nNgrok As a forwarding server, it can redirect external requests to a specified port on your machine.\nThe principle is to connect to the Ngrok cloud server, which exposes the address you specify on your local machine.\nNgrok then provides a public URL for accessing the content.\nThe advantages are that it is fast, provides HTTPS services for enhanced security, and you can even set up password protection.\nOfficial documentation and download links Install Mac brew install ngrok Linux Determine the hardware architecture of your host machine.\ncat /proc/cpuinfo Download the specified file from the official website and follow the steps to install.\nOr just install using snap\nsudo apt update sudo apt install snapd sudo snap install ngrok Start service Enter the command to start and listen on port 8080.\nngrok http 8080 You will then be able to see the publicly accessible URL.\n註冊 ngrok You can use the service without registering, but after a period of time, the connection will be terminated.\nAnd upon restarting, a new URL will be assigned.\nHowever, when testing webhooks or providing the URL to others, having to reassign the URL means reconfiguring webhooks or notifying others, which can be inconvenient.\nAfter logging into your Ngrok account, go to the Your Authtoken page.\nCopy the Authtoken and then enter it in the terminal using the following command:\nngrok config add-authtoken {Your Authtoken} Seeing the following message indicates that the authentication process is complete.\nAuthtoken saved to configuration file: /Users/user_name/.ngrok2/ngrok.yml After completing the registration, you can use the Ngrok service without worrying about the connection being terminated after a while.\n",
    "ref": "/en/blog/202204-ngrok/"
  },{
    "title": "Jenkins CI/CD 04 - Using SSH Commands to Operate another VM Instance in GCP",
    "date": "",
    "description": "Enable Jenkins to perform remote operations on a remote host through SSH, replacing manual deployment.",
    "body": "In a non-automated deployment scenario, manually connecting to the server host\u0026rsquo;s internals is required every time.\nExecuting deployment commands or running deployment command executables not only involves inconvenience but also carries the risk of human error.\nWith Jenkins\u0026rsquo; pipeline, we can replace manual execution, making deployments easier and more pleasant.\nPackage Installation Yes, you guessed it; we need to install a package again.\nSpoiler alert: Don\u0026rsquo;t install it just yet. Consider it after reading the next paragraph.\nThis time, we need to install support for the pipeline: SSH Pipeline Steps .\nThe installation process is the same as for other packages.\nIn the side menu, select Manage Jenkins -\u0026gt; Manage Plugins and search for ssh-steps.\nThen install the SSH Pipeline StepsVersion package.\nUsing gcloud for Login Then it suddenly occurred to me that we had installed gcloud earlier, and gcloud itself has the capability to directly connect via SSH to other instances within the same project! No need to install additional packages! Congratulations!\nIf you haven\u0026rsquo;t installed the gcloud command yet, you can refer to the tutorial in a previous article: Jenkins CI/CD 03 - Building and Pushing Docker Images to GCR .\nThe prerequisite for using gcloud for SSH commands is to have the corresponding permissions set in your Identity and Access Management (IAM).\nIf the commands in the following steps are denied, you\u0026rsquo;ll have to figure it out yourself.\nIf you see an error message, adjust the permissions on the IAM page as suggested.\nFirst, switch the user to Jenkins on the Jenkins host:\nsudo su jenkins Then, enter the command:\ngcloud compute ssh INTERNAL_INSTANCE_NAME --zone=ZONE --internal-ip The first time you connect, it will prompt you to generate and add an SSH key.\nThis is where IAM permissions issues are most likely to occur.\nIf you can\u0026rsquo;t log in, it means you need to adjust the permissions for the key you previously set.\nAfter setting up the IAM correctly, follow the prompts to generate an SSH key, which will be automatically added to the metadata in Compute Engine. Afterward, you will be able to log in directly.\nConnecting with gcloud Commands in the Pipeline // jenkinsfile pipeline { agent any stages { stage(\u0026#39;Deploy branch: develop to beta\u0026#39;) { when { branch \u0026#39;develop\u0026#39; } steps { echo \u0026#34;ssh to store-beta-api instance.\u0026#34; withCredentials([file(credentialsId: \u0026#39;jenkins-gcr\u0026#39;, variable: \u0026#39;GC_KEY\u0026#39;)]) { sh \u0026#34;gcloud compute ssh store-beta-api --zone=asia-east1-b --internal-ip --command \u0026#39;cd /data/store-backend \u0026amp;\u0026amp; sudo sh ./_scripts/deploy_beta.sh\u0026#39;\u0026#34; } echo \u0026#34;Deploy beta done\u0026#34; } } } } A crucial point to note is that you need to use --command to specify the remote command, and it should be a single command string. because the connection is terminated after executing that line. The state doesn\u0026rsquo;t persist!\nIn the example, we demonstrate SSH connection using gcloud commands to a VM instance named store-beta-api within the same GCP project in the asia-east1-b region, accessed through its internal static IP address.\nOnce the connection is successful, two commands are executed: one to enter the /data/store-backend directory and the other to run the ./_scripts/deploy_beta.sh executable file within that directory.\nIn small-scale services or internal testing environments, you typically have only one VM instance running.\nWith this example, you can automatically connect to the host\u0026rsquo;s internals for deployment when the trigger conditions are met.\nReference: gcloud compute ssh ",
    "ref": "/en/blog/202204-jenkins-cicd-4-ssh-remote-server/"
  },{
    "title": "Jenkins CI/CD 03 - Building and Pushing Docker Images to GCR",
    "date": "",
    "description": "This article discusses the process of building a Docker image and pushing it to the Google Container Registry (GCR) within a Jenkins pipeline using the 'gcloud' command.",
    "body": "Google Container Registry is a service provided by Google for storing, managing, and securing Docker container images.\nThe objective at hand is to package a Docker image and push it to Google Cloud Platform (GCP) using the \u0026lsquo;gcloud\u0026rsquo; command for future deployment purposes.\nGoogle Container Registry (GCR) Service First and foremost, you need a Google Cloud project, which can be created following the instructions provided in the official documentation: Creating and managing projects .\nInstalling gcloud on the Jenkins Host The following steps outline the installation process for gcloud as the official documentation: Installing the gcloud CLI .\nPrerequisites Ensure that the following packages are installed: apt-transport-https, ca-certificates, and gnupg. You can list all installed packages using the following command:\napt list --installed If any of the packages are missing, you can install them using the following command:\nsudo apt-get install apt-transport-https ca-certificates gnupg Add the gcloud CLI Distribution URI as a Package Source echo \u0026#34;deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list # If \u0026#39;signed-by\u0026#39; is not supported, you can use the following command instead echo \u0026#34;deb https://packages.cloud.google.com/apt cloud-sdk main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list Import the Google Cloud Public Key If the apt-key command supports the --keyring parameter, you can use the following command:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Otherwise, you can use the following command:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - For Debian 11+ or Ubuntu 21.10+ distributions that do not support the apt-key command, you can use this command:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo tee /usr/share/keyrings/cloud.google.gpg Update and Install gcloud CLI Run the following commands to update and install the gcloud CLI:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install google-cloud-cli Initialize gcloud Since we are operating gcloud within a Google Compute Engine virtual machine under the same GCP project,\nthe gcloud auth login step is skipped.\nInstead, the VM host is used as the login account, and the command will prompt for confirmation as follows:\nYou are running on a Google Compute Engine virtual machine. It is recommended that you use service accounts for authentication. You can run: $ gcloud config set account `ACCOUNT` to switch accounts if necessary. Your credentials may be visible to others with access to this virtual machine. Are you sure you want to authenticate with your personal account? Do you want to continue (Y/n)? You can check the list of projects with the following command:\ngcloud projects list If you encounter the error message:\nERROR: (gcloud.projects.list) PERMISSION_DENIED: Request had insufficient authentication scopes.\nFirst stop the VM and navigate to the editing page.\nFind the Access scopes section and enable Full access to all Cloud APIs.\nAfter restarting the VM, execute the command and follow the prompts:\ngcloud init # Your Google Cloud SDK is configured and ready to use! You can use the following command to confirm the configuration settings:\ngcloud config configurations list Setting Jenkins Permissions for Push Once you\u0026rsquo;ve verified that the gcloud command is functional, you need to set up Jenkins to push images to GCR.\nGenerate a GCP Key Navigate to the GCP Console, click on APIs \u0026amp; Services, and select the Credentials tab.\nClick Create credentials and choose Service Account.\nFollow the prompts to create the service account, then click on the account you just created to enter the edit page.\nSwitch to the Keys tab and click \u0026ldquo;Add Key -\u0026gt; create a new key\u0026rdquo; choosing the \u0026ldquo;JSON\u0026rdquo; type.\nA download prompt will appear; download the JSON key file to your local machine.\nConfigure the Key in Jenkins 進到Jenkins -\u0026gt; 管理 Jenkins -\u0026gt; Manage Credentials\n選擇作用範圍，這邊用預設的Jenkins也就是全域(global)\nNavigate to Jenkins -\u0026gt; Manage Jenkins -\u0026gt; Manage Credentials.\nChoose the scope, which, in this example, is the default Jenkins as global.\nClick \u0026ldquo;Add Credentials\u0026rdquo; and select Secret file. Fill in the required information:\nID: A unique identifier for calling within Jenkins, in this example, it is named jenkins-gcr. File: Upload the JSON key file you downloaded earlier. This completes the Jenkins key setup.\nPipeline Example // jenkinsfile pipeline { agent any environment { GCR_HOST = \u0026#34;asia.gcr.io\u0026#34; // Choose a GCR host location that is geographically close or simply use gcr.io PROJECT_ID = \u0026#34;my-project-315408\u0026#34; // The ID of your GCP project FOLDER = \u0026#34;my-project-backend-test\u0026#34; VERSION = \u0026#34;${TAG_NAME}\u0026#34; IMAGE = \u0026#34;$GCR_HOST/$PROJECT_ID/$FOLDER:$VERSION\u0026#34; // Assemble the image name } stages { stage(\u0026#39;Build docker image\u0026#39;) { when { branch \u0026#39;main\u0026#39; } steps { // Docker build command specifying the path to the Dockerfile in your project sh \u0026#34;docker build . -f ./builds/docker/php81/Dockerfile -t ${IMAGE}\u0026#34; } } stage(\u0026#39;Push image to Google Container Registry\u0026#39;) { when { branch \u0026#39;main\u0026#39; } steps { // \u0026#39;jenkins-gcr\u0026#39; is the unique identifier ID set up earlier withCredentials([file(credentialsId: \u0026#39;jenkins-gcr\u0026#39;, variable: \u0026#39;GC_KEY\u0026#39;)]) { sh \u0026#34;cat \u0026#39;$GC_KEY\u0026#39; | docker login -u _json_key --password-stdin https://asia.gcr.io\u0026#34; sh \u0026#34;gcloud auth activate-service-account --key-file=${GC_KEY}\u0026#34; sh \u0026#34;gcloud auth configure-docker\u0026#34; echo \u0026#34;Pushing image to GCR\u0026#34; sh \u0026#34;docker push ${IMAGE}\u0026#34; } } } } } Finally, trigger the Jenkins service, which, in the example, is done by pushing commits to the main branch.\nChecking the Storage Location 若是沒有錯誤訊息，則可以到Google Cloud Registry頁面查看\n把剛剛上傳的 image 名稱想成在 linux 終端機下指令，就可以理解 GCR 中的資料夾結構\nIf no error messages were encountered, you can check the Google Cloud Registry page to view the image.\nThe image name in GCR is analogous to the path structure in Linux.\n",
    "ref": "/en/blog/202204-jenkins-cicd-3-push-docker-image-to-gcr/"
  },{
    "title": "Scalable Server 02 Load Balancing",
    "date": "",
    "description": "Configuring Load Balancing on GCP for Enhanced Server Flexibility",
    "body": "In the previous post Scalable Server 01 Auto Scaling , it was mentioned that we need to configure a set of Load Balancers to distribute internet requests to different server hosts.\nThis allows us to dynamically adjust the number of hosts based on traffic and maintain stable service operations.\nThe previously created instance group consists of individual hosts with unique IPs.\nHowever, using load balancing allows us to conceal the individual instance IPs and redirect traffic to new machines as traffic increases.\nThis ensures a certain level of security and system stability, preventing the original hosts from crashing due to excessive load.\nCreate a Load Balancing First, locate Network Services in the sidebar menu and select Load Balancing.\nAfter pressing Create Load Balancer on the top toolbar, select HTTP(S) Load Balancer.\nSince the traffic is coming from the internet, select From the internet to VM or serverless service.\nBackend Service configuration Next, select Create a backend service\nChoose port number 80 for internal communication, as our internal servers do not have SSL certificates attached.\nFrontend Service configuration Next, go to Frontend Configuration.\nIf we want to allow access via a fixed set of IP addresses, we\u0026rsquo;ll need to create a set of IP addresses here.\nYou can either pre-create them or reserve a set of fixed IP addresses through on-screen prompts.\nIf you have an SSL certificate, you can also configure it here.\nAfter saving, wait for it to be created.\nThen, click into the details of the newly created load balancer.\nIn the Frontend section, you\u0026rsquo;ll find an IP address.\nPaste this into your browser to access the service you just configured!\nSo, our load balancer is now successfully set up!\nWith the automatic scaling feature, when traffic increases and triggers the scaling rules, new virtual machines will be launched.\nThe load balancer will distribute requests, helping us handle the traffic and maintain stable service operations.\nReference:\n[GCP 教學] 打造彈性、快速且安全的雲端基本服務架構 – 負載平衡 Load Balancer 和 Instance Group GCP VM 雲端主機最基本防護 - Load Balance ",
    "ref": "/en/blog/202204-gcp-load-balancing/"
  },{
    "title": "Introducing Laravel Sail and Basic Operations",
    "date": "",
    "description": "Using Laravel Sail to launch the development environment with ease and joy.",
    "body": "In the context of Laravel development environment setup, both official and unofficial sources offer a plethora of approaches.\nThis time, we will be introducing a package introduced after Laravel 8: Laravel Sail .\nIn the past, to save beginners time in the initial setup of environments, there have been numerous tools for development environments.\nFor example, Laravel Homestead , built using virtual machines, was one option.\nThere were also Docker-based development environments like laradock .\nOr creating a Laravel development environment using Docker , as I personally do.\nIn Laravel 8 and onwards, Laravel Sail has been integrated as a built-in package.\nCompared to Laravel Homestead, it requires fewer resources as it utilizes Docker.\nIn contrast to laradock, which also operates within a Docker environment, Laravel Sail offers simpler configuration, and in some cases, even requires no configuration, achieving a plug-and-play experience.\nYou need to install Docker before start using Laravel Sail\nInstall Sail In Laravel 9, Sail is now included as a built-in feature, allowing you to directly launch services through commands.\nphp artisan sail:install For older versions of Laravel that do not have Sail, you can install it using Composer.\ncomposer require laravel/sail --dev Upon executing the command, an interactive interface will prompt you to select the desired services.\nWhich services would you like to install? [mysql]: [0] mysql [1] pgsql [2] mariadb [3] redis [4] memcached [5] meilisearch [6] minio [7] mailhog [8] selenium \u0026gt; 0,3,7 Sail scaffolding installed successfully. After making your selections, a docker-compose.yml file will be generated in the project\u0026rsquo;s root directory.\nYou can then use a command to launch the Sail service.\n./vendor/bin/sail up 設定指令別名 If you\u0026rsquo;re feeling a bit lazy and don\u0026rsquo;t want to type out the entire command, you can set up an alias for it.\nOpen either vim ~/.bashrc or vim ~/.zshrc (depending on your terminal).\nAdd the following line to the file:\nalias sail=\u0026#34;./vendor/bin/sail\u0026#34; After that, you can directly execute Sail commands within the project using the sail alias.\nThe following operations are performed assuming you\u0026rsquo;ve added the alias.\nSail command The usage is similar to Docker commands:\nsail up -d：Start and run in the background sail stop：Stop sail down：Stop and remove containers sail build --no-cache：Rebuild containers, ignoring all caches for a complete rebuild. Adjust docker image The default image includes the basic Laravel environment.\nIf you need to make adjustments, such as installing additional PHP extensions, you will need to export the relevant configurations.\nphp artisan sail:publish After executing the command, a docker folder will be added to the project, containing container settings and Dockerfiles.\nExecute command Sail provides a convenient way to call various commands by adding sail in front of them.\nEssentially, you prepend sail to the desired commands.\nExecute PHP command sail php --version Execute Composer command sail composer install # composer install Execute Artisan command sail artisan queue:work # php artisan queue:work sail artisan schedule:work # php artisan schedule:work Execute shell command sail shell myShell.sh # sh myShell.sh By following these simple steps, you can easily set up a local development environment and execute commands as well.\nLaravel Sail significantly reduces many of the development hassles!\n",
    "ref": "/en/blog/202204-laravel-sail/"
  },{
    "title": "Jenkins CI/CD 02 Basic Pipeline Setup and GitHub Integration",
    "date": "",
    "description": "Creating a basic pipeline workflow and triggering it through GitHub webhooks",
    "body": "In Jenkins , the build process is known as a pipeline.\nIt can also be triggered through webhooks, offering various triggering and execution methods.\nBelow, we\u0026rsquo;ll demonstrate how to integrate GitHub webhooks and provide common trigger condition examples.\nCreating the First Pipeline Navigate to Open Blue Ocean in the sidebar to create a new pipeline.\nFollow the prompts to create an access token. In this demonstration, I had to generate a new token because I deleted the old one, resulting in a Token deleted error message.\nClick Create an access token here to open the personal access token creation page on GitHub.\nYou can also find it by going to\nGitHub \u0026gt; Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new token\nThe necessary permissions are typically pre-filled; simply click Generate token.\nCopy the generated token and paste it into Jenkins.\nIf you forget to do so at this point, you\u0026rsquo;ll need to regenerate the token.\nOnce verification is complete, you can select a project.\nClick \u0026ldquo;Create,\u0026rdquo; and Jenkins will scan all branches within the GitHub repository.\nConnecting Webhooks to Listen for Events 進入專案的 Settings \u0026gt; Webhook \u0026gt; Add webhook\n在Payload URL填入jenkins主機網址/github-webhook/\n如果 port 不是使用 80 或 443，則要完整輸入。例如 Jenkins 預設使用的是8080port\nNow, let\u0026rsquo;s make some changes to the project and commit and push them to GitHub.\nYou might notice that Jenkins isn\u0026rsquo;t responding.\nThis is because we haven\u0026rsquo;t configured Jenkins to listen to GitHub events.\nHow can we do that? Through webhooks!\nWebhooks transmit events to a specified URL when events configured in the webhook settings are triggered.\nTo allow Jenkins to receive GitHub events, you must bind the webhook to your project.\nThis is typically automated in Drone when you build a project, but not in Jenkins.\nGo to your project\u0026rsquo;s Settings \u0026gt; Webhook \u0026gt; Add webhook.\nIn the Payload URL field, enter your Jenkins server\u0026rsquo;s URL followed by /github-webhook/.\nIf your port isn\u0026rsquo;t 80 or 443, you should include the complete URL.\nFor example, if Jenkins is using the default port 8080:\nClick \u0026ldquo;Add webhook\u0026rdquo; to create it and then test the connection.\nIf the test fails, you can adjust the settings and click the three dots on the right to Redeliver for a retest.\nFrom now on, every time a push event occurs(based on the webhook settings), Jenkins will be notified.\nThis way, we achieve the goal of triggering a Jenkins build process when new content is pushed to GitHub.\nListening for Tag Events In version releases, we often tag the current version.\nIn my previous experience with Drone, when a tag was pushed to the repository, the corresponding event was sent via webhook. However, in Jenkins, it\u0026rsquo;s not that simple.\nI struggled with this for a while until I found the solution in the official documentation: When using tags in Jenkins Pipeline .\nBy default, Jenkins doesn\u0026rsquo;t set up tag listening.\nYou need to access your pipeline settings, navigate to Branch Sources, and in the Behaviours section at the bottom, check Discover tags to enable tag listening.\nI\u0026rsquo;m still not entirely sure why this design choice was made.\nBasic Pipeline Example Here, I\u0026rsquo;ll provide a basic demonstration of several commonly used trigger conditions.\nDetails on what actions to take after triggering will be discussed later.\nStart by creating a file named jenkinsfile in the project\u0026rsquo;s root directory.\nJenkins uses this file as the default pipeline configuration.\n// jenkinsfile pipeline { agent any stages { stage(\u0026#39;Example Build\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } stage(\u0026#39;Example Deploy\u0026#39;) { when { branch \u0026#39;production\u0026#39; } steps { echo \u0026#39;Deploying\u0026#39; } } stage(\u0026#39;Example Tag Deploy\u0026#39;) { when { buildingTag() } steps { echo \u0026#34;Building $TAG_NAME\u0026#34; } } stage(\u0026#39;Example Tag Deploy\u0026#39;) { when { tag \u0026#34;release-*\u0026#34; } steps { echo \u0026#34;Building $TAG_NAME\u0026#34; } } } } Here, we demonstrate four trigger conditions:\nNo when statement: This stage triggers regardless of the conditions. when branch: This stage triggers only when a specified branch is pushed. when buildingTag(): This stage triggers when any tag is created. when tag \u0026quot;release-*: This stage triggers only when tags conforming to the format \u0026ldquo;release-*\u0026rdquo; are created. These four conditions provide a foundation for various pipeline workflows.\nFor more usage options, consult the official documentation: Pipeline Syntax .\nAfter reviewing the documentation, you might find that Drone is preferable.\nAs Jenkins offers a multitude of options, making it easy to get lost.\nDifferent developer have different preferences for using the UI or command line.\nHowever, the fundamental concepts remain the same, and experience in one can be applied to the other.\nThis concludes the basic introduction to Jenkins pipelines.\n",
    "ref": "/en/blog/202203-jenkins-cicd-2-basic-pipeline/"
  },{
    "title": "Scalable Server 01 Auto Scaling",
    "date": "",
    "description": "Configuring Auto Scaling on GCP for Enhanced Server Flexibility",
    "body": "Create image First, you need to stop the specified VM.\nNext, in the sidebar, navigate to Storage -\u0026gt; Images, then click on it to select Create Image.\nMake sure not to choose Virtual Machines -\u0026gt; achine Image as it will lead to confusion later on!\nIn the beginning, I mistakenly pressed Create Machine Image from the VM section and got stuck in this alternate world, which wasted my whole afternoon.\nThe simplest way to determine this is by not stopping the VM in the first step.\nWhen creating an image, if it doesn\u0026rsquo;t prompt you to stop, then you\u0026rsquo;re doing it wrong.\nFollowing the instructions and successfully creating the image, you can proceed to the next step.\nCreate Instance templates Go to Instance Templates, then click on Create Instance Template.\nChoose the hardware specifications or hardware rules This section is where you determine the specifications required for each new instance when launched within the instance group. The setup here resembles the configuration during VM creation. Additionally, you can configure the firewall settings to open ports 80 and 443 if your network service requires them.\nSelect the VM image as the boot template The difference here is that we are using the recently created image as the template.\nSo, under the boot disk section, click on Change -\u0026gt; Custom image and select the image you\u0026rsquo;ve just created.\nExecute commands automatically at startup Sometimes, when you start a new VM, you need to run certain commands to start services like Docker.\nWithout these commands at startup, your VM will only be powered on but won\u0026rsquo;t have services like the Docker daemon running.\nClick on the Management section to expand its contents, and you\u0026rsquo;ll find instructions for automation.\nYou can specify boot scripts that will run when the instance starts up or restarts. Boot scripts are useful for installing software, updating items, and ensuring that services run smoothly within the virtual machine.\nYou can include commands in this section to ensure that when the VM starts, the necessary services are initiated.\n#! /bin/bash sudo systemctl start docker sudo docker start laravel-app nginx redis sudo docker exec -i laravel-app /var/www/html/artisan migrate sudo docker exec -i laravel-app /var/www/html/artisan l5-swagger:generate --all sudo docker exec -i laravel-app /usr/bin/supervisord -c ./builds/docker/beta/php81/horizon.conf sudo docker exec -i laravel-app /var/www/html/artisan queue:work \u0026amp; Official documentation: Using startup scripts on Linux VMs Instance Group When the triggering conditions are met, the instance group will automatically configure machines based on the previously set instance template.\nTo begin, open the Instance Groups and click on Create Instance Group.\nOn the interface, you will see three options on the left.\nThe interface might have changed since the existing online articles were published.\nNew managed instance group (stateless) New managed instance group (stateful) New unmanaged instance group Among these options, only the two under Managed Instance Group have Auto Scaling functionality.\nAdditionally, the Unmanaged Instance Group allows you to add existing VM instances without the need to create an instance template first.\nReference documentation: Using managed instance groups Select the instance template you want to use and specify the Auto Scaling rules. Once done, click on the Create button at the bottom to complete the creation of the instance group and the Auto Scaling configuration.\nConfigure instance group health checks and automatic healing To ensure that newly started instances are available, automatically shut down any non-functional instances, and replace them with new ones, open the instance group\u0026rsquo;s editing page, and scroll down to the Autohealing section.\nSelect Create Health Check and configure it by providing a name and other settings. For example, if you choose the HTTP protocol, you can send requests to specific routes, while the default TCP option simply checks for normal transmission.\nAfter configuring the health check, let it run, and then go back to the instance group\u0026rsquo;s detailed page to check the health report. If everything is fine, it should indicate that everything is healthy without any issues.\nRolling Update When a new image is created, you\u0026rsquo;ll need to create a new instance template first.\nThen, you need to go back to the instance group and edit it to use the new instance template.\nThis ensures that new instances launched in the future will use the updated template.\nModifying the template won\u0026rsquo;t automatically restart existing instances.\nIf you want to achieve seamless updates for online services, you might consider implementing a rolling update.\nTo perform a rolling update, go to the Instance Groups, select the newly created instance group, and look for a button like UPDATE VMS. The button name may vary, so please check for a similar option.\nNext, select the new instance template and in the Update Settings, choose Automatic for the Update type.\nThis ensures that not only new VMs but also existing ones are replaced with the new template.\nYou will see that it starts a new instance using the new instance template.\nOnce the new instance template has been successfully started, it will proceed to delete the old instances.\nThis ensures a seamless upgrade without impacting your online services.\nAt this point, the service is not fully functional. The instance group is still a collection of individual, independent hosts. While it has the ability to automatically scale the number of hosts, it doesn\u0026rsquo;t inherently handle request routing or load balancing.\nIn accordance with the diagram below. Currently, we have only completed the Instance Group and the subsequent steps.\nWe also need to use the Load Balancing service to provide a fixed IP address responsible for receiving all requests.\nThrough Load Balancing, we can distribute traffic to individual hosts within the instance group.\nWe will talk about this in the further post.\n",
    "ref": "/en/blog/202203-gcp-auto-scaling/"
  },{
    "title": "Jenkins CI/CD 01 Installation",
    "date": "",
    "description": "Setup Jenkins CI/CD step by step",
    "body": "Create Your Own Automated Deployment Service with Jenkins Jenkins is a Java-based CI/CD system tool that we are exploring in this tutorial,\nfollowing our previous introduction to Drone If you plan to use Docker for installation and require Docker commands within Jenkins, you\u0026rsquo;ll need to address Docker-in-Docker issues.\nFor those less familiar with Docker and operating systems, it is recommended to follow the method described below, installing Jenkins directly on your host machine.\nBelow, we\u0026rsquo;ll go through the step-by-step installation process as per the official installation documentation First, ensure that you have Java installed:\njava -version sudo apt install openjdk-11-jre Next, install the Jenkins service on your host machine, opting for the stable version (LTS - Long Term Support):\ncurl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc \u0026gt; /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install jenkins Start the Jenkins service using the following command, and then access it via your web browser at localhost:8080.\nYou\u0026rsquo;ll be prompted to enter the initial admin password:\nsudo systemctl enable --now jenkins Retrieve the initial admin password using the following command:\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword Plugin Installation By default, Jenkins does not support pipelines, so you\u0026rsquo;ll need to install a pipeline plugin.\nIn this tutorial, we\u0026rsquo;re using Blue Ocean .\nTo use Docker as an agent in your build process, you\u0026rsquo;ll also need to install two additional extensions:\nDocker plugin Docker Pipeline You can search for and install these plugins in the Jenkins plugin management section:\nAdditionally, ensure that Docker is properly installed on your Jenkins host machine.\nDocker Command Permissions To configure Docker command permissions, first switch to the Jenkins user:\nsudo su jenkins Then, execute any Docker command, e.g., docker ps.\nIf you encounter a permission denied message like:\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post\nAvoid running sudo chmod 777 /var/run/docker.sock.\nThe correct approach is to add the Jenkins user to the Docker user group:\ncat /etc/passwd # List all users sudo groupadd docker # Create a Docker user group (usually created when Docker is installed) sudo usermod -aG docker jenkins # Add the Jenkins user to the Docker group Restart the Jenkins service. You can trigger the restart by entering the following URL in your web browser:\nhttp://jenlins_url/restart\nClick the restart button, and once it\u0026rsquo;s back up, it will load the new configuration.\nNow you can run Docker commands as the Jenkins user.\n",
    "ref": "/en/blog/202203-jenkins-cicd-1-installation/"
  },{
    "title": "Laravel Queue Usage",
    "date": "",
    "description": "Using queues to defer processing of tasks that don't need immediate attention.",
    "body": "The purpose of queues is to defer processing of a time-consuming task that doesn\u0026rsquo;t require immediate completion. Like sending emails, users don\u0026rsquo;t need to wait for the email to be sent successfully before the next step.\nAllowing the application to respond more quickly to web requests and smooth user experience.\nPreparation Laravel supports multiple queue drivers\nDatabase Redis Amazon SQS If testing locally, you can configure it in the .env file as follows:\nQUEUE_CONNECTION=sync This will execute immediately after sending the task, making it more convenient for testing Queue-related code.\nIn the following example, we will demonstrate using database as the driver.\nQUEUE_CONNECTION=database Create Queue table Because Queue is a feature provided by Laravel.\nYou can directly create a database table named Jobs to record pending Queue information using a command.\nphp artisan queue:table php artisan migrate Create Job files You can create it manually or by using a command.\nphp artisan make:job SyncBlockchainNftItemJob At this point, a file will be generated in the specified path: app/Jobs/SyncBlockchainNftItemJob.php\nWrite the Job program logic. Just modify the SyncBlockchainNftItemJob.php we created earlier.\nAnd the main functionality should be written inside the handle() method, like this\npublic function handle() { if ($this-\u0026gt;userId) { Artisan::call(\u0026#34;nft-items:sync $this-\u0026gt;userId\u0026#34;); } } Call an Artisan command to execute something.\nThe required parameters can be initialized in the __construct at the beginning of the Job program file and can be used as input parameters when creating Queue tasks in the future.\nprotected $userId; public function __construct($userId) { $this-\u0026gt;userId = $userId; } Call the Job to create a task. Now that the Job is ready, how do you call it from the Controller?\nAfter including SyncBlockchainNftItemJob, you can use dispatch wherever you want to create a task and assign it to the specified Job.\n$this-\u0026gt;dispatch(new SyncBlockchainNftItemJob($user-\u0026gt;id)); // Alternatively, you can keep it even simpler. SyncBlockchainNftItemJob::dispatch($user-\u0026gt;id); Starting Queue Worker If sync was not used as the driver earlier, the Queue won\u0026rsquo;t execute!\nYou need to use a command to instruct the Queue to start working!\nphp artisan queue:work It\u0026rsquo;s important to note that once Queue Workers are started, they won\u0026rsquo;t automatically update when there are code changes.\nDuring the deployment phase, remember to use a command to restart the Queue worker. Otherwise, it will continue running the old version of the code!\nphp artisan queue:restart Check status ps -ef|grep queue:work Check execution status Executing the Controller\u0026rsquo;s code, you\u0026rsquo;ll notice that when the Queue is triggered, a new record is added to the jobs table.\nAnd during execution, the terminal will display corresponding information.\nIf you see Processed, it means the task has been completed, and at this point, the record in jobs will be removed.\nSupervisor When the Queue is running, various situations can lead to critical errors, preventing the execution of tasks.\nIn such cases, it\u0026rsquo;s recommended by the official documentation to use Supervisor for management.\nIn the event that the Queue unexpectedly stops operating, Supervisor will restart the Queue service based on the configuration file, ensuring that Jobs can run continuously!\nInstall from docker image FROM php:7.4-fpm RUN apt-get install supervisor CMD /var/www/html/_scripts/cron.sh In the last line of our CMD, we executed a cron.sh as the entry point, which will be used later.\nSupervisor Config Place the file wherever you like.\n# supervisord.conf [program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /var/www/html/artisan queue:work --sleep=90 --tries=3 autostart=true autorestart=true startsecs=5 user=root numprocs=3 redirect_stderr=true stdout_logfile=/var/www/html/storage/logs/supervisord.log stopwaitsecs=3600 [supervisord] logfile=/var/log/supervisor/supervisord_main.log pidfile=/var/www/html/storage/logs/supervisord.pid [supervisorctl] Start Supervisor Create the _scripts/cron.sh file in the project and mount it to the container\u0026rsquo;s /var/www/html path.\n#!/usr/bin/env bash cd /var/www/html supervisord -c ./docker/cron/supervisord.conf # Start supervisord using the configuration file located at the specified path php /var/www/html/artisan schedule:work # Simultaneously initiate the cron job. When the Dockerfile executes a shell script via CMD, it can be considered as\nsh _scripts/cron.sh\nIn cron.sh, we\u0026rsquo;ve done three things:\nNavigate to the container\u0026rsquo;s /var/www/html path. Start the supervisord service using the specified config. Execute php artisan schedule:work. This way, both supervisord and the cron job service are initiated.\nCheck Supervisor status You can use a command to confirm whether supervisord is running.\npgrep -fl supervisord # or ps aux|grep supervisord If an error message appears indicating that the ps or pgrep commands do not exist, you will need to install the package using a command.\napt-get update \u0026amp;\u0026amp; apt-get install procps If you see supervisord listed, it means it\u0026rsquo;s already running.\n",
    "ref": "/en/blog/202203-laravel-queue/"
  },{
    "title": "Enhancing SEO for SPA with Prerender",
    "date": "",
    "description": "Addressing SEO challenges for CSR web pages, demonstrating both self-hosted and SaaS solutions",
    "body": "A Single Page Application (SPA) website utilizes Client-Side Rendering (CSR) to render its content.\nUpon initial loading, the server only returns the root component, and subsequent data is fetched through API interactions.\nHowever, for web crawlers responsible for ranking websites, the content obtained during crawling may appear empty.\nWhile Google claims that their crawler executes JavaScript, other search engines may not necessarily do so.\nTo address this issue, a Prerender service is employed to achieve pre-rendering for web crawlers, optimizing search results.\nPrerender The principle involves setting up an internal headless Chrome to achieve pre-rendering of the webpage.\nTwo methods for using Prerender will be introduced below:\nSelf-hosted: prerender SaaS service: Prerender.io For self-hosting, a reverse proxy needs to be established.\nUpon incoming requests, the user agent is checked. If it is a bot, the request is sent to the self-hosted Prerender service.\nThe service itself includes a headless Chrome, allowing rendering of HTML to be returned.\nIf it\u0026rsquo;s a regular user, the request is forwarded to the frontend server without rendering.\nUsing the Prerender.io SaaS service comes with limitations and may require payment if exceeding the free usage limits. However, it eliminates the need for self-management and setup. The advantage of self-hosting is greater flexibility in configuration.\nBoth solutions can be evaluated based on the use case.\nIf opting for the Prerender.io solution, configuration can only be done when the website is already online.\nSelf-hosted Package: Prerender The documentation is quite clear, and the setup is straightforward. Below is a step-by-step demonstration.\n1. Installation Create a folder, for convenience, let\u0026rsquo;s call it prerender.\nInside the folder, enter the command in the terminal:\nnpm install prerender This completes the installation.\n2. Configuration In the prerender folder, create a file named server.js.\nconst prerender = require(\u0026#39;prerender\u0026#39;); const server = prerender(); server.start(); 3. Start Enter the command in the terminal to start the service.\nnode server.js The service will start on localhost at port 3000. If your service is also running on port 3000, choose a different port.\nWhile your frontend project is running, open the terminal and enter:\ncurl \u0026#34;https://www.google.com.tw/\u0026#34; # Your SPA project URL You will receive an empty DOM with only the root component, as the JS for fetching component content has not been executed.\nThis is a reason for poor SEO in SPAs. Although Google claims that their crawler executes JS, other search engines may not.\nNow comes the magic moment. Open the terminal and enter:\ncurl \u0026#34;http://localhost:3000/render?url=https://www.google.com.tw/\u0026#34; # Your SPA project URL You will get a large package of rendered results!\nThe preceding URL is the Prerender service address, and the parameter includes your project URL.\nPrerender internally starts a headless Chrome to render the page and then returns the result.\nCache Configuration Usually, when you self-host a Prerender service, it will re-render the page each time a request is made.\nHowever, in practice, frequent re-rendering is unnecessary.\nThis can be optimized by setting up caching to retain rendered results for a certain period.\nCache package used: prerender-memory-cache 1. Installation In the terminal, enter the command in the prerender folder:\nnpm install prerender-memory-cache --save 2. Configuration Open the server.js file in the prerender folder and add the configuration declaration before server.start();;\nserver.use(require(\u0026#39;prerender-memory-cache\u0026#39;)) That\u0026rsquo;s it. The second request should use the cached result.\n3. Parameters You can also set parameters in the prerender folder using the terminal.\nCache Maximum Number of Items\nexport CACHE_MAXSIZE=1000 default: 100\nCache Lifespan (seconds)\nexport CACHE_TTL=600 default: 60\n4. Testing After setting the lifespan, use the terminal to test:\ncurl \u0026#34;http://localhost:3000/render?url=http://localhost:3030/\u0026#34; # Your SPA project URL 打完第一次之後，去修改自己專案的一些內容（footer, title 之類）\n確認用瀏覽器直接造訪畫面有出現變更\n接著再重新使用終端機測試，應該會出現舊的 HTML 內容\n這就證明他是使用快取的內容，而不是請求一來就重新渲染\n所以剛剛才會建議修改 footer 或是 title，因為比較好確認\nAfter the first test, modify some content in your project (footer, title, etc.) and confirm the changes by directly visiting the page in the browser. Then, retest using the terminal. You should see the old HTML content.\nThis proves that it is using the cached content rather than re-rendering with each request.\nOnline Service Pricing Plans Below is an example using Cloudflare Workers to set up the Prerender.io service.\nThis is because the frontend project seems to have its own Nginx service, and I don\u0026rsquo;t want to intervene too much.\nBesides, I\u0026rsquo;ve heard that Cloudflare Workers are powerful. It\u0026rsquo;s like a proxy at the forefront, so it can do many things.\nAlthough I\u0026rsquo;ve never had the chance to use it, it seems like a good opportunity.\nPrerender.io Pricing Plans Cloudflare Workers Pricing Plans Configuration Go to the Prerender page, click on Install middleware and select the cloudflare option.\nThe official documentation has been updated.\nJust follow the instructions in the document, and even those who can\u0026rsquo;t code can configure it confidently!\nFollow the document and pictures for Cloudflare Integration .\nOpen the sample configuration file as prompted: prerender/prerender-cloudflare-worker .\nYou\u0026rsquo;ll see the index.js example. The only things to change are API_KEY and PRERENDERED_DOMAINS.\nAPI_KEY：The token in the top left corner of the Prerender.io dashboard. PRERENDERED_DOMAINS：The target domain for which routing rules will be set later. Create a Cloudflare worker and paste the configuration file.\nOrdinary Workers can be tested using the provided playground by Cloudflare, but since we\u0026rsquo;re using a third-party service, it must be verified by Prerender to take effect.\nTherefore, testing with the playground here is not possible.\nAfter configuring the worker, open the Worker tab and choose Add route.\nNext, set the Request Limit Failure Mode.\nOpen Failure (Continue) means that if the Worker fails when a request comes in, it will go directly to the service.\nClose Failure (Block) will throw an error page, preventing users from continuing to access.\nSince we don\u0026rsquo;t want the website to be inaccessible when reaching the free plan limit and not pre-rendering content doesn\u0026rsquo;t affect the website\u0026rsquo;s functionality, choose to skip the Worker and execute it directly in case of failure.\nAfter all, it\u0026rsquo;s just for pre-rendering, and it doesn\u0026rsquo;t matter if crawlers can\u0026rsquo;t access it this time; they can come back next time.\nThe routing rules should match the URL you set in the Worker configuration.\nAfter setting it up, remember to go back to Prerender.io and click \u0026ldquo;Next\u0026rdquo; to complete the verification.\nThen, wait for the verification to be completed.\nUsually, after clicking \u0026ldquo;Verify,\u0026rdquo; the verification results will appear after a few seconds.\nIf you want to simulate the result obtained when crawling after verification, you can use:\ncurl -A Googlebot \u0026#34;https://{your-online-service-url}\u0026#34; This simulates the result obtained by the Googlebot crawler.\nSince we specified a specific user agent in the Worker\u0026rsquo;s Nginx configuration file, the Cloudflare Worker will determine whether to forward it to the Prerender.io service for processing.\nWithout the -A Googlebot parameter, it won\u0026rsquo;t forward for rendering.\nIt will return the CSR result that regular users would get, which is the unrendered webpage content.\n",
    "ref": "/en/blog/202201-using-prerender-improve-spa-seo/"
  },{
    "title": "Successfully launched a VM on GCP on the first try",
    "date": "",
    "description": "Step-by-step guide to launching your first VM instance and configuring it on Google Cloud Platform (GCP).",
    "body": "How to launch a VM virtual machine on Google Cloud Platform (GCP)?\nIt may sound complex, but in practice, it\u0026rsquo;s quite straightforward.\nIf you haven\u0026rsquo;t had any experience with setting up a host before, this time, we\u0026rsquo;ll document the steps for you.\nCreate a new VM instance There\u0026rsquo;s not much to be concerned about. Once you\u0026rsquo;re in the GCP console and on the VM instances page, simply choose the desired specifications, give it a name, and click \u0026lsquo;Create\u0026rsquo;.\nAssigning a static external IP address To ensure that our service has a consistent IP address, we can change the external IP type from ephemeral to static.\nPlease note that converting an ephemeral IP to a static IP may incur additional charges.\nFirst, select the VM instance you want to modify.\nClick on the ellipsis menu (the three dots), and choose View network details.\nNext, from the left-hand menu, select the \u0026lsquo;External IP addresses\u0026rsquo; tab.\nIn the Users column, locate the VM instance you just created.\nYou\u0026rsquo;ll notice that the Name field for the target is currently empty.\nClick on Reserve on the left-hand side, and at this point, it will prompt you to enter a name.\nOnce you\u0026rsquo;ve provided a name, your VM instance\u0026rsquo;s IP address will remain unchanged even after service restarts.\nOpening external ports Your virtual machine is up and running, and you\u0026rsquo;ve also secured a static external IP.\nNow, it\u0026rsquo;s time to establish a connection!\nYou enter the IP address in your browser and\u0026hellip; can\u0026rsquo;t access it! Huh?\nCommonly used network service ports By default, when you create a VM, only port 22 for SSH connections is open.\nIf you set up firewall rules during the creation, it will apply the http-server and https-server network tags, allowing your VM to receive requests on ports 80 and 443.\nYou can check the \u0026lsquo;Network tags\u0026rsquo; section in the VM\u0026rsquo;s information page.\nIf you find that these ports are not open, and you wish to enable them, click on the \u0026lsquo;Edit\u0026rsquo; button for the VM, and navigate to the Firewall settings.\nSelect the http-server and https-server rules that were not checked during initial setup.\nAfter successfully applying these changes, your VM will be able to receive incoming requests on ports 80 and 443.\nOther port If your service requires the use of a different port, you will need to configure it manually.\nLet\u0026rsquo;s use port 8080 as an example for configuration.\nCreate a firewall rule Select the VM instance you want to modify, and from the rightmost three-dot menu, choose \u0026lsquo;View network details\u0026rsquo; to access the \u0026lsquo;Virtual Private Cloud network.\nThen, on the left-hand menu, select \u0026lsquo;Firewall\u0026rsquo; and click on \u0026lsquo;Create firewall rule\u0026rsquo;.\nFor the most part, you can use the default values. Determine if you need to customize based on the provided explanations.\nThere are only a few areas that require adjustments.\nTarget tags These are network tags used when selecting firewall rules later on.\nThey serve the same purpose as the http-server tag when configuring port 80, acting as identifiers.\nSource IPv4 range This is where you specify trusted IP locations.\nIf you want to make it publicly accessible, set it to 0.0.0.0/0 to accept requests from all sources.\nProtocol and Ports To demonstrate opening multiple ports simultaneously, we\u0026rsquo;ve added port 8089.\nThis allows you to specify the external availability of both TCP ports 8080 and 8089.\nYou can observe that ports are separated by a comma ,.\nIf you use a dash -, you can specify a range of ports to be opened.\nApplying the Firewall Rule Go back to Compute Engine and select the VM instance where you want to apply the rule. Click the \u0026lsquo;Edit\u0026rsquo; button.\nManually enter the Target tags you filled out when creating the firewall rule in the Network tags field.\nAfter saving, the rule will be applied.\n",
    "ref": "/en/blog/202201-gcp-vm-basic/"
  },{
    "title": "Importing Excel or CSV Files into Laravel",
    "date": "",
    "description": "Utilize the Laravel-excel package to import Excel or CSV files and write data to the database.",
    "body": "When the backend interface isn\u0026rsquo;t fully developed or when users have extensive data that they prefer not to input manually, opting for a bulk import via a form becomes essential.\nThis is where the Laravel-excel package comes into play. Additionally, it\u0026rsquo;s worth discussing the challenges encountered when importing a large volume of data at once.\nCurrently, the most widely adopted package among developers is Laravel-excel .\nIt supports most file formats commonly used during import processes and encompasses numerous essential functionalities.\nThe official documentation is well-written and provides clear examples within the code snippets.\nInstallation Install using the command, which auto-discovers and registers into the ServiceProvider and Facade.\nThe current latest version is: Documentation 3.1 composer require maatwebsite/excel If you need to add a custom config, execute the command to generate config/excel.php\ncomposer require maatwebsite/excel php artisan vendor:publish --provider=\u0026#34;Maatwebsite\\Excel\\ExcelServiceProvider\u0026#34; --tag=config Defining File Reading Formats Because Laravel\u0026rsquo;s built-in collection type is incredibly useful, we aim to convert the imported form data into a collection format.\nFirst, create an object definition for the data to be loaded.\nLet\u0026rsquo;s assume we\u0026rsquo;re importing CSV files for product specifications, so we\u0026rsquo;ll create a file named app/Imports/ProductImport.php\n\u0026lt;?php namespace App\\Imports; use Illuminate\\Support\\Collection; use Maatwebsite\\Excel\\Concerns\\ToCollection; class ProductImport implements ToCollection { public $collection; public function collection(Collection $collection) { $this-\u0026gt;collection = $collection-\u0026gt;transform(function ($row) { return collect([ \u0026#39;name\u0026#39; =\u0026gt; $row[1], \u0026#39;main_image\u0026#39; =\u0026gt; $row[2], \u0026#39;author_name\u0026#39; =\u0026gt; $row[3], \u0026#39;material\u0026#39; =\u0026gt; $row[4], \u0026#39;specification\u0026#39; =\u0026gt; $row[5], \u0026#39;description\u0026#39; =\u0026gt; $row[6], ]); }); } } As you can see, we\u0026rsquo;ve used the ToCollection interface and defined the correspondence between collection and the content read from the form.\nReading CSV Files Within the controller, call the Excel that was previously registered via auto-discovery through the Facade to read the uploaded file.\nFirstly, create an object named ProductImport.\nThen, use the Facade to invoke the Excel package and employ its loading functionality. Once loaded, utilize the shift() method of the collection to remove the header data from the first row.\npublic function uploadCsv(Request $request) { $import = new ProductImport(); Excel::import($import, $request-\u0026gt;file(\u0026#39;file\u0026#39;)); $productCollection = $import-\u0026gt;collection; $productCollection-\u0026gt;shift(); } This way, you can load the uploaded CSV file into a collection for further operations.\nUploading Testing For testing with Postman, simply select form-data in the Body tab.\nIn the \u0026lsquo;key\u0026rsquo; field, you can select the \u0026lsquo;file\u0026rsquo; format from the dropdown menu.\nCommon Issues 413 Payload Too Large This error originates from Nginx and implies that the uploaded file is too large, causing the server to reject it.\nReference: 413 Payload Too Large Solution using PHP development environment as an example:\nAdjusting Nginx Config By default, it\u0026rsquo;s set to 2M, meaning files larger than this require adjustments.\nOpen nginx.conf and add this within the server block:\nclient_max_body_size 10M; This allows requests larger than 2M to pass to Nginx for further PHP processing.\nAdjusting php.ini php.ini holds various PHP configurations.\nWe\u0026rsquo;re interested in modifying upload_max_filesize, initially set to 2M, to match Nginx:\nupload_max_filesize = 10M; When using Docker, after creating a custom php.ini file, mount it to the designated location within the container:\n- ./php/config/php.ini:/usr/local/etc/php/conf.d/php.ini Once these adjustments are made, restart or reload the settings for the containers that were modified.\nEnter the container to ensure the configurations are applied — use phpinfo() for PHP and nginx -T for Nginx.\nError: 1390 too many placeholders After successfully uploading the file and starting the program, everything seems to execute as expected during data checks.\nHowever, when constructing SQL statements using query builders, an error pops up:\n1390 Prepared statement contains too many placeholders\nThis MySQL error arises from our use of Prepared statement , which supports only 65535 (2^16-1) parameters.\nIf you attempt to insert a vast amount of data exceeding this limit, whether in columns or rows, the assembly fails, resulting in this error.\nNow that we understand the cause, the solution is evident: avoid inserting too much data at once\u0026hellip; pretty straightforward.\nThe solution involves breaking down a large dataset, like an array containing ten thousand product entries, into multiple arrays of 500 entries each.\nThen, execute the insertion or modification in chunks of 500 entries to circumvent the \u0026rsquo;too many placeholders\u0026rsquo; issue.\nIn PHP, we\u0026rsquo;ll use array_chunk to split:\n$productChunks = array_chunk($products, 500); After execution, you\u0026rsquo;ll obtain multiple arrays, each containing 500 entries from the original dataset.\nThen, iterate through each set of 500 entries, resulting in 20 iterations of 500 entries each, instead of a single loop handling 10,000 entries.\nThis method illustrates how to use the Laravel-excel package to read CSV files and write into a database. Exporting files will be covered in a future example with a practical case study.\n",
    "ref": "/en/blog/202112-laravel-excel-import/"
  },{
    "title": "[Git Tutorial] Configuring Global gitignore Locally",
    "date": "",
    "description": "Stop uploading editor configuration files! I really don't want to know what editor you use.",
    "body": "Do you include automatically generated editor configuration files in your .gitignore while developing a project?\nWhy add your personal environment settings to the project?\nFor instance, if you\u0026rsquo;re using PhpStorm, it generates a .idea folder: What is the .idea folder? If you\u0026rsquo;re on an Apple system, you\u0026rsquo;ll encounter .DS_Store: wiki .DS_Store During personal development, it\u0026rsquo;s convenient to add these to .gitignore for a clean and tidy workspace.\nBut when you\u0026rsquo;re part of a team project, this approach becomes impractical.\nTypically, .gitignore is for excluding project settings, keys, rendered content, etc., from version control.\nTherefore, including idea and .DS_Store in gitignore isn\u0026rsquo;t reasonable, as not everyone needs to ignore these files.\nActually, we can configure this locally in the git settings because your machine won\u0026rsquo;t suddenly switch from Mac to Windows.\nExcluding specific files or paths from version control in the local environment sounds much more reasonable.\nOperation Steps 1 Create a Global gitignore Configuration File The filename can be anything; use the terminal to execute the command:\ntouch .gitignore_global 2 Add Files to be Ignored Similar to .gitignore, you can manually open the file for editing or execute:\necho /.idea \u0026gt;\u0026gt; .gitignore_global 3 Add the Configuration File to .gitconfig You can manually edit or use the following command:\ngit config --global core.excludesfile ~/.gitignore_global 4 Check if the Configuration is Successful The simplest way is to open the project and delete the corresponding settings in .gitignore.\nHowever, let\u0026rsquo;s review what we\u0026rsquo;ve just configured.\nYou can use commands to view or directly open the file.\nUsing commands seems more sophisticated, so let\u0026rsquo;s give it a try:\nvim .gitignore_global Here, you can see the list of files we want to ignore in the configuration file.\nSome might have been added during the initialization of Git. Just ensure our added content is written in the file:\n*~ .DS_Store /.idea Next, check if the git configuration file has been set:\nvim .gitconfig You can see in the excluded file settings, it\u0026rsquo;s specified to the ignore list configuration file we just created:\n[core] excludesfile = {path to .gitignore_global} By doing this, we\u0026rsquo;ve completed the \u0026ldquo;Local Global gitignore Configuration\u0026rdquo; task. Reference: git gitignore ",
    "ref": "/en/blog/202111-git-global-gitignore/"
  },{
    "title": "About",
    "date": "",
    "description": "About Byte Ebi",
    "body": "Familiar with PHP including CodeIgniter, WordPress, Laravel.\nKnow a little about frontend and have experience with container.\nNow I am using Golang most of the time. And studying for AWS services.\nMake some small tool in my free time.\nFeel free to contact me via LinkedIn. I wanna have some foreign friends.\nLove Japanese culture after I star watching Vtuber.\nIf you want to talk about that, please do not hesitate to contact me.\n",
    "ref": "/en/about/"
  }]
